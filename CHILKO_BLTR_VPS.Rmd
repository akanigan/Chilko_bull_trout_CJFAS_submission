---
title: "CHILKO_BLTR_VPS"
author: "A. Kanigan"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

**Directions**

The code chunk named 'setup' will need to be run at the start of each session and before any other code chunk, even if steps are skipped.

1. To filter raw VPS data and compile required data sets, including smolt outmigration density, solar data, receiver and reference tag deployment data, and bull trout tagging data, run the code in 'Step 1. Prepare datasets'. Processed data can also be loaded in 'Step 2. Calculate movement metrics'.

2. To re-calculate movement metrics, including utilization distributions (UD), distances between UD centroids and the fence location, and hourly distances moved, run 'Step 2. Calculate movement metrics'. Note that calculating UDs as AKDEs is computationally demanding, and may take several hours to estimates UDs. Data sets compiled via this step can be loaded directly in 'Step 3. Pre-analysis data formatting'. 

3. To replicate statistical analyses and recreate figures, both 'Step3. Pre-analysis data formatting' and 'Step 4. Statistical analysis & Figures' must be run.

```{r setup, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}
## Clear the console and global environment
cat("\014") 
rm(list = ls())

require("knitr")
knitr::opts_knit$set(root.dir = "~/Desktop/BLTR_VPS_CJFAS")

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

# keep warnings visible but non-fatal
options(warn = 1)
knitr::opts_chunk$set(error = FALSE, warning = TRUE, message = TRUE)

# Turn off scientific notation
options(scipen = 999)

# Load packages
library(tidyverse)
library(sf)
library(PBSmapping)
library(ctmm)
library(terra)
library(actel)
library(gdistance)
library(glmmTMB)
library(gratia)
library(MuMIn)
library(mgcv)
library(ggtext)
library(actel)

# Load ggplot theme
source("my_theme.R")

``` 

# Step 1. Prepare datasets

In this section, required data are loaded and formatted; bull trout VPS positions are filtered.

This section may be skipped and the processed datasets can be loaded at the start of the "Calculate movement metrics" section.

## Load and format data

```{r bltr_metadata_2014, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Load metadata for bull trout tracked in 2014 ####

tagging.data.14 <- readRDS("DATA/tagging_data_2014.rds")

# 2. Convert capture and release coordinates from decimal degrees to UTM ####

library(PBSmapping)

# A. Convert capture location coordinates from decimal degrees to UTM (using the PBSmapping package)
tagging.data.14 <- tagging.data.14 %>%
  dplyr::mutate(row = row_number())

tagll <- data.frame(ID = tagging.data.14$row, X = tagging.data.14$Capture_Long, Y = tagging.data.14$Capture_Lat)
attr(tagll, "projection") <- "LL"
xyrec <- convUL(tagll) * 1000
xyrec <- xyrec %>%
  dplyr::mutate(row = ID/1000) %>%
  dplyr::select(-ID) %>%
  dplyr::rename(Capture_East = X, Capture_North = Y)

tagging.data.14 <- tagging.data.14 %>%
  dplyr::left_join(xyrec, by = "row")


# B. Convert release location coordinates from decimal degrees to UTM
tagll <- data.frame(ID = tagging.data.14$row, X = tagging.data.14$Release_Long, Y = tagging.data.14$Release_Lat)
attr(tagll, "projection") <- "LL"
xyrec <- convUL(tagll) * 1000
xyrec <- xyrec %>%
  dplyr::mutate(row = ID/1000) %>%
  dplyr::select(-ID) %>%
  dplyr::rename(Release_East = X, Release_North = Y)

tagging.data.14 <- tagging.data.14 %>%
  dplyr::left_join(xyrec, by = "row")

# 3. Convert timestamps to local time ####

tz(tagging.data.14$UTC_Release_DateTime)

tagging.data.14 <- tagging.data.14 %>%
  dplyr::mutate(ReleaseDateTime = lubridate::with_tz(UTC_Release_DateTime, tzone = "Canada/Pacific"))

tz(tagging.data.14$ReleaseDateTime)

# 4. Remove columns not needed for analysis ####

# First convert FL_m to FL_cm
tagging.data.14 <- tagging.data.14 %>%
  dplyr::mutate(FL_cm = FL_m*100)

bltr.data.14 <- tagging.data.14 %>%
  dplyr::select(Fish_ID, ID, AcousticID, FL_cm, TL_m, Mass_kg, Release_East, Release_North, ReleaseDateTime, UTC_Release_DateTime)


```

```{r bltr_metadata_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Load metadata for bull trout tracked in 2015 ####

# Must load data for 2014-tagged fish b/c some were also tracked in 2015
tagging.data.14 <- readRDS("DATA/tagging_data_2014.rds")

tagging.data.15 <- readRDS("DATA/tagging_data_2015.rds")

# Add a tag_year column to each data set
tagging.data.14 <- tagging.data.14 %>%
  dplyr::mutate(tag_year = "2014",
                FL_cm = FL_m*100) # Convert FL from m to cm

tagging.data.15 <- tagging.data.15 %>%
  dplyr::mutate(tag_year = "2015",
                FL_cm = FL_m,
                TL_m = TL_m/100) # FL is in cm, but column name suggests it's in m

# Only 6 bull trout that were tagged in 2014 were detected in 2015; remove all other fish
tagging.data.14 <- tagging.data.14 %>%
  dplyr::filter(Fish_ID == "BT06" | Fish_ID == "BT08" | Fish_ID == "BT10" |
                  Fish_ID == "BT12" | Fish_ID == "BT13" | Fish_ID == "BT17")

# Combine the data sets
bltr.data.15 <- tagging.data.15 %>%
  dplyr::bind_rows(tagging.data.14)


# 2. Convert capture and release coordinates from decimal degrees to UTM ####

library(PBSmapping)

# A. Convert capture location coordinates from decimal degrees to UTM (using the PBSmapping package)
bltr.data.15 <- bltr.data.15 %>%
  dplyr::mutate(row = row_number())

tagll <- data.frame(ID = bltr.data.15$row, X = bltr.data.15$Capture_Long, Y = bltr.data.15$Capture_Lat)
attr(tagll, "projection") <- "LL"
xyrec <- convUL(tagll) * 1000
xyrec <- xyrec %>%
  dplyr::mutate(row = ID/1000) %>%
  dplyr::select(-ID) %>%
  dplyr::rename(Capture_East = X, Capture_North = Y)

bltr.data.15 <- bltr.data.15 %>%
  dplyr::left_join(xyrec, by = "row")


# B. Convert release location coordinates from decimal degrees to UTM
tagll <- data.frame(ID = bltr.data.15$row, X = bltr.data.15$Release_Long, Y = bltr.data.15$Release_Lat)
attr(tagll, "projection") <- "LL"
xyrec <- convUL(tagll) * 1000
xyrec <- xyrec %>%
  dplyr::mutate(row = ID/1000) %>%
  dplyr::select(-ID) %>%
  dplyr::rename(Release_East = X, Release_North = Y)

bltr.data.15 <- bltr.data.15 %>%
  dplyr::left_join(xyrec, by = "row")

# 3. Convert timestamps to local time ####

tz(bltr.data.15$UTC_Release_DateTime)

bltr.data.15 <- bltr.data.15 %>%
  dplyr::mutate(ReleaseDateTime = lubridate::with_tz(UTC_Release_DateTime, tzone = "Canada/Pacific"))

tz(bltr.data.15$ReleaseDateTime)

# 4. Remove columns that are not needed for analysis ####

bltr.data.15 <- bltr.data.15 %>%
  dplyr::select(Fish_ID, ID, AcousticID, FL_cm, TL_m, Mass_kg, Release_East, Release_North, ReleaseDateTime, UTC_Release_DateTime, tag_year)
 
# 5. Clean up working environment ####

remove(tagging.data.14, tagging.data.15, tagll, xyrec)

gc()

```

```{r receiver_and_reference_tag_data_2014, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Load the receiver and reference tag coordinates ####

receivers.14 <- readRDS("DATA/VPS_station_locations_2014.rds")

# 2. Add a temporary column that identifies the row number by group (will be used below for a left join) ####
receivers.14 <- receivers.14 %>%
  dplyr::group_by(Name) %>%
  dplyr::mutate(n = row_number())

# 3. Convert receiver and reference tag coordinates from decimal degrees to UTM ####

receivers.14 <- receivers.14 %>%
  dplyr::ungroup() %>%
  dplyr::mutate(row = row_number())

tagll <- data.frame(row = receivers.14$row, X = receivers.14$Longitude, Y = receivers.14$Latitude)
attr(tagll, "projection") <- "LL"
xyrec <- convUL(tagll) * 1000
xyrec <- xyrec %>%
  dplyr::mutate(row = row/1000) %>%
  dplyr::rename(Easting = X, Northing = Y)

receivers.14 <- receivers.14 %>%
  dplyr::left_join(xyrec, by = "row")

remove(tagll, xyrec)

# 4. Re-format the start time and end time columns ####

# The start time and end time columns are in a format that's not useful
# Load a data set with start and end times that have been formatted correctly (extracted from the data submitted to Vemco)
receiver_start_end <- readRDS("DATA/receiver_start_end_2014.rds")

# Add a column that will be used below for a left join
receiver_start_end <- receiver_start_end %>%
  dplyr::group_by(Name) %>%
  dplyr::mutate(n = row_number())

# Join the correctly formatted start and end times to the receivers data set
receivers.14 <- receivers.14 %>%
  dplyr::left_join(receiver_start_end, by = c("Name" = "Name", "Latitude" = "Latitude", "Longitude" = "Longitude", "n" = "n"))

# The start and end time columns that were just added to the receivers data set are in UTC; change them to Pacific time
tz(receivers.14$Start.time)
tz(receivers.14$End.time)

receivers.14 <- receivers.14 %>%
  dplyr::rename(Start.time.UTC = Start.time,
                End.time.UTC = End.time)

receivers.14 <- receivers.14 %>%
  dplyr::mutate(Start.time = lubridate::with_tz(Start.time.UTC, tzone = "Canada/Pacific"),
                End.time = lubridate::with_tz(End.time.UTC, tzone = "Canada/Pacific"))

tz(receivers.14$Start.time)
tz(receivers.14$End.time)

# Remove columns that are not needed
receivers.14 <- receivers.14 %>%
  dplyr::select(Name, Northing, Easting, Receiver, Device_Depth, SyncTag, Start.time, End.time, Start.time.UTC, End.time.UTC)

remove(receiver_start_end)    

# 5. Create separate data sets for receiver and reference tag positions ####

# The receiver/sync tag locations and the reference tag locations were recorded multiple times to account for spatial drift
# So, the receiver/sync tag and reference tag locations are calculated as the mean of these coordinates 
# These mean receiver and ref tag locations will only be used for mapping

receiver.locations.14 <- receivers.14 %>%
  dplyr::select(Name, Receiver, Easting, Northing) %>%
  dplyr::group_by(Name) %>%
  dplyr::summarise(meanEasting = mean(Easting), meanNorthing = mean(Northing)) %>%
  dplyr::filter(!grepl("Ref", Name))

ref.tag.locations.14 <- receivers.14 %>%
  dplyr::select(Name, Receiver, Easting, Northing) %>%
  dplyr::group_by(Name) %>%
  dplyr::summarise(meanEasting = mean(Easting), meanNorthing = mean(Northing)) %>%
  dplyr::filter(grepl("Ref", Name))

saveRDS(receiver.locations.14, file = "DATA/receiver_mapping_coordinates_2014.rds")

saveRDS(ref.tag.locations.14, file = "DATA/reference_tag_mapping_coordinates_2014.rds")

```

```{r receiver_and_reference_tag_data_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Load the receiver and reference tag coordinates ####

receivers.15 <- readRDS("DATA/VPS_station_locations_2015.rds")

# 2. Add a temporary column that identifies the row number by group (will be used below for a left join) ####
receivers.15 <- receivers.15 %>%
  dplyr::group_by(Name) %>%
  dplyr::mutate(n = row_number())

# 3. Convert receiver and reference tag coordinates from decimal degrees to UTM ####

receivers.15 <- receivers.15 %>%
  dplyr::ungroup() %>%
  dplyr::mutate(row = row_number())

tagll <- data.frame(row = receivers.15$row, X = receivers.15$Longitude, Y = receivers.15$Latitude)
attr(tagll, "projection") <- "LL"
xyrec <- convUL(tagll) * 1000
xyrec <- xyrec %>%
  dplyr::mutate(row = row/1000) %>%
  dplyr::rename(Easting = X, Northing = Y)

receivers.15 <- receivers.15 %>%
  dplyr::left_join(xyrec, by = "row")

remove(tagll, xyrec)


# 4. Re-format the start time and end time columns ####

# The start time and end time column names for the receivers.15 data set need to be re-named 
receivers.15 <- receivers.15 %>%
  dplyr::rename(Start.time = `Start Time`,
                End.time = `End Time`)

# The start and end time columns are in UTC; change them to Pacific time
tz(receivers.15$Start.time)
tz(receivers.15$End.time)

receivers.15 <- receivers.15 %>%
  dplyr::rename(Start.time.UTC = Start.time,
                End.time.UTC = End.time)

receivers.15 <- receivers.15 %>%
  dplyr::mutate(Start.time = lubridate::with_tz(Start.time.UTC, tzone = "Canada/Pacific"),
                End.time = lubridate::with_tz(End.time.UTC, tzone = "Canada/Pacific"))

tz(receivers.15$Start.time)
tz(receivers.15$End.time)


# Remove columns that are not needed
receivers.15 <- receivers.15 %>%
  dplyr::select(Name, Northing, Easting, Receiver, Device_Depth, SyncTag, Start.time, End.time, Start.time.UTC, End.time.UTC)

  
# 5. Create separate data sets for receiver and reference tag positions that will be used for mapping purposes ####

# The receiver/sync tag locations and the reference tag locations were recorded multiple times to account for any spatial drift
# So, the receiver/sync tag and reference tag locations are calculated as the mean of these coordinates 

receiver.locations.15 <- receivers.15 %>%
  dplyr::select(Name, Receiver, Easting, Northing) %>%
  dplyr::group_by(Name) %>%
  dplyr::summarise(meanEasting = mean(Easting), meanNorthing = mean(Northing)) %>%
  dplyr::filter(!grepl("Ref", Name))

ref.tag.locations.15 <- receivers.15 %>%
  dplyr::select(Name, Receiver, Easting, Northing) %>%
  dplyr::group_by(Name) %>%
  dplyr::summarise(meanEasting = mean(Easting), meanNorthing = mean(Northing)) %>%
  dplyr::filter(grepl("Ref", Name))


saveRDS(receiver.locations.15, file = "DATA/receiver_mapping_coordinates_2015.rds")

saveRDS(ref.tag.locations.15, file = "DATA/reference_tag_mapping_coordinates_2015.rds")

```

```{r VPS_data, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Load the VPS positions for bull trout and reference tags for 2014

bltr.positions.14 <- readRDS("DATA/bltr_VPS_positions_2014.rds")

reference.tag.positions.14 <- readRDS("DATA/ref_tag_VPS_positions_2014.rds")

# Load the VPS positions for bull trout and reference tags for 2015

bltr.positions.15 <- readRDS("DATA/bltr_VPS_positions_2015.rds")

reference.tag.positions.15 <- readRDS("DATA/ref_tag_VPS_positions_2015.rds")

```

```{r sunrise_sunset_timing, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

library(suncalc)

# 2014 ####

# Create a vector of dates between 2014-04-22 and 2014-05-22
date <- seq.Date(from = as.Date("2014-04-22", tz = "Canada/Pacific"), to = as.Date("2014-05-22", tz = "Canada/Pacific"), by = "days")
date.range <- data.frame(date)

# Add columns for latitude and longitude to the date.range data frame; coordinates are for the Chilko outlet
date.range$lat <- 51.62277877885005
date.range$lon <- -124.14224779916107

# For each day in the date.range vector, determine the timing of sunrise and sunset
solar.data.14 <- suncalc::getSunlightTimes(data = date.range, keep = c("sunrise", "sunset"), tz = "Canada/Pacific")

solar.data.14 <- solar.data.14 %>%
  dplyr::mutate(Date = as.POSIXct(date, format = "%Y-%m-%d")) %>%
  dplyr::select(Date, sunrise, sunset)

# 2015 ####

# Create a vector of dates between 2015-04-18 and 2015-05-12
date <- seq.Date(from = as.Date("2015-04-18", tz = "Canada/Pacific"), to = as.Date("2015-05-12", tz = "Canada/Pacific"), by = "days")
date.range <- data.frame(date)

# Add columns for latitude and longitude to the date.range data frame
date.range$lat <- 51.62277877885005
date.range$lon <- -124.14224779916107

# For each day in the date.range vector, determine the timing of sunrise and sunset
solar.data.15 <- suncalc::getSunlightTimes(data = date.range, keep = c("sunrise", "sunset"), tz = "Canada/Pacific")

solar.data.15 <- solar.data.15 %>%
  dplyr::mutate(Date = as.POSIXct(date, format = "%Y-%m-%d")) %>%
  dplyr::select(Date, sunrise, sunset)


# Clean up work space ####

remove(date.range, date)
```

```{r smolt_migration_data, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Load the hourly smolt outmigration densities for 2014 (no fence in 2015, thus no smolt data).
smolt.density <- readRDS("DATA/chilko_smolt_density.rds")

# Format the smolt outmigration data
smolt.density <- smolt.density %>%
  tidyr::unite(DateTime, Date, Time, sep = " ", remove = FALSE) %>%
  dplyr::mutate(DateTime = as.POSIXct(DateTime, format = "%Y-%m-%d %H:%M:%S", tz = "Canada/Pacific"))

```

## Data filtering - 2014

```{r VPS_data_filtering_prep, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# For each fish, calculate the euclidean distance between VPS positions, and calculate the time at liberty
foo <- bltr.positions.14 %>%
  arrange(Fish_ID, DateTime) %>%
  group_by(Fish_ID) %>%
  dplyr::mutate(euclidean_dist_m = sqrt((Easting-lag(Easting))^2 + (Northing-lag(Northing))^2),
                time_interval = DateTime - lag(DateTime))

# Convert distance from meters to body lengths
X <- bltr.data.14 %>%
  dplyr::select(Fish_ID, TL_m)

foo <- foo %>%
  dplyr::left_join(X, by = "Fish_ID")

foo <- foo %>%
  dplyr::mutate(BL_dist = euclidean_dist_m/TL_m)

# Convert the time interval from `difftime` to `numeric`
foo$time_interval <- as.numeric(foo$time_interval)

# For each fish, calculate the mean swim speed as m/s and BL/s
foo <- foo %>%
  dplyr::mutate(time_interval_s = time_interval*60,
                rate_ms = euclidean_dist_m/time_interval_s,
                rate_BLs = BL_dist/time_interval_s) %>%
  dplyr::filter(euclidean_dist_m > 0)

# For each fish, calculate the step length for the tag transmission duration of 180 s
foo <- foo %>%
  dplyr::mutate(sl_m = rate_ms*180)

mean(foo$rate_ms)

mean(foo$rate_BLs)

remove(foo)

```

```{r Input_and_import_VPS_data, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Fixed tag data for reference tags ####

# All ref tags collectively
estPosRef <- reference.tag.positions.14 %>%
  dplyr::filter(grepl("Ref", ID)) %>%
  dplyr::rename(TAG = ID) %>%
  dplyr::filter(!is.na(HPEm))

# 2. VPS fish data (i.e., all fish tag positions) ####
pos <- bltr.positions.14
  
# 3. All receiver locations ####
rec2014 <- receivers.14 %>%
  dplyr::select(Name, Easting, Northing) %>%
  dplyr::group_by(Name) %>%
  dplyr::summarise(Easting = mean(Easting),
                   Northing = mean(Northing)) %>%
  dplyr::filter(!grepl("Ref", Name))

```

```{r Set_configurations, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Set the known UTM coordinates for the reference tag locations ####

# All ref tags
foo <- ref.tag.locations.14 %>%
  dplyr::rename(TAG = Name, East = meanEasting, North = meanNorthing) %>%
  dplyr::filter(grepl("Ref", TAG))

estPosRef <- estPosRef %>%
  dplyr::left_join(foo, by = "TAG")

remove(foo)

# 2. Set the average reference tag transmission rate ####
transRateStat <- mean(c(500,700))

# 3. Select HPE filter (can be changed later during array evaluation) ####
HPEfilter <- 20

# 4. Establish plotting limits for maps (i.e., identify min and max UTM coordinates to be used as cut offs for each grid) ####
xAxisMax <- 421570
xAxisMin <- 419823
yAxisMax <- 5720250
yAxisMin <- 5718250

# 5. Error range of interest (just leave this as is for most applications) ####
ErrorRange <- c(1:25)

# 6. Accuracy goals: These are determined by your research questions and analysis needs ####

# Outlier maximum 
OutlierGoal <- 25

# Accuracy goal for all points
accGoal <- 5

# Goal for average accuracy if studying path lengths (1 order of magnitude less than the maximum step length)
avgAccGoal <- 0.743

# Setup for evaluations
HPELow <- 11 # Lowest HPE to consider in a plot
HPEHigh <- 25 # Highest HPE to consider in a plot

```

```{r Evaluate_array_performance, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Evaluate coverage of the 6 reference tags collectively

# 1. Summarize the performance of each reference tag ####
estPosRef_summary <- estPosRef %>%
  dplyr::mutate(tagDateTime = as.POSIXct(DateTime, tz = "Canada/Pacific")) %>%
  dplyr::group_by(TAG) %>%
  dplyr::summarise(testLength = as.numeric(max(tagDateTime)) - as.numeric(min(tagDateTime)),
                   possibleFixes = (as.numeric(max(tagDateTime)) - as.numeric(min(tagDateTime)))/transRateStat, # Transmissions that should occur
                   actualFixes = length(DateTime), # Total number of transmissions heard
                   fixRateStat = (actualFixes/possibleFixes)*100, # Determine the fix rate %
                   avgAccStat = mean(HPEm), # Average accuracy (m)
                   medianAccStat = median(HPEm), # Median accuracy (m)
                   propAccStat = length(which(HPEm < accGoal)) / length(HPEm), # Proportion of positions that meet the accuracy goal set in Step 2
                   outlierCountStat = length(which(HPEm > OutlierGoal))) # Number of positions greater than outlier goal set in Step 2

# 2. Determine the percent of positions with accuracy equal to or less than each measured value for the stationary tag ####
percentGood <- data.frame(ErrorRange)
percentGood$goodperc <- NA

for(i in 1:length(percentGood$ErrorRange)){
  #What percentage of positions had an error below this value
  percentGood$goodperc [i] <- ((length(estPosRef$HPEm[estPosRef$HPEm <  percentGood$ErrorRange[i]])) / (length(estPosRef$HPEm)))*100
}

#percent good
loc <- percentGood
loc$type <- c("All Ref Tags")

# 3. Plot the performance ####

ggplot() + 
  geom_point(data = loc, aes(x = ErrorRange, y = round(goodperc, digits = 0), shape = factor(type)), size = 3) + 
  geom_line(data = loc, aes(x = ErrorRange, y = round(goodperc, digits = 0), group = factor(type))) +
  #geom_hline(yintercept = 80, linetype = "dashed", colour = "red") +
  #geom_hline(yintercept = 85, linetype = "dashed", colour = "red") +
  geom_hline(yintercept = 95, linetype = "dashed", colour = "red") +
  theme_bw() + 
  theme(text = element_text(size = 18, colour = "black"),
        axis.text = element_text(size = 18),
        axis.text.y = element_text(angle = 0, hjust = 0.0, colour = "black"),
        axis.text.x = element_text(colour = "black"),
        axis.title.y = element_text(vjust = -0.02 ),
        axis.title.x = element_text(vjust = -0.02 ),
        plot.margin = unit(c(1,1.5,1,1.5), "cm"),
        panel.border = element_rect(colour = "black", linewidth = 1),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        #legend.background = element_rect(fill = "transparent", colour = NA),
        legend.position = "none",
        legend.title = element_blank()) +
  scale_y_continuous("Percent of all positions\n", limits = c(70,100), breaks = c(70, 75, 80, 85, 90, 95, 100)) +
  scale_x_continuous("\nMeasured error (m)", breaks = round(seq(min(ErrorRange), max(ErrorRange), by = 1), 1)) +
  coord_cartesian(xlim=c(0.9,10.1))

```

```{r Develop_relationship_b/w_HPE_and_HPEm, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Calculate twice the distance root mean square (2DRMS) error for the 6 reference tags collectively

# 1. Write a function to add a linear model to the 2DRMS error plot ####
lm_eqn <- function(m) {
  l <- list(a = format(as.numeric(coef(m)[1]), digits = 2),
            b = format(abs(as.numeric(coef(m)[2])), digits = 2),
            r2 = format(summary(m)$r.squared, digits = 3));
  if(is.na(coef(m)[2])) {eq <- NA} else {
  if (coef(m)[2] >= 0) {
    eq <- substitute(italic(y) == a + b %.% italic(x)*","
                     ~~italic(r)^2~"="~r2,l)
    } else {
      eq <- substitute(italic(y) == a - b %.% italic(x)*","
                       ~~italic(r)^2~"="~r2,l)
      }
  as.character(as.expression(eq));
  }
}

# 2. Calculate 2DRMS ####

# 1. Calculate the average error for each 1 unit HPE bin 

# Bin HPE values (create increments to bin by)
breaks <- seq(0, ceiling(max(estPosRef$HPE)), by = 1)
breaks

# Create a column with bins
estPosRef$bin <- cut(estPosRef$HPE, breaks)

# 2. For each 1 m bin, calculate the mean HPE
binMean <- tapply(estPosRef$HPEm, estPosRef$bin, mean)
binMean

# Count the number of HPE values that fit into each bin
binNum <- tapply(estPosRef$HPEm, estPosRef$bin, length)
binNum[is.na(binNum)] <- 0

#New data frame for bin data
bin <- data.frame(binMean, binNum)

# 3. Every HPE value in a given 1 m bin has a corresponding HPEm. Each of these HPEm distances is composed of 2 elements: the error (difference between the calculated and measured position) in the X direction, and the error in the Y direction. These error values are referred to as Xe and Ye, respectively.

# Calculate error in the X direction
estPosRef$xe <- sqrt((estPosRef$Easting-(estPosRef$East))^2)

# Calculate error in the Y direction
estPosRef$ye <- sqrt((estPosRef$Northing-(estPosRef$North))^2)

# 4. For each bin, the standard deviations of Xe and Ye are calculated
bin$xeSd <- tapply(estPosRef$xe, estPosRef$bin, sd)
bin$yeSd <- tapply(estPosRef$ye, estPosRef$bin, sd)

# 5. To convert the 2-dimensional standard deviations calculated in #4 into a single measure, the twice distance root mean square (2DRMS) error is calculated from the standard deviations of Xe and Ye
bin$RMS2d <- 2*sqrt((bin$xeSd)^2 + (bin$yeSd)^2)

# 6. Now create a line plot and a data frame just for the numbers we need (i.e., when we have at least 10 tag transmissions and an HPE less than 21) 
bin$avgHPE <- tapply(estPosRef$HPE, estPosRef$bin, mean)
smallBin <- bin[which(bin$binNum > 10),]
smallBin <- smallBin[which(smallBin$avgHPE < 31 ),]
res3 <- lm(smallBin$RMS2d ~ smallBin$avgHPE)

# 7. Plot the HPE to error relationship

# This plot depicts the HPE versus measured error to the median point for each estimated position during the test for the fixed reference and sync tags. The white circles with black outlines and red X represent twice the distance root mean square error of X and Y components of error within an HPE bin of 1; 95% of tag positions have an error less than this point within each bin.

ggplot() +
    geom_point(data = estPosRef, aes(x = HPE, y = HPEm), size = 1, alpha = 0.1) +
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape= 21, size= 5, col = 'black', fill = 'black', alpha = 1) +
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape= 19, size= 4.2, col='white', alpha = 1) + 
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape = 4, size = 3, color = 'red') +
    geom_abline(intercept = res3$coefficients[1], slope = res3$coefficients[2], 
                col='white', linewidth = 1) +
    geom_abline(intercept = res3$coefficients[1], slope = res3$coefficients[2], 
                col='black', linetype = 'dashed', linewidth =  1) +
    #annotate("text", x = 15.5, y = 70, label = lm_eqn(res3), size= 5, parse = TRUE) +
    scale_y_continuous(limits = c(0, 75)) + 
    scale_x_continuous(limits = c(11, 25), breaks = c(11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)) + 
    xlab("\nHPE") +
    ylab("Measured error (m)\n") +
    ggtitle("All reference tags") +
    theme_bw() + 
    theme(plot.margin = unit(c(1,1.5,1,1.5), "cm"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.position = "none",
          text = element_text(size = 12),
          plot.title = element_text(vjust= 1), 
          axis.title.y = element_text(vjust= -0.02),
          axis.title.x = element_text(vjust= -0.02))



# 3. Calculate 2DRMS w/out the high HPEm values for Ref Tag 4 ####

# In the 2DRMS error plot created for the 6 reference tags, it's clear that the slope of the line is being largely influenced by high HPEm values in bins 20 and 21. 

# Remove these large errors (HPEm > 170) and re-calculate the 2DRMS error with the Ref Tags.

foo <- estPosRef %>%
  dplyr::filter(HPEm < 170)

# 2. For each 1 m bin, calculate the mean HPE
binMean <- tapply(foo$HPEm, foo$bin, mean)
binMean

# Count the number of HPE values that fit into each bin
binNum <- tapply(foo$HPEm, foo$bin, length)
binNum[is.na(binNum)] <- 0

#New data frame for bin data
bin <- data.frame(binMean, binNum)

# 3. Every HPE value in a given 1 m bin has a corresponding HPEm. Each of these HPEm distances is composed of 2 elements: the error (difference between the calculated and measured position) in the X direction, and the error in the Y direction. These error values are referred to as Xe and Ye, respectively.

# Calculate error in the X direction
foo$xe <- sqrt((foo$Easting-(foo$East))^2)

# Calculate error in the Y direction
foo$ye <- sqrt((foo$Northing-(foo$North))^2)

# 4. For each bin, the standard deviations of Xe and Ye are calculated
bin$xeSd <- tapply(foo$xe, foo$bin, sd)
bin$yeSd <- tapply(foo$ye, foo$bin, sd)

# 5. To convert the 2-dimensional standard deviations calculated in #4 into a single measure, the twice distance root mean square (2DRMS) error is calculated from the standard deviations of Xe and Ye
bin$RMS2d <- 2*sqrt((bin$xeSd)^2 + (bin$yeSd)^2)

# 6. Now create a line plot and a data frame just for the numbers we need (i.e., when we have at least 10 tag transmissions and an HPE less than 21) 
bin$avgHPE <- tapply(foo$HPE, foo$bin, mean)
smallBin <- bin[which(bin$binNum > 10),]
smallBin <- smallBin[which(smallBin$avgHPE < 31 ),]
res3 <- lm(smallBin$RMS2d ~ smallBin$avgHPE)

# 7. Plot the HPE to error relationship

# This plot depicts the HPE versus measured error to the median point for each estimated position during the test for the fixed reference and sync tags. The white circles with black outlines and red X represent twice the distance root mean square error of X and Y components of error within an HPE bin of 1; 95% of tag positions have an error less than this point within each bin.

(A1 <- ggplot() +
    geom_point(data = foo, aes(x = HPE, y = HPEm), size = 1, alpha = 0.1) +
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape= 21, size= 5, col = 'black', fill = 'black', alpha = 1) +
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape= 19, size= 4.2, col='white', alpha = 1) + 
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape = 4, size = 3, color = 'red') +
    geom_abline(data = res3, aes(intercept = res3$coefficients[1], slope = res3$coefficients[2]), 
                col='white', linewidth = 1) +
    geom_abline(data = res3, aes(intercept = res3$coefficients[1], slope = res3$coefficients[2]), 
                col='black', linetype = 'dashed', linewidth =  1) +
    ggplot2::annotate("text", x = 15.5, y = 20, label = lm_eqn(res3), size= 5, parse = TRUE) +
    scale_x_continuous(limits = c(11, 25), breaks = seq(11, 25, by = 1)) + 
    scale_y_continuous(limits = c(0, 25), breaks = seq(0, 25, by = 5)) +
    labs(x = "\nHPE",
         y = "Measured error (m)\n") +
    ggtitle("2014") +
    theme_bw() + 
    theme(plot.margin = unit(c(1,1,1,1), "cm"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.position = "none",
          text = element_text(size = 12),
          plot.title = element_text(vjust= 1), 
          axis.title.y = element_text(vjust = -0.02),
          axis.title.x = element_text(vjust = -0.02),
          axis.text = element_text(size = 12, colour = "black"),
          panel.border = element_rect(colour = "black", linewidth = 1)))

```

```{r Evaluate_filter_metrics_for_reasonable_HPE_values, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Here, we determine what HPE cut off we should use if we know the level of error that we want to accept.
# NOTE - You MUST change the value of `accGoal` to the desired error.

accGoal <- 5

# 1. Create a data frame to evaluate all possible HPE cutoffs ####

HPE <- c(HPELow:HPEHigh) # Create a list of all possible HPE cut offs

AllHPE <- data.frame(HPE)

AllHPE$ptsReject <- NA # The number of points removed

AllHPE$ptsRejectP <- NA # The percentage of points rejected

AllHPE$ptsRetained <- NA # The number of points retained 

AllHPE$ptsRetainedP <- NA # The percentage of points retained 

AllHPE$incorrectReject <- NA # good = HPEm < accGoal; unacceptable erroneous = HPEm > accGoal

AllHPE$incorrectRejectP <- NA

AllHPE$incorrectRetain <- NA

AllHPE$incorrectRetainP <- NA

AllHPE$incorrectRetainPvsRetain <-NA # incorrectly retained/all retained *100

AllHPE$correctReject <- NA

AllHPE$correctRejectP <- NA

AllHPE$correctRetain <- NA

AllHPE$correctRetainP <- NA

AllHPE$goodDataLossP <- NA # incorrectReject/goodpts *100

AllHPE$badDataRetainP <- NA # incorrectRetain/AllHPE$ptsRetain * 100

AllHPE$sdDEV <- NA

AllHPE$avgErr <- NA

AllHPE$medErr < NA

AllHPE$maxErr <- NA

AllHPE$P90 <- NA

AllHPE$P99 <- NA

AllHPE$P95 <- NA

badpts <- length(estPosRef$HPEm[estPosRef$HPEm > accGoal]) # Number of all points that were unacceptably erroneous

badptsP <-(length(estPosRef$HPEm[estPosRef$HPEm > accGoal])/(length(estPosRef$HPEm)))*100 # % of all points that were unacceptably erroneous

goodpts <- length(estPosRef$HPEm[estPosRef$HPEm <= accGoal]) #Number of all points that are good

goodptsP <- (length(estPosRef$HPEm[estPosRef$HPEm <= accGoal])/(length(estPosRef$HPEm)))*100 # % of all points that are good

# 2. Calculate summary data in the `AllHPE` data frame ####

for(i in 1:length(AllHPE$HPE)) {

  AllHPE$ptsReject[i] <- length(which(estPosRef$HPE > AllHPE$HPE[i]))
  
  AllHPE$ptsRejectP[i] <- length(which(estPosRef$HPE > AllHPE$HPE[i]))/(length(estPosRef$HPEm))*100 #% of all points dropped

  AllHPE$ptsRetain[i] <- length(which(estPosRef$HPE < AllHPE$HPE[i]))
  
  AllHPE$ptsRetainP[i] <- length(which(estPosRef$HPE < AllHPE$HPE[i]))/(length(estPosRef$HPEm))*100 # % of all points retained
  
  AllHPE$incorrectReject[i] <- length(which((estPosRef$HPE[estPosRef$HPEm <= accGoal] > AllHPE$HPE[i]) == TRUE)) # incorrect reject
  
  AllHPE$incorrectRejectP[i] <- (AllHPE$incorrectReject[i]/length(estPosRef$HPEm[estPosRef$HPEm <= accGoal]))*100 # divided by total with acceptable error
  
  AllHPE$incorrectRetain[i] <- length(which((estPosRef$HPE[estPosRef$HPEm > accGoal] < AllHPE$HPE[i]) == TRUE)) # incorrect retain
  
  AllHPE$incorrectRetainP[i] <- (AllHPE$incorrectRetain[i]/length(estPosRef$HPEm[estPosRef$HPEm > accGoal ]))*100 # divided by accurate points
  
  AllHPE$incorrectRetainPvsRetain[i] <- (AllHPE$incorrectRetain[i]/AllHPE$ptsRetain[i])*100 # divided by total retained by filter
 
  AllHPE$correctReject[i] <- (length(which((estPosRef$HPE[estPosRef$HPEm > accGoal] > AllHPE$HPE[i]) == TRUE))) # Correctly Rejected
  
  AllHPE$correctRejectP[i] <- (AllHPE$correctReject[i]/AllHPE$ptsRetain[i])*100 # divided by sum of all positions
  
  AllHPE$correctRetain[i] <- length(which((estPosRef$HPE[estPosRef$HPEm < accGoal] < AllHPE$HPE[i]) == TRUE)) # Correctly Retained
  
  AllHPE$correctRetainP[i] <- (AllHPE$correctRetain[i]/length(estPosRef$HPEm))*100 # divided by sum of all positions
  
  AllHPE$goodDataLossP[i] <- (AllHPE$incorrectReject[i]/goodpts)*100 # incorrectReject/goodpts *100
  
  AllHPE$badDataRetainP[i] <- (AllHPE$incorrectRetain[i]/AllHPE$ptsRetain[i])*100 # incorrectRetain/AllHPE$ptsRetain * 100
  
  AllHPE$avgErr[i] <- mean(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]]) # mean of all error for each HPE
  
  AllHPE$sdDEV [i] <- sd(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]]) # Standard deviation of all error for each HPE cut off
  
  AllHPE$medErr[i] <- median(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]]) # median of all error for each HPE cut off

  AllHPE$maxErr[i] <- max(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]]) # max of all error for each HPE cut off
  
  AllHPE$P99[i] <- quantile(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]], probs = 0.99) # The 99th percentile of all error for each HPE cut off 
  
  AllHPE$P90[i] <- quantile(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]], probs = 0.9) # The 90th percentile of all error for each HPE cut off
  
  AllHPE$P95[i] <- quantile(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]], probs = 0.95) # The 95th percentile of all error for each HPE cut off 
  
  AllHPE$count15[i] <- length(which((estPosRef$HPE[estPosRef$HPEm > 20] < AllHPE$HPE[i]) == TRUE))
}

# 3. Establish a relationship between incorrectly rejected data and incorrectly retained data ####

start <- 1
end <- 15 

AllHPEperf1 <- data.frame(HPE = AllHPE$HPE[start:end],
                          perc = AllHPE$correctRejectP[start:end])

AllHPEperf1$var <- "Correct Rejection"

AllHPEperf2 <- data.frame(HPE = AllHPE$HPE[start:end],
                          perc = AllHPE$incorrectRetainP[start:end])

AllHPEperf2$var <- "Incorrect Retainment"

AllHPEperf3 <- data.frame(HPE = AllHPE$HPE[start:end],
                          perc = AllHPE$incorrectRejectP[start:end])

AllHPEperf3$var <-"Incorrect Rejection"

# Combine all objects above for plotting
AllHPEperf <- rbind(AllHPEperf1,AllHPEperf3,AllHPEperf2) # Note that legend is alphabetical but graph is by data frame order


# 4. Plot data loss vs. error retention for the desired accuracy goal ####

ggplot(data = AllHPE) +
    geom_point(aes(x = incorrectRetainPvsRetain, y = incorrectRejectP), shape = 20, size = 3) + 
    geom_hline(yintercept = 5, linetype = "dashed", colour = "red", linewidth = 0.5) +
    geom_vline(xintercept = 1, linetype = "dashed", colour = "red", linewidth = 0.5) +
    ggrepel::geom_text_repel(cex = 5, aes(x = incorrectRetainPvsRetain, y = incorrectRejectP, label = HPE, group = NULL), 
                             nudge_x = 0.05, nudge_y = 0, max.iter = 50000) +
    xlab("\n% Incorrecly Retained\n(of all retained positions)") +
    ylab("% Incorrectly Rejected\n(of all acceptable positions)\n") +
   ggtitle("2014") +
    theme_bw() + 
    theme(text = element_text(size = 14, colour = "black"),
          axis.text = element_text(colour = "black", size = 14),
        plot.title = element_text(vjust = 2),
        axis.text.y = element_text(angle = 0, hjust = 0),
        axis.title.y = element_text(vjust = 0.3),
        axis.title.x = element_text(vjust = -0.02 ),
        plot.margin = unit(c(1,1.5,1,1.5), "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "inside",
        legend.position.inside = c(1.5, 0.8),
        legend.background = element_rect(fill = "transparent", colour = NA),
        legend.title = element_blank(),
        panel.border = element_rect(colour = "black", linewidth = 1))

# 5. Plot mean error, max error, and number of unacceptable high errors for each HPE cut off ####

ggplot(data = AllHPE) + 
   geom_hline(yintercept = 0.75, linetype = "dashed", colour = "red") +
     geom_hline(yintercept = 5, linetype = "solid", colour = "blue") +
     geom_point(aes(x = HPE, y = avgErr, shape = "Mean error"), size = 3, color = 'black') +
     geom_point(aes(x = HPE, y = P99, shape = "99th percentile"), size = 3, colour = "black") +
     geom_point(aes(x = HPE, y = P90, shape = "90th percentile"), size = 3, colour = "black") +
     geom_point(aes(x = HPE, y = P95, shape = "95th percentile"), size = 3, colour = "black") +
     
     xlab("\nHPE") + 
     ylab("Measured error (m)\n") +
   ggtitle("2014") +
   
     scale_shape_manual(values = c(18,17,16,15)) +
   
     scale_x_continuous("HPE", breaks = round(seq(min(HPE), max(HPE), by = 1),1)) +
     theme_bw() + 
     theme(text = element_text(size = 14),
        plot.title = element_text(vjust = 2),
        axis.text = element_text(colour = "black", size = 14),
        axis.text.y = element_text(angle = 0, hjust = 0),
        axis.title.y = element_text(vjust = -0.02 ),
        axis.title.x = element_text(vjust = -0.02 ),
        plot.margin = unit(c(0.2,0.2,0.2,0.2), "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "top",
        panel.border = element_rect(colour = "black", linewidth = 1),
        legend.background = element_rect(fill = "transparent", colour = NA),
        legend.title = element_blank())

```

```{r Filter_VPS_data, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Filter bull trout VPS positions based on the start and end dates for the study ####

# Smolt outmigration data are available for the period between April 20 at 00:00:00 and May 19 at 03:00:00. 

# Filter the bull trout VPS data to these dates. 49,308
bltr.positions.14 <- bltr.positions.14 %>%
  dplyr::filter(DateTime < "2014-05-19 04:00:00")

# Count the number of positions that were estimated for each fish prior to filtering
foo <- bltr.positions.14 %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::summarise(n = length(Fish_ID))

remove(foo)

# 2. Discard bull trout positions that are located on land ####

# 49,308 --> 49,256 

# Load the study area polygon
study_area_UTM <- sf::read_sf(dsn = "GIS/", layer = "Chilko_Outlet_UTM_VPS_Study")

# Convert the `bltr.positions.14` data frame to a sf object
pos_sf <- sf::st_as_sf(bltr.positions.14, coords = c(9:10))
st_crs(pos_sf) <- 32610

# Separate the VPS positions into two data sets based on whether the positions are in water or on land
X <- sapply(st_intersects(pos_sf, study_area_UTM), function(x){length(x)==0})
X <- as_tibble(X)

# Combine the `X` column w/ bltr.positions, and filter positions that are on land (value = TRUE)
bltr.positions.14 <- bltr.positions.14 %>%
  dplyr::bind_cols(X)

bltr.positions.14 <- bltr.positions.14 %>%
  dplyr::filter(value == FALSE)

# Count the number of positions that were estimated for each fish after filtering on-land positions
foo <- bltr.positions.14 %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::summarise(n = length(Fish_ID))

foo %>% 
  dplyr::summarise(min = min(n),
                   max = max(n),
                   mean = mean(n),
                   sd = sd(n))


# 3. Filter bull trout VPS positions based on HPE 17 ####

bltr.positions.14 <- bltr.positions.14 %>%
  dplyr::filter(HPE <= 17) # 49,256 --> 46,050


# 4. Discard columns from the bull trout positions data that are not needed for further analyses ####

bltr.positions.14 <- bltr.positions.14 %>%
  dplyr::select(Fish_ID, AcousticID, DateTime, DateTimeUTC, Lat, Long, Easting, Northing, HPE)

# Save filtered data

#saveRDS(bltr.positions.14, file = "DATA/bltr_positions_filtered_2014.rds")

```

## Data filtering - 2015

```{r VPS_data_filtering_prep_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# The tags implanted in the 2014 bull trout have a different transmission rate than those in the 2015 fish
# So, step lengths will need to be calculated separately for 2014 and 2015 fish; though, some steps can still be completed using all bull trout

# For each fish, calculate the euclidean distance between VPS positions, and calculate the time between positions
foo <- bltr.positions.15 %>%
  arrange(Fish_ID, DateTime) %>%
  group_by(Fish_ID) %>%
  dplyr::mutate(euclidean_dist_m = sqrt((Easting-lag(Easting))^2 + (Northing-lag(Northing))^2),
                time_interval = DateTime - lag(DateTime))

# Convert distance from meters to body lengths
X <- bltr.data.15 %>%
  dplyr::select(Fish_ID, TL_m)

foo <- foo %>%
  dplyr::left_join(X, by = "Fish_ID")

foo <- foo %>%
  dplyr::mutate(BL_dist = euclidean_dist_m/TL_m)

# Convert the time interval from `difftime` to `numeric`
foo$time_interval <- as.numeric(foo$time_interval)

# For each fish, calculate the mean swim speed as m/s and BL/s
foo <- foo %>%
  dplyr::mutate(time_interval_s = time_interval, # time interval is already in seconds
                rate_ms = euclidean_dist_m/time_interval_s,
                rate_BLs = BL_dist/time_interval_s) %>%
  dplyr::filter(euclidean_dist_m > 0)

# In 2015, there were several movement rates that exceed biologically possible rates; remove any row w/ BL_s > 3
foo <- foo %>%
  dplyr::filter(rate_BLs < 3)

# For each fish, calculate the step length for the tag transmission duration of 180 s for 2014 fish and 80 s for 2015 fish
# THE 2015 TAGS HAVE A DIFFERENT TRANSMISSION RATE (40-120s; 80s average) THAN THE 2014 FISH (110-250s; 180s average)
foo <- foo %>%
  dplyr::mutate(sl_m = if_else(AcousticID <15000, rate_ms*180, rate_ms*80)) # Acoustic ID for 2014 bltr are all < 15000 and for 2015 they're > 15000

mean(foo$rate_BLs)

mean(foo$rate_ms)

```

```{r Input_and_import_VPS_data_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Fixed tag data for reference tags ####

# All ref tags collectively
estPosRef <- reference.tag.positions.15 %>%
  dplyr::filter(grepl("Ref", ID)) %>%
  dplyr::rename(TAG = ID) %>%
  dplyr::filter(!is.na(HPEm))

# 2. VPS fish data (i.e., all fish tag positions) ####
pos <- bltr.positions.15
  
# 3. All receiver positions ####
rec2015 <- receivers.15 %>%
  dplyr::select(Name, Easting, Northing) %>%
  dplyr::group_by(Name) %>%
  dplyr::summarise(Easting = mean(Easting),
                   Northing = mean(Northing)) %>%
  dplyr::filter(!grepl("Ref", Name))

```

```{r Set_configurations_fixed_variables_and_limits_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Set the known UTM coordinates for the reference and sync tag locations ####

# All ref tags
foo <- ref.tag.locations.15 %>%
  dplyr::rename(TAG = Name, East = meanEasting, North = meanNorthing) %>%
  dplyr::filter(grepl("Ref", TAG))

estPosRef <- estPosRef %>%
  dplyr::left_join(foo, by = "TAG")

remove(foo)


# 2. Set the average tag transmission rate ####
transRateStat <- mean(c(500,700))

# 3. Select HPE filter (can be changed later during array evaluation) ####
HPEfilter <- 20

# 4. Establish plotting limits for maps (i.e., identify min and max UTM coordinates to be used as cut offs for each grid) ####
xAxisMax <- 421570
xAxisMin <- 419823
yAxisMax <- 5720250
yAxisMin <- 5718250

# 5. Error range of interest (just leave this as is for most applications) ####
ErrorRange <- c(1:20)

# 6. Accuracy goals: These are determined by your research questions and analysis needs ####

# Outlier maximum 
OutlierGoal <- 25 

# Accuracy goal for all points
accGoal <- 5

# Goal for average accuracy if studying path lengths 
avgAccGoal <- 1.20

# Setup for evaluations
HPELow <- 6 # Lowest HPE to consider in a plot
HPEHigh <- 25 # Highest HPE to consider in a plot

```

```{r Evaluate_array_performance_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Evaluate coverage of the reference tags

# 1. Summarize the performance of each reference tag ####
estPosRef_summary <- estPosRef %>%
  dplyr::mutate(tagDateTime = as.POSIXct(DateTime, tz = "Canada/Pacific")) %>%
  dplyr::group_by(TAG) %>%
  dplyr::summarise(testLength = as.numeric(max(tagDateTime)) - as.numeric(min(tagDateTime)),
                   possibleFixes = (as.numeric(max(tagDateTime)) - as.numeric(min(tagDateTime)))/transRateStat, # Transmissions that should occur
                   actualFixes = length(DateTime), # Total number of transmissions heard
                   fixRateStat = (actualFixes/possibleFixes)*100, # Determine the fix rate %
                   avgAccStat = mean(HPEm), # Average accuracy (m)
                   medianAccStat = median(HPEm), # Median accuracy (m)
                   propAccStat = length(which(HPEm < accGoal)) / length(HPEm), # Proportion of positions that meet the accuracy goal set in Step 2
                   outlierCountStat = length(which(HPEm > OutlierGoal))) # Number of positions greater than outlier goal set in Step 2

# 2. Determine the percent of positions with accuracy equal to or less than each measured value for the stationary tag ####
percentGood <- data.frame(ErrorRange)
percentGood$goodperc <- NA

for(i in 1:length(percentGood$ErrorRange)){
  #What percentage of positions had an error below this value
  percentGood$goodperc [i] <- ((length(estPosRef$HPEm[estPosRef$HPEm <  percentGood$ErrorRange[i]])) / (length(estPosRef$HPEm)))*100
}


#percent good
loc.15 <- percentGood
loc.15$type <- c("All Ref Tags")

# 3. Plot the performance ####

ggplot() + 
  geom_point(data = loc.15, aes(x = ErrorRange, y = round(goodperc, digits = 0), shape = factor(type)), size = 3) + 
  geom_line(data = loc.15, aes(x = ErrorRange, y = round(goodperc, digits = 0), group = factor(type))) +
  geom_hline(yintercept = 95, linetype = "dashed", colour = "red") +
  theme_bw() + 
  theme(text = element_text(size = 18, colour = "black"),
        axis.text = element_text(size = 18),
        axis.text.y = element_text(angle = 0, hjust = 0.0, colour = "black"),
        axis.text.x = element_text(colour = "black"),
        axis.title.y = element_text(vjust = -0.02 ),
        axis.title.x = element_text(vjust = -0.02 ),
        plot.margin = unit(c(1,1.5,1,1.5), "cm"),
        panel.border = element_rect(colour = "black", linewidth = 1),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        legend.position = "none",
        legend.title = element_blank()) +
  scale_y_continuous("Percent of all positions\n", limits = c(60,100), breaks = c(60, 65, 70, 75, 80, 85, 90, 95, 100)) +
  scale_x_continuous("\nMeasured error (m)", breaks = round(seq(min(ErrorRange), max(ErrorRange), by = 1), 1)) +
  coord_cartesian(xlim=c(0.9,10.1))

```

```{r Develop_relationship_b/w_HPE_and_HPEm_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Calculate twice the distance root mean square (2DRMS) error for the 6 reference tags collectively

# 1. Write a function to add a linear model to the 2DRMS error plot ####
lm_eqn <- function(m) {
  l <- list(a = format(as.numeric(coef(m)[1]), digits = 2),
            b = format(abs(as.numeric(coef(m)[2])), digits = 2),
            r2 = format(summary(m)$r.squared, digits = 3));
  if(is.na(coef(m)[2])) {eq <- NA} else {
  if (coef(m)[2] >= 0) {
    eq <- substitute(italic(y) == a + b %.% italic(x)*","
                     ~~italic(r)^2~"="~r2,l)
    } else {
      eq <- substitute(italic(y) == a - b %.% italic(x)*","
                       ~~italic(r)^2~"="~r2,l)
      }
  as.character(as.expression(eq));
  }
}

# 2. Calculate 2DRMS ####

# 1. Calculate the average error for each 1 unit HPE bin 

# Bin HPE values (create increments to bin by)
breaks <- seq(0, ceiling(max(estPosRef$HPE)), by = 1)
breaks

# Create a column with bins
estPosRef$bin <- cut(estPosRef$HPE, breaks)

# 2. For each 1 m bin, calculate the mean HPE
binMean <- tapply(estPosRef$HPEm, estPosRef$bin, mean)
binMean

# Count the number of HPE values that fit into each bin
binNum <- tapply(estPosRef$HPEm, estPosRef$bin, length)
binNum[is.na(binNum)] <- 0

#New data frame for bin data
bin <- data.frame(binMean, binNum)

# 3. Every HPE value in a given 1 m bin has a corresponding HPEm. Each of these HPEm distances is composed of 2 elements: the error (difference between the calculated and measured position) in the X direction, and the error in the Y direction. These error values are referred to as Xe and Ye, respectively.

# Calculate error in the X direction
estPosRef$xe <- sqrt((estPosRef$Easting-(estPosRef$East))^2)

# Calculate error in the Y direction
estPosRef$ye <- sqrt((estPosRef$Northing-(estPosRef$North))^2)

# 4. For each bin, the standard deviations of Xe and Ye are calculated
bin$xeSd <- tapply(estPosRef$xe, estPosRef$bin, sd)
bin$yeSd <- tapply(estPosRef$ye, estPosRef$bin, sd)

# 5. To convert the 2-dimensional standard deviations calculated in #4 into a single measure, the twice distance root mean square (2DRMS) error is calculated from the standard deviations of Xe and Ye
bin$RMS2d <- 2*sqrt((bin$xeSd)^2 + (bin$yeSd)^2)

# 6. Now create a line plot and a data frame just for the numbers we need (i.e., when we have at least 10 tag transmissions and an HPE less than 21) 
bin$avgHPE <- tapply(estPosRef$HPE, estPosRef$bin, mean)
smallBin <- bin[which(bin$binNum > 10),]
smallBin <- smallBin[which(smallBin$avgHPE < 31 ),]
res3 <- lm(smallBin$RMS2d ~ smallBin$avgHPE)

# 7. Plot the HPE to error relationship

# This plot depicts the HPE versus measured error to the median point for each estimated position during the test for the fixed reference and sync tags. The white circles with black outlines and red X represent twice the distance root mean square error of X and Y components of error within an HPE bin of 1; 95% of tag positions have an error less than this point within each bin.

ggplot() +
    geom_point(data = estPosRef, aes(x = HPE, y = HPEm), size = 1, alpha = 0.1) +
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape= 21, size= 5, col = 'black', fill = 'black', alpha = 1) +
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape= 19, size= 4.2, col='white', alpha = 1) + 
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape = 4, size = 3, color = 'red') +
    geom_abline(data = res3, aes(intercept = res3$coefficients[1], slope = res3$coefficients[2]), 
                col='white', linewidth = 1) +
    geom_abline(data = res3, aes(intercept = res3$coefficients[1], slope = res3$coefficients[2]), 
                col='black', linetype = 'dashed', linewidth =  1) +
    ggplot2::annotate("text", x = 15.5, y = 70, label = lm_eqn(res3), size= 5, parse = TRUE) +
    scale_y_continuous(limits = c(0, 75)) + 
    scale_x_continuous(limits = c(6, 25), breaks = c(6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)) + 
    xlab("\nHPE") +
    ylab("Measured error (m)\n") +
    ggtitle("All reference tags") +
    theme_bw() + 
    theme(plot.margin = unit(c(1,1.5,1,1.5), "cm"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.position = "none",
          text = element_text(size = 12),
          plot.title = element_text(vjust= 1), 
          axis.title.y = element_text(vjust= -0.02),
          axis.title.x = element_text(vjust= -0.02))



# 3. Calculate 2DRMS w/out the high HPEm values ####

# Remove these large errors and re-calculate the 2DRMS error with the Ref Tags.

foo <- estPosRef %>%
  dplyr::filter(HPEm < 30) %>%
  dplyr::filter(HPE < 20)

# 2. For each 1 m bin, calculate the mean HPE
binMean <- tapply(foo$HPEm, foo$bin, mean)
binMean

# Count the number of HPE values that fit into each bin
binNum <- tapply(foo$HPEm, foo$bin, length)
binNum[is.na(binNum)] <- 0

#New data frame for bin data
bin <- data.frame(binMean, binNum)

# 3. Every HPE value in a given 1 m bin has a corresponding HPEm. Each of these HPEm distances is composed of 2 elements: the error (difference between the calculated and measured position) in the X direction, and the error in the Y direction. These error values are referred to as Xe and Ye, respectively.

# Calculate error in the X direction
foo$xe <- sqrt((foo$Easting-(foo$East))^2)

# Calculate error in the Y direction
foo$ye <- sqrt((foo$Northing-(foo$North))^2)

# 4. For each bin, the standard deviations of Xe and Ye are calculated
bin$xeSd <- tapply(foo$xe, foo$bin, sd)
bin$yeSd <- tapply(foo$ye, foo$bin, sd)

# 5. To convert the 2-dimensional standard deviations calculated in #4 into a single measure, the twice distance root mean square (2DRMS) error is calculated from the standard deviations of Xe and Ye
bin$RMS2d <- 2*sqrt((bin$xeSd)^2 + (bin$yeSd)^2)

# 6. Now create a line plot and a data frame just for the numbers we need (i.e., when we have at least 10 tag transmissions and an HPE less than 21) 
bin$avgHPE <- tapply(foo$HPE, foo$bin, mean)
smallBin <- bin[which(bin$binNum > 10),]
smallBin <- smallBin[which(smallBin$avgHPE < 31 ),]
res3 <- lm(smallBin$RMS2d ~ smallBin$avgHPE)

# 7. Plot the HPE to error relationship

# This plot depicts the HPE versus measured error to the median point for each estimated position during the test for the fixed reference and sync tags. The white circles with black outlines and red X represent twice the distance root mean square error of X and Y components of error within an HPE bin of 1; 95% of tag positions have an error less than this point within each bin.

(A2 <- ggplot() +
    geom_point(data = foo, aes(x = HPE, y = HPEm), size = 1, alpha = 0.1) +
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape= 21, size= 5, col = 'black', fill = 'black', alpha = 1) +
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape= 19, size= 4.2, col='white', alpha = 1) + 
    geom_point(data = smallBin, aes(x = avgHPE, y = RMS2d), shape = 4, size = 3, color = 'red') +
    geom_abline(data = res3, aes(intercept = res3$coefficients[1], slope = res3$coefficients[2]), 
                col='white', linewidth = 1) +
    geom_abline(data = res3, aes(intercept = res3$coefficients[1], slope = res3$coefficients[2]), 
                col='black', linetype = 'dashed', linewidth =  1) +
    ggplot2::annotate("text", x = 10.5, y = 20, label = lm_eqn(res3), size= 5, parse = TRUE) +
    #coord_cartesian(xlim = c(6,20), ylim = c(0, 25)) +
    scale_x_continuous(limits = c(6,20), breaks = seq(6,20, by = 1)) + 
    scale_y_continuous(limits = c(0,25), breaks = seq(0,25, by = 5)) +
    #coord_fixed(ratio = 0.5) +
    labs(x = "\nHPE",
         y = "") +
    ggtitle("2015") +
    theme_bw() + 
        theme(plot.margin = unit(c(1,1,1,1), "cm"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          legend.position = "none",
          text = element_text(size = 12),
          plot.title = element_text(vjust= 1), 
          axis.title.y = element_text(vjust = -0.02),
          axis.title.x = element_text(vjust = -0.02),
          axis.text = element_text(size = 12, colour = "black"),
          panel.border = element_rect(colour = "black", linewidth = 1)))
```

```{r Evaluate_filter_metrics_for_reasonable_HPE_values_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Here, we determine what HPE cut off we should use if we know the level of error that we want to accept.
# NOTE - You MUST change the value of `accGoal` to the desired error.

accGoal <- 5

# 1. Create a data frame to evaluate all possible HPE cutoffs ####

HPE <- c(HPELow:HPEHigh) # Create a list of all possible HPE cut offs

AllHPE <- data.frame(HPE)

AllHPE$ptsReject <- NA # The number of points removed

AllHPE$ptsRejectP <- NA # The percentage of points rejected

AllHPE$ptsRetained <- NA # The number of points retained 

AllHPE$ptsRetainedP <- NA # The percentage of points retained 

AllHPE$incorrectReject <- NA # good = HPEm < accGoal; unacceptable erroneous = HPEm > accGoal

AllHPE$incorrectRejectP <- NA

AllHPE$incorrectRetain <- NA

AllHPE$incorrectRetainP <- NA

AllHPE$incorrectRetainPvsRetain <-NA # incorrectly retained/all retained *100

AllHPE$correctReject <- NA

AllHPE$correctRejectP <- NA

AllHPE$correctRetain <- NA

AllHPE$correctRetainP <- NA

AllHPE$goodDataLossP <- NA # incorrectReject/goodpts *100

AllHPE$badDataRetainP <- NA # incorrectRetain/AllHPE$ptsRetain * 100

AllHPE$sdDEV <- NA

AllHPE$avgErr <- NA

AllHPE$medErr < NA

AllHPE$maxErr <- NA

AllHPE$P90 <- NA

AllHPE$P99 <- NA

AllHPE$P95 <- NA

badpts <- length(estPosRef$HPEm[estPosRef$HPEm > accGoal]) # Number of all points that were unacceptably erroneous

badptsP <-(length(estPosRef$HPEm[estPosRef$HPEm > accGoal])/(length(estPosRef$HPEm)))*100 # % of all points that were unacceptably erroneous

goodpts <- length(estPosRef$HPEm[estPosRef$HPEm <= accGoal]) #Number of all points that are good

goodptsP <- (length(estPosRef$HPEm[estPosRef$HPEm <= accGoal])/(length(estPosRef$HPEm)))*100 # % of all points that are good

# 2. Calculate summary data in the `AllHPE` data frame ####

for(i in 1:length(AllHPE$HPE)) {

  AllHPE$ptsReject[i] <- length(which(estPosRef$HPE > AllHPE$HPE[i]))
  
  AllHPE$ptsRejectP[i] <- length(which(estPosRef$HPE > AllHPE$HPE[i]))/(length(estPosRef$HPEm))*100 #% of all points dropped

  AllHPE$ptsRetain[i] <- length(which(estPosRef$HPE < AllHPE$HPE[i]))
  
  AllHPE$ptsRetainP[i] <- length(which(estPosRef$HPE < AllHPE$HPE[i]))/(length(estPosRef$HPEm))*100 # % of all points retained
  
  AllHPE$incorrectReject[i] <- length(which((estPosRef$HPE[estPosRef$HPEm <= accGoal] > AllHPE$HPE[i]) == TRUE)) # incorrect reject
  
  AllHPE$incorrectRejectP[i] <- (AllHPE$incorrectReject[i]/length(estPosRef$HPEm[estPosRef$HPEm <= accGoal]))*100 # divided by total with acceptable error
  
  AllHPE$incorrectRetain[i] <- length(which((estPosRef$HPE[estPosRef$HPEm > accGoal] < AllHPE$HPE[i]) == TRUE)) # incorrect retain
  
  AllHPE$incorrectRetainP[i] <- (AllHPE$incorrectRetain[i]/length(estPosRef$HPEm[estPosRef$HPEm > accGoal ]))*100 # divided by accurate points
  
  AllHPE$incorrectRetainPvsRetain[i] <- (AllHPE$incorrectRetain[i]/AllHPE$ptsRetain[i])*100 # divided by total retained by filter
 
  AllHPE$correctReject[i] <- (length(which((estPosRef$HPE[estPosRef$HPEm > accGoal] > AllHPE$HPE[i]) == TRUE))) # Correctly Rejected
  
  AllHPE$correctRejectP[i] <- (AllHPE$correctReject[i]/AllHPE$ptsRetain[i])*100 # divided by sum of all positions
  
  AllHPE$correctRetain[i] <- length(which((estPosRef$HPE[estPosRef$HPEm < accGoal] < AllHPE$HPE[i]) == TRUE)) # Correctly Retained
  
  AllHPE$correctRetainP[i] <- (AllHPE$correctRetain[i]/length(estPosRef$HPEm))*100 # divided by sum of all positions
  
  AllHPE$goodDataLossP[i] <- (AllHPE$incorrectReject[i]/goodpts)*100 # incorrectReject/goodpts *100
  
  AllHPE$badDataRetainP[i] <- (AllHPE$incorrectRetain[i]/AllHPE$ptsRetain[i])*100 # incorrectRetain/AllHPE$ptsRetain * 100
  
  AllHPE$avgErr[i] <- mean(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]]) # mean of all error for each HPE
  
  AllHPE$sdDEV [i] <- sd(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]]) # Standard deviation of all error for each HPE cut off
  
  AllHPE$medErr[i] <- median(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]]) # median of all error for each HPE cut off

  AllHPE$maxErr[i] <- max(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]]) # max of all error for each HPE cut off
  
  AllHPE$P99[i] <- quantile(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]], probs = 0.99) # The 99th percentile of all error for each HPE cut off 
  
  AllHPE$P90[i] <- quantile(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]], probs = 0.9) # The 90th percentile of all error for each HPE cut off
  
  AllHPE$P95[i] <- quantile(estPosRef$HPEm[estPosRef$HPE < AllHPE$HPE[i]], probs = 0.95) # The 95th percentile of all error for each HPE cut off 
  
  AllHPE$count15[i] <- length(which((estPosRef$HPE[estPosRef$HPEm > 20] < AllHPE$HPE[i]) == TRUE))
}

# 3. Establish a relationship between incorrectly rejected data and incorrectly retained data ####

start <- 1
end <- 15 

AllHPEperf1 <- data.frame(HPE = AllHPE$HPE[start:end],
                          perc = AllHPE$correctRejectP[start:end])

AllHPEperf1$var <- "Correct Rejection"

AllHPEperf2 <- data.frame(HPE = AllHPE$HPE[start:end],
                          perc = AllHPE$incorrectRetainP[start:end])

AllHPEperf2$var <- "Incorrect Retainment"

AllHPEperf3 <- data.frame(HPE = AllHPE$HPE[start:end],
                          perc = AllHPE$incorrectRejectP[start:end])

AllHPEperf3$var <-"Incorrect Rejection"

# Combine all objects above for plotting
AllHPEperf <- rbind(AllHPEperf1,AllHPEperf3,AllHPEperf2) # Note that legend is alphabetical but graph is by data frame order


# 4. Plot data loss vs. error retention for the desired accuracy goal ####

AllHPE %>%
   dplyr::filter(HPE < 21) %>%
   ggplot() +
    geom_point(aes(x = incorrectRetainPvsRetain, y = incorrectRejectP), shape = 20, size = 3) + 
    geom_hline(yintercept = 5, linetype = "dashed", colour = "red", size = 0.5) +
    geom_vline(xintercept = 1, linetype = "dashed", colour = "red", size = 0.5) +
    ggrepel::geom_text_repel(cex = 5, aes(x = incorrectRetainPvsRetain, y = incorrectRejectP, label = HPE, group = NULL), 
                             nudge_x = 0.05, nudge_y = 0, max.iter = 50000) +
    xlab("\n% Incorrecly Retained\n(of all retained positions)") +
    ylab("% Incorrectly Rejected\n(of all acceptable positions)\n") +
   ggtitle("2015") +
    theme_bw() + 
    theme(text = element_text(size = 14, colour = "black"),
          axis.text = element_text(colour = "black", size = 14),
        plot.title = element_text(vjust = 2),
        axis.text.y = element_text(angle = 0, hjust = 0),
        axis.title.y = element_text(vjust = 0.3),
        axis.title.x = element_text(vjust = -0.02 ),
        plot.margin = unit(c(1,1.5,1,1.5), "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = c(1.5, 0.8),
        legend.background = element_rect(fill = "transparent", colour = NA),
        legend.title = element_blank(),
        panel.border = element_rect(colour = "black", linewidth = 1))

# 5. Plot mean error, max error, and number of unacceptable high errors for each HPE cut off ####

AllHPE %>%
   dplyr::filter(HPE < 21 & HPE > 6) %>%
   ggplot() + 
   geom_hline(yintercept = 0.90, linetype = "dashed", colour = "red") +
     geom_hline(yintercept = 5, linetype = "solid", colour = "blue") +
     geom_point(aes(x = HPE, y = avgErr, shape = "Mean error"), size = 3, color = 'black') +
     geom_point(aes(x = HPE, y = P99, shape = "99th percentile"), size = 3, colour = "black") +
     geom_point(aes(x = HPE, y = P90, shape = "90th percentile"), size = 3, colour = "black") +
     geom_point(aes(x = HPE, y = P95, shape = "95th percentile"), size = 3, colour = "black") +
     
     xlab("\nHPE") + 
     ylab("Measured error (m)\n") +
   
   ggtitle("2015") +
   
     scale_shape_manual(values = c(18,17,16,15)) +
   
     scale_x_continuous("HPE", breaks = round(seq(min(HPE), max(HPE), by = 1),1)) +
     theme_bw() + 
     theme(text = element_text(size = 14),
        plot.title = element_text(vjust = 2),
        axis.text = element_text(colour = "black", size = 14),
        axis.text.y = element_text(angle = 0, hjust = 0),
        axis.title.y = element_text(vjust = -0.02 ),
        axis.title.x = element_text(vjust = -0.02 ),
        plot.margin = unit(c(0.2,0.2,0.2,0.2), "cm"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "top",
        panel.border = element_rect(colour = "black", linewidth = 1),
        legend.background = element_rect(fill = "transparent", colour = NA),
        legend.title = element_blank())
```

```{r Filter_VPS_data_2015, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Filter bull trout VPS positions based on the start and end dates for the study ####

# Smolt outmigration data are not available for 2015; filter based on bltr positions

# Count the number of positions that were estimated for each fish prior to filtering
foo <- bltr.positions.15 %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::summarise(n = length(Fish_ID))

remove(foo)

# 2. Discard bull trout positions that are located on land ####

# 61,897 --> 61,229 

# Load the study area polygon
study_area_UTM <- sf::read_sf(dsn = "GIS/", layer = "Chilko_Outlet_UTM_VPS_Study")

# Convert the `bltr.positions.15` data frame to a sf object
pos_sf <- sf::st_as_sf(bltr.positions.15, coords = c(9:10))
st_crs(pos_sf) <- 32610

# Separate the VPS positions into two data sets based on whether the positions are in water or on land
X <- sapply(st_intersects(pos_sf, study_area_UTM), function(x){length(x)==0})
X <- as_tibble(X)

# Combine the `X` column w/ bltr.positions.15, and filter positions that are on land (value = TRUE)
bltr.positions.15 <- bltr.positions.15 %>%
  dplyr::bind_cols(X)

bltr.positions.15 <- bltr.positions.15 %>%
  dplyr::filter(value == FALSE)


# Count the number of positions that were estimated for each fish after filtering on-land positions
foo <- bltr.positions.15 %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::summarise(n = length(Fish_ID))


foo %>% 
  dplyr::summarise(min = min(n),
                   max = max(n),
                   mean = mean(n),
                   sd = sd(n))

# 3. Filter bull trout VPS positions based on HPE 14 ####

bltr.positions.15 <- bltr.positions.15 %>%
  dplyr::filter(HPE <= 14) # 61,229 --> 53,239

# 4. Discard columns from the bull trout positions data that are not needed for further analyses ####

bltr.positions.15 <- bltr.positions.15 %>%
  dplyr::select(Fish_ID, AcousticID, DateTime, DateTimeUTC, Lat, Long, Easting, Northing, HPE)

# Save filtered data

#saveRDS(bltr.positions.15, file = "DATA/bltr_positions_filtered_2015.rds")

```

```{r Clean_up_working_environment, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Keep only these objects
#keep <- c("bltr.data.14", "bltr.data.15", "bltr.positions.14", "bltr.positions.15", "my_theme", "receiver.locations.14", "receiver.locations.15", "receivers.14", "receivers.15", "ref.tag.locations.14", "ref.tag.locations.15", "reference.tag.positions.14", "reference.tag.positions.15", "smolt.density", "solar.data.14", "solar.data.15")

#rm(list = setdiff(ls(), keep))

#gc()

# Save all objects in the global environment to an RDS file
#processed_datasets <- mget(ls(), envir = .GlobalEnv)

#saveRDS(processed_datasets, "DATA/processed_datasets.rds")

```


# Step 2. Calculate movement metrics

Movement metrics are calculated separately for 2014 and 2015.

If movement metrics are being calculated without first running 'Step 1. Prepare datasets', the code chunk named 'load_processed_data' must be run before any other code chunks in this step. 

Note that calculating UDs as AKDEs is computationally demanding, and may take several hours to estimates UDs if being done with a conventional computer. However, in the code chunks named 'AKDE_downstream', 'AKDE_upstream', and 'AKDE_2015', ctmm telemetry list objects (i.e., ctmm_telemetry_list_DS, ctmm_telemetry_list_US, and ctmm_telemetry_list_15, respectively) can be subsetted, and UDs can be estimated via AKDE for a small number of bull trout and time slot combinations.

Calculating the shortest-path distances between sequential VPS positions can also be computationally demanding, though not as demanding as AKDE estimation.

```{r Load_processed_data, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Load processed data

processed_datasets <- readRDS("DATA/processed_datasets.rds")

list2env(processed_datasets, envir = .GlobalEnv)

remove(processed_datasets)

```

```{r Study_area_rasters, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Create rasters of the study area for mapping
# Separate rasters are needed for US and DS
# Using this raster as a grid for wAKDE estimation will mute an error message, but it will also slow down the processing speed and create much larger AKDE outputs.

# Create a spatial data frame for DS and for US
# Remove hydrophones that are not in the respective DS or US areas to avoid an error while creating the rasters

# A. Entire study area raster and spatial file

spatial <- receiver.locations.14 %>%
  dplyr::rename(Station.name = Name, Easting = meanEasting, Northing = meanNorthing) %>%
  dplyr::mutate(Section = "Outlet",
                Array = Station.name,
                Type = "Hydrophone") 


write_csv(spatial, "DATA/temp_data/spatial.csv")


# Create the raster from the outlet shapefile
outlet.raster <- actel::shapeToRaster(shape = "GIS/Chilko_Outlet_UTM_VPS_Study.shp", size = 0.5, spatial = "DATA/temp_data/spatial.csv", coord.x = "Easting", coord.y = "Northing", buffer = 0, type = "water")


# B. Downstream raster and spatial file

# The receivers that were located DS of the fence are R34, R35, R36, and R37.
spatial_DS <- spatial %>%
  dplyr::filter(Station.name == "R34" | Station.name == "R35" | Station.name == "R36" | Station.name == "R37")

write_csv(spatial_DS, "DATA/temp_data/spatial.csv")

# Create the raster from the outlet shapefile
outlet.raster.DS <- actel::shapeToRaster(shape = "GIS/Chilko_Outlet_UTM_VPS_downstream.shp", size = 0.5, spatial = "DATA/temp_data/spatial.csv", coord.x = "Easting", coord.y = "Northing", buffer = 0, type = "water")


# C. Upstream raster and spatial file

# The receivers that were located US of the fence did not include R34, R35, R36, or R37.
spatial_US <- spatial %>%
  dplyr::filter(Station.name != "R34" & Station.name != "R35" & Station.name != "R36" & Station.name != "R37")

write_csv(spatial_US, "DATA/temp_data/spatial.csv")

# Create the raster from the outlet shapefile
outlet.raster.US <- actel::shapeToRaster(shape = "GIS/Chilko_Outlet_UTM_VPS_upstream.shp", size = 0.5, spatial = "DATA/temp_data/spatial.csv", coord.x = "Easting", coord.y = "Northing", buffer = 0, type = "water")


# D. View the rasters
raster::plot(outlet.raster)
raster::plot(outlet.raster.DS)
raster::plot(outlet.raster.US)

```

## Movement metrics for 2014

Utilization distributions are calculated separately for bull trout that were downstream of the smolt fence in 2014 and those that were upstream because the models used to estimate AKDEs incorporate a shapefile of the study area over which UDs can be estimated, and we're assuming UDs cannot extend past the fence in either direction. 

```{r Assign_bltr_to_DS_or_US_side_of_the_fence, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Check that all data sets are properly formatted ####

str(bltr.positions.14)

lubridate::tz(bltr.positions.14$DateTime)

str(bltr.data.14)

lubridate::tz(bltr.data.14$ReleaseDateTime)

str(smolt.density)

lubridate::tz(smolt.density$DateTime)

lubridate::tz(smolt.density$Date)

str(solar.data.14)

lubridate::tz(solar.data.14$Date)
lubridate::tz(solar.data.14$sunrise)
lubridate::tz(solar.data.14$sunset)

# Convert the Date column in `smolt.density` from UTC to Canada/Pacific
smolt.density <- smolt.density %>%
  dplyr::mutate(Date = lubridate::force_tz(Date, tzone = "Canada/Pacific"))

# Set the time zone for the Date column in `solar.data.14` to Canada/Pacific
solar.data.14 <- solar.data.14 %>%
  dplyr::mutate(Date = lubridate::force_tz(Date, tzone = "Canada/Pacific")) %>%
  dplyr::select(Date, sunrise, sunset)

lubridate::tz(solar.data.14$Date)
lubridate::tz(smolt.density$DateTime)
lubridate::tz(smolt.density$Date)

# 2. Create lists of fish IDs and time slot IDs ####

# Create a list with names of combined fish IDs and time slot IDs; use as reference for naming the objects in the list of ctmm telemetry objects
# There's definitely a cleaner way to do this (e.g., with an interval join)

bltr.positions.14 <- bltr.positions.14 %>%
  dplyr::rename(timestamp = DateTime) %>%
  dplyr::mutate(TOD = case_when(timestamp < "2014-04-27 05:56:29" ~ "N1",
                                timestamp >= "2014-04-27 05:56:29" & timestamp < "2014-04-27 20:34:12" ~ "D1",
                                timestamp >= "2014-04-27 20:34:12" & timestamp < "2014-04-28 05:54:32" ~ "N2",
                                timestamp >= "2014-04-28 05:54:32" & timestamp < "2014-04-28 20:35:53" ~ "D2",
                                timestamp >= "2014-04-28 20:35:53" & timestamp < "2014-04-29 05:52:37" ~ "N3",
                                timestamp >= "2014-04-29 05:52:37" & timestamp < "2014-04-29 20:37:33" ~ "D3",
                                timestamp >= "2014-04-29 20:37:33" & timestamp < "2014-04-30 05:50:43" ~ "N4",
                                timestamp >= "2014-04-30 05:50:43" & timestamp < "2014-04-30 20:39:13" ~ "D4",
                                timestamp >= "2014-04-30 20:39:13" & timestamp < "2014-05-01 05:48:50" ~ "N5",
                                timestamp >= "2014-05-01 05:48:50" & timestamp < "2014-05-01 20:40:53" ~ "D5",
                                timestamp >= "2014-05-01 20:40:53" & timestamp < "2014-05-02 05:46:59" ~ "N6",
                                timestamp >= "2014-05-02 05:46:59" & timestamp < "2014-05-02 20:42:32" ~ "D6",
                                timestamp >= "2014-05-02 20:42:32" & timestamp < "2014-05-03 05:45:09" ~ "N7",
                                timestamp >= "2014-05-03 05:45:09" & timestamp < "2014-05-03 20:44:11" ~ "D7",
                                timestamp >= "2014-05-03 20:44:11" & timestamp < "2014-05-04 05:43:20" ~ "N8",
                                timestamp >= "2014-05-04 05:43:20" & timestamp < "2014-05-04 20:45:50" ~ "D8",
                                timestamp >= "2014-05-04 20:45:50" & timestamp < "2014-05-05 05:41:33" ~ "N9",
                                timestamp >= "2014-05-05 05:41:33" & timestamp < "2014-05-05 20:47:28" ~ "D9",
                                timestamp >= "2014-05-05 20:47:28" & timestamp < "2014-05-06 05:39:48" ~ "N10",
                                timestamp >= "2014-05-06 05:39:48" & timestamp < "2014-05-06 20:49:06" ~ "D10",
                                timestamp >= "2014-05-06 20:49:06" & timestamp < "2014-05-07 05:38:04" ~ "N11",
                                timestamp >= "2014-05-07 05:38:04" & timestamp < "2014-05-07 20:50:44" ~ "D11",
                                timestamp >= "2014-05-07 20:50:44" & timestamp < "2014-05-08 05:36:22" ~ "N12",
                                timestamp >= "2014-05-08 05:36:22" & timestamp < "2014-05-08 20:52:21" ~ "D12",
                                timestamp >= "2014-05-08 20:52:21" & timestamp < "2014-05-09 05:34:42" ~ "N13",
                                timestamp >= "2014-05-09 05:34:42" & timestamp < "2014-05-09 20:53:57" ~ "D13",
                                timestamp >= "2014-05-09 20:53:57" & timestamp < "2014-05-10 05:33:03" ~ "N14",
                                timestamp >= "2014-05-10 05:33:03" & timestamp < "2014-05-10 20:55:33" ~ "D14",
                                timestamp >= "2014-05-10 20:55:33" & timestamp < "2014-05-11 05:31:26" ~ "N15",
                                timestamp >= "2014-05-11 05:31:26" & timestamp < "2014-05-11 20:57:08" ~ "D15",
                                timestamp >= "2014-05-11 20:57:08" & timestamp < "2014-05-12 05:29:51" ~ "N16",
                                timestamp >= "2014-05-12 05:29:51" & timestamp < "2014-05-12 20:58:43" ~ "D16",
                                timestamp >= "2014-05-12 20:58:43" & timestamp < "2014-05-13 05:28:18" ~ "N17",
                                timestamp >= "2014-05-13 05:28:18" & timestamp < "2014-05-13 21:00:16" ~ "D17",
                                timestamp >= "2014-05-13 21:00:16" & timestamp < "2014-05-14 05:26:46" ~ "N18",
                                timestamp >= "2014-05-14 05:26:46" & timestamp < "2014-05-14 21:01:49" ~ "D18",
                                timestamp >= "2014-05-14 21:01:49" & timestamp < "2014-05-15 05:25:17" ~ "N19",
                                timestamp >= "2014-05-15 05:25:17" & timestamp < "2014-05-15 21:03:21" ~ "D19",
                                timestamp >= "2014-05-15 21:03:21" & timestamp < "2014-05-16 05:23:50" ~ "N20",
                                timestamp >= "2014-05-16 05:23:50" & timestamp < "2014-05-16 21:04:52" ~ "D20",
                                timestamp >= "2014-05-16 21:04:52" & timestamp < "2014-05-17 05:22:25" ~ "N21",
                                timestamp >= "2014-05-17 05:22:25" & timestamp < "2014-05-17 21:06:22" ~ "D21",
                                timestamp >= "2014-05-17 21:06:22" & timestamp < "2014-05-18 05:21:02" ~ "N22",
                                timestamp >= "2014-05-18 05:21:02" & timestamp < "2014-05-18 21:07:51" ~ "D22",
                                timestamp >= "2014-05-18 21:07:51" & timestamp < "2014-05-19 05:19:41" ~ "N23",
                                timestamp >= "2014-05-19 05:19:41" & timestamp < "2014-05-19 21:09:18" ~ "D23"))




# 3. Determine if each fish is DS or US of the fence ####

# Rather than discard positions that are US of the fence for DS fish and vice versa, we will snap positions to the closest position on the side of the fence they should be located, as determined by the side with the majority of their VPS postions.

# To do this, first identify which fish are primarily DS and which are primarily US.

# Load the buffered DS and US polygons.
study_area_UTM_DS <- sf::read_sf(dsn = "GIS/", layer = "Chilko_Outlet_UTM_VPS_downstream")
study_area_UTM_US <- sf::read_sf(dsn = "GIS/", layer = "Chilko_Outlet_UTM_VPS_upstream")

# Convert the `bltr.positions` data frame to a sf object
pos_sf <- sf::st_as_sf(bltr.positions.14, coords = c(7:8))
st_crs(pos_sf) <- 32610

# Separate the bltr VPS positions into two data sets based on whether the positions are downstream or upstream of the smolt fence.
pos_DS <- sapply(st_intersects(pos_sf, study_area_UTM_DS), function(x){length(x)==0})
pos_DS <- as_tibble(pos_DS)

pos_US <- sapply(st_intersects(pos_sf, study_area_UTM_US), function(x){length(x)==0})
pos_US <- as_tibble(pos_US)

# Combine the pos_DS and pos_US columns w/ bltr.positions
bltr.positions.DS <- bltr.positions.14 %>%
  dplyr::bind_cols(pos_DS) 

bltr.positions.US <- bltr.positions.14 %>%
  dplyr::bind_cols(pos_US) 

# Filter positions that are outside of the respective polygons (value = TRUE)
bltr.positions.DS <- bltr.positions.DS %>%
  dplyr::filter(value == FALSE)

bltr.positions.US <- bltr.positions.US %>%
  dplyr::filter(value == FALSE)

# Summarize the number of DS and US positions for each fish
summary.pos.DS <- bltr.positions.DS %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::summarise(n = length(Fish_ID))
  
summary.pos.US <- bltr.positions.US %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::summarise(n = length(Fish_ID))

summary.pos <- summary.pos.US %>%
  dplyr::left_join(summary.pos.DS, by = "Fish_ID") %>%
  dplyr::mutate(US = n.x,
                DS = n.y) %>%
  dplyr::select(Fish_ID, DS, US) %>%
  dplyr::mutate(DS = if_else(is.na(DS), 0, DS)) %>%
  dplyr::mutate(total = US + DS) %>%
  dplyr::mutate(DS.percent = DS/total,
                US.percent = US/total)

# If Fish ID is one of BT02, BT04, BT08, or BT10, snap US positions to DS shapefile.
# For all other fish, snap DS positions to US shapefile.

# US positions that need to be snapped to the DS shapefile.
bltr.positions.US.to.DS <- bltr.positions.US %>%
  dplyr::filter(Fish_ID == "BT02" | Fish_ID == "BT04" | Fish_ID == "BT08" | Fish_ID == "BT10")

# DS positions that need to be snapped to the US shapefile.
bltr.positions.DS.to.US <- bltr.positions.DS %>%
  dplyr::filter(Fish_ID != "BT02" & Fish_ID != "BT04" & Fish_ID != "BT08" & Fish_ID != "BT10")

# DS positions that are ok.
bltr.positions.DS <- bltr.positions.DS %>%
  dplyr::mutate(fence_side = "DS") %>%
  dplyr::filter(Fish_ID == "BT02" | Fish_ID == "BT04" | Fish_ID == "BT08" | Fish_ID == "BT10")

# US positions that are ok.
bltr.positions.US <- bltr.positions.US %>%
  dplyr::mutate(fence_side = "US") %>%
  dplyr::filter(Fish_ID != "BT02" & Fish_ID != "BT04" & Fish_ID != "BT08" & Fish_ID != "BT10")

# 4. Snap positions to proper side of fence ####

# Read in buffered shapefiles for DS and US outlet areas 
# These shapefiles have been buffered by -1 m relative to the original shapefile for the outlet
# The negative buffer ensures points are snapped to water
study_area_UTM_DS <- sf::read_sf(dsn = "GIS/", layer = "Chilko_Outlet_UTM_VPS_downstream_negative_buffer")
study_area_UTM_US <- sf::read_sf(dsn = "GIS/", layer = "Chilko_Outlet_UTM_VPS_upstream_negative_buffer")

# Convert the points tibbles to sf objects
US.to.DS <- sf::st_as_sf(bltr.positions.US.to.DS, coords = c(7:8))
st_crs(US.to.DS) <- 32610

DS.to.US <- sf::st_as_sf(bltr.positions.DS.to.US, coords = c(7:8))
st_crs(DS.to.US) <- 32610

# Plot the positions over a map of the buffered study areas (DS and US)
# Notice that all the positions are immediately adjacent to the fence
ggplot() +
  geom_sf(data = study_area_UTM_DS, stat = "sf", position = "identity") +
  geom_sf(data = US.to.DS, size = 0.2) +
  coord_sf(datum = st_crs(study_area_UTM_DS))

ggplot() +
  geom_sf(data = study_area_UTM_US, stat = "sf", position = "identity") +
  geom_sf(data = DS.to.US, size = 0.2) +
  coord_sf(datum = st_crs(study_area_UTM_US))

# Snap the points to the side of the fence to which they belong.

# A. US points to the DS shapefile

# Find the LINESTRING object which is the shortest line between the two geometries
st_nearest_points(US.to.DS, study_area_UTM_DS) %>% 
  {. ->> my_linestring}

my_linestring

# Determine the closest point on the boundary for each point in `US.to.DS`
US.to.DS %>% 
  mutate(
    my_linestring = st_nearest_points(geometry, study_area_UTM_DS),
    closest_point = st_cast(my_linestring, 'POINT')[seq(2, nrow(.)*2, 2)],
  ) %>% 
  {. ->> closest_points}

closest_points

# Visualize the results 
ggplot() +
  geom_sf(data = study_area_UTM_DS) +
  geom_sf(data = US.to.DS, shape = 1, colour = "red") + 
  geom_sf(data = closest_points$my_linestring, linetype = "solid") +
  geom_sf(data = closest_points$closest_point, shape = 1, colour = "blue") +
  coord_sf(datum = st_crs(study_area_UTM_DS))

# Isolate the snapped points in a new data set that can be joined w/ the other US positions
snapped.US <- as_tibble(closest_points)

snapped.US <- snapped.US %>%
  dplyr::mutate(closest_point = as.character(closest_point)) %>%
  dplyr::mutate(geometry = as.character(geometry)) %>%
  tidyr::separate(closest_point, into = c("x", "y"), sep = " ") %>%
  tidyr::separate(x, into = c("foo1", "Easting_snapped"), sep = c(2,10)) %>%
  tidyr::separate(y, into = c("Northing_snapped", "foo2"), sep = 7) %>%
  dplyr::select(Fish_ID, timestamp, Easting_snapped, Northing_snapped) %>%
  dplyr::mutate(Easting_snapped = as.double(Easting_snapped),
                Northing_snapped = as.double(Northing_snapped))

# Join snapped positions w/ non snapped positions and summarize the euclidean distance each position was moved.
foo <- bltr.positions.US.to.DS %>%
  dplyr::left_join(snapped.US, by = c("Fish_ID" = "Fish_ID", "timestamp" = "timestamp"))

snapped.US <- foo %>%
  dplyr::mutate(distance_snapped = sqrt(((Easting - Easting_snapped)^2) + ((Northing - Northing_snapped)^2)))



# B. DS points to the US shapefile

# Find the LINESTRING object which is the shortest line between the two geometries
st_nearest_points(DS.to.US, study_area_UTM_US) %>% 
  {. ->> my_linestring}

my_linestring

# Determine the closest point on the boundary for each point in `US.to.DS`
DS.to.US %>% 
  mutate(
    my_linestring = st_nearest_points(geometry, study_area_UTM_US),
    closest_point = st_cast(my_linestring, 'POINT')[seq(2, nrow(.)*2, 2)],
  ) %>% 
  {. ->> closest_points}

closest_points

# Visualize the results 
ggplot() +
  geom_sf(data = study_area_UTM_US) +
  geom_sf(data = DS.to.US, shape = 1, colour = "red") + 
  geom_sf(data = closest_points$my_linestring, linetype = "solid") +
  geom_sf(data = closest_points$closest_point, shape = 1, colour = "blue") +
  coord_sf(datum = st_crs(study_area_UTM_US))

# Isolate the snapped points in a new data set that can be joined w/ the other US positions
snapped.DS <- as.tibble(closest_points)

snapped.DS <- snapped.DS %>%
  dplyr::mutate(closest_point = as.character(closest_point)) %>%
  dplyr::mutate(geometry = as.character(geometry)) %>%
  tidyr::separate(closest_point, into = c("x", "y"), sep = " ") %>%
  tidyr::separate(x, into = c("foo1", "Easting_snapped"), sep = c(2,10)) %>%
  tidyr::separate(y, into = c("Northing_snapped", "foo2"), sep = 7) %>%
  dplyr::select(Fish_ID, timestamp, Easting_snapped, Northing_snapped) %>%
  dplyr::mutate(Easting_snapped = as.double(Easting_snapped),
                Northing_snapped = as.double(Northing_snapped))

# Join snapped positions w/ non snapped positions and summarize the euclidean distance each position was moved.
foo <- bltr.positions.DS.to.US %>%
  dplyr::left_join(snapped.DS, by = c("Fish_ID" = "Fish_ID", "timestamp" = "timestamp"))

snapped.DS <- foo %>%
  dplyr::mutate(distance_snapped = sqrt(((Easting - Easting_snapped)^2) + ((Northing - Northing_snapped)^2)))



# C. Determine the range of distances that positions were snapped to other side of fence

# Combine the DS and US snapped positions in a temp file
snapped.DS %>%
  dplyr::bind_rows(snapped.US) %>%
  dplyr::summarise(min = min(distance_snapped),
                   max = max(distance_snapped),
                   mean = mean(distance_snapped),
                   sd = sd(distance_snapped))

# So, 397 positions were moved from DS to US, and 162 positions were moved from US to DS.
# The range of Euclidean distances that these points were moved ranged from 0.41 m to 7.96 m
# Mean ± SD = 2.04 ± 0.96 m


# D. Create bltr position data sets for AKDE estimation (one for US and one for DS)

snapped.DS <- snapped.DS %>%
  dplyr::select(-c(Easting, Northing, distance_snapped, value)) %>%
  dplyr::rename("Easting" = "Easting_snapped",
                "Northing" = "Northing_snapped") %>%
  dplyr::relocate(Easting, .after = Long) %>%
  dplyr::relocate(Northing, .after = Easting)

snapped.US <- snapped.US %>%
  dplyr::select(-c(Easting, Northing, distance_snapped, value)) %>%
  dplyr::rename("Easting" = "Easting_snapped",
                "Northing" = "Northing_snapped") %>%
  dplyr::relocate(Easting, .after = Long) %>%
  dplyr::relocate(Northing, .after = Easting)

bltr.positions.DS <- bltr.positions.DS %>%
  dplyr::bind_rows(snapped.US) %>%
  dplyr::arrange(Fish_ID, timestamp) %>%
  dplyr::select(-c(value, fence_side))

bltr.positions.US <- bltr.positions.US %>%
  dplyr::bind_rows(snapped.DS) %>%
  dplyr::arrange(Fish_ID, timestamp) %>%
  dplyr::select(-c(value, fence_side))


# 5. Clean up global environment ####

remove(bltr.positions.DS.to.US, bltr.positions.US.to.DS, closest_points, DS.to.US, foo, my_linestring, pos_DS,
       pos_sf, pos_US, snapped.DS, snapped.US, study_area_UTM_DS, study_area_UTM_US, summary.pos,
       summary.pos.DS, summary.pos.US, US.to.DS)

gc()

```

```{r Set_up_data_for_AKDE_estimation, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Create ctmm_ID_list objects for DS and US AKDE estimation ####

ctmm_ID_list_DS <- bltr.positions.DS

ctmm_ID_list_US <- bltr.positions.US 

# 2. Filter time slots with an insufficient number of positions for UD estimation ####

# For each fish, remove any time slots where the fish was detected only three or fewer times (i.e., < 4 VPS positions exist)
# The choice to set 4 VPS positions as the required threshold was arbitrary

ctmm_ID_list_DS <- ctmm_ID_list_DS %>% 
  dplyr::group_by(Fish_ID, TOD) %>% 
  dplyr::mutate(TODn = length(TOD)) %>% 
  dplyr::filter(TODn > 3) %>% 
  dplyr::select(-TODn) %>% 
  dplyr::ungroup()

ctmm_ID_list_US <- ctmm_ID_list_US %>% 
  dplyr::group_by(Fish_ID, TOD) %>% 
  dplyr::mutate(TODn = length(TOD)) %>% 
  dplyr::filter(TODn > 3) %>% 
  dplyr::select(-TODn) %>% 
  dplyr::ungroup()

# 3. Create lists of Fish IDs and time slots ####
  
# Arrange by Fish ID and time slot ID to match the order that will be produced in the bltr_list  
ctmm_ID_list_DS <- ctmm_ID_list_DS %>% 
  dplyr::select(Fish_ID, TOD) %>% 
  dplyr::arrange(Fish_ID, TOD) %>% 
  dplyr::distinct() %>% 
  tidyr::unite(ctmm_ID, Fish_ID, TOD, sep = "_", remove = TRUE)

ctmm_ID_list_US <- ctmm_ID_list_US %>%
  dplyr::select(Fish_ID, TOD) %>%
  dplyr::arrange(Fish_ID, TOD) %>%
  dplyr::distinct() %>%
  tidyr::unite(ctmm_ID, Fish_ID, TOD, sep = "_", remove = TRUE)


ctmm_ID_list_DS <- as.list(ctmm_ID_list_DS$ctmm_ID)

ctmm_ID_list_US <- as.list(ctmm_ID_list_US$ctmm_ID)

names(ctmm_ID_list_DS) <- ctmm_ID_list_DS

names(ctmm_ID_list_US) <- ctmm_ID_list_US

# 4. Separate the bltr_list into two lists based on the DS or US designation for each fish ####

# This won't be as simple as just filtering the list based on fish IDs
# We also need to remove the positions for DS and US fish that are on the opposite side of the fence.
# Might be easier to just re-create the bltr_list, but make separate lists for DS and US.

# Create a list of bull trout positions for each of the DS and US data sets

# DS
bltr_list_DS <- split(bltr.positions.DS, f = bltr.positions.DS$Fish_ID)
bltr_list_DS <- purrr::map(bltr_list_DS, tibble::as_tibble)
list2env(bltr_list_DS, envir = .GlobalEnv)
remove(bltr_list_DS)

# US
bltr_list_US <- split(bltr.positions.US, f = bltr.positions.US$Fish_ID)
bltr_list_US <- purrr::map(bltr_list_US, tibble::as_tibble)
list2env(bltr_list_US, envir = .GlobalEnv)
remove(bltr_list_US)

# Convert individual data frames into lists
BT02 <- list("bltr.positions.DS" = BT02)
BT03 <- list("bltr.positions.US" = BT03)
BT04 <- list("bltr.positions.DS" = BT04)
BT05 <- list("bltr.positions.US" = BT05)
BT06 <- list("bltr.positions.US" = BT06)
BT07 <- list("bltr.positions.US" = BT07)
BT08 <- list("bltr.positions.DS" = BT08)
BT09 <- list("bltr.positions.US" = BT09)
BT10 <- list("bltr.positions.DS" = BT10)
BT11 <- list("bltr.positions.US" = BT11)
BT12 <- list("bltr.positions.US" = BT12)
BT13 <- list("bltr.positions.US" = BT13)
BT14 <- list("bltr.positions.US" = BT14)
BT15 <- list("bltr.positions.US" = BT15)
BT16 <- list("bltr.positions.US" = BT16)
BT17 <- list("bltr.positions.US" = BT17)
BT18 <- list("bltr.positions.US" = BT18)
BT19 <- list("bltr.positions.US" = BT19)
BT20 <- list("bltr.positions.US" = BT20)
BT21 <- list("bltr.positions.US" = BT21)
BT22 <- list("bltr.positions.US" = BT22)
BT23 <- list("bltr.positions.US" = BT23)
BT24 <- list("bltr.positions.US" = BT24)
BT25 <- list("bltr.positions.US" = BT25)

# Combine lists into a single nested list, depending on DS or US designation
bltr_list_DS <- list("BT02" = BT02, "BT04" = BT04, "BT08" = BT08, "BT10" = BT10)

bltr_list_US <- list("BT03" = BT03, "BT05" = BT05, "BT06" = BT06, "BT07" = BT07, "BT09" = BT09, "BT11" = BT11, "BT12" = BT12, "BT13" = BT13, "BT14" = BT14, "BT15" = BT15, "BT16" = BT16, "BT17" = BT17, "BT18" = BT18, "BT19" = BT19, "BT20" = BT20, "BT21" = BT21, "BT22" = BT22, "BT23" = BT23, "BT24" = BT24,"BT25" = BT25)

# Remove individual bull trout lists
remove(BT02, BT03, BT04, BT05, BT06, BT07, BT08, BT09, BT10, BT11, BT12, BT13, BT14, BT15, BT16, BT17, BT18, BT19, BT20, BT21, BT22, BT23, BT24, BT25)

```

```{r AKDE_downstream, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Create ctmm telemetry data sets ####

# Create a ctmm telemetry data set for each time slot of interest (e.g., Night 1, Day 1,... Day 23), for each fish 
for(i in seq_along(bltr_list_DS)){
  
  # 1. Analysis of data with the ctmm package requires that animal relocation data conforms to Movebank naming conventions
  ctmm.data <- bltr_list_DS[[i]]$bltr.positions %>%
    dplyr::select(Fish_ID, timestamp, Easting, Northing) %>%
    dplyr::rename(ID = Fish_ID,
                  UTM.Easting = Easting,
                  UTM.Northing = Northing) %>%
    dplyr::mutate(UTM.zone = as.character("10 +north"))
  
  
  # Save the ctmm.data object to bltr_list
  bltr_list_DS[[i]] <- rlist::list.append(bltr_list_DS[[i]], "ctmm.data" = ctmm.data) 
  
  
  # 2. Separate positions into individual data sets for each time slot
  # Time slots correspond to specific daytime or nighttime periods
  # This code could be updated to be executed with an interval join
  bltr_list_DS[[i]]$ctmm.data <- bltr_list_DS[[i]]$ctmm.data %>%
    dplyr::mutate(TOD = case_when(timestamp < "2014-04-27 05:56:29" ~ "N1",
                                timestamp >= "2014-04-27 05:56:29" & timestamp < "2014-04-27 20:34:12" ~ "D1",
                                timestamp >= "2014-04-27 20:34:12" & timestamp < "2014-04-28 05:54:32" ~ "N2",
                                timestamp >= "2014-04-28 05:54:32" & timestamp < "2014-04-28 20:35:53" ~ "D2",
                                timestamp >= "2014-04-28 20:35:53" & timestamp < "2014-04-29 05:52:37" ~ "N3",
                                timestamp >= "2014-04-29 05:52:37" & timestamp < "2014-04-29 20:37:33" ~ "D3",
                                timestamp >= "2014-04-29 20:37:33" & timestamp < "2014-04-30 05:50:43" ~ "N4",
                                timestamp >= "2014-04-30 05:50:43" & timestamp < "2014-04-30 20:39:13" ~ "D4",
                                timestamp >= "2014-04-30 20:39:13" & timestamp < "2014-05-01 05:48:50" ~ "N5",
                                timestamp >= "2014-05-01 05:48:50" & timestamp < "2014-05-01 20:40:53" ~ "D5",
                                timestamp >= "2014-05-01 20:40:53" & timestamp < "2014-05-02 05:46:59" ~ "N6",
                                timestamp >= "2014-05-02 05:46:59" & timestamp < "2014-05-02 20:42:32" ~ "D6",
                                timestamp >= "2014-05-02 20:42:32" & timestamp < "2014-05-03 05:45:09" ~ "N7",
                                timestamp >= "2014-05-03 05:45:09" & timestamp < "2014-05-03 20:44:11" ~ "D7",
                                timestamp >= "2014-05-03 20:44:11" & timestamp < "2014-05-04 05:43:20" ~ "N8",
                                timestamp >= "2014-05-04 05:43:20" & timestamp < "2014-05-04 20:45:50" ~ "D8",
                                timestamp >= "2014-05-04 20:45:50" & timestamp < "2014-05-05 05:41:33" ~ "N9",
                                timestamp >= "2014-05-05 05:41:33" & timestamp < "2014-05-05 20:47:28" ~ "D9",
                                timestamp >= "2014-05-05 20:47:28" & timestamp < "2014-05-06 05:39:48" ~ "N10",
                                timestamp >= "2014-05-06 05:39:48" & timestamp < "2014-05-06 20:49:06" ~ "D10",
                                timestamp >= "2014-05-06 20:49:06" & timestamp < "2014-05-07 05:38:04" ~ "N11",
                                timestamp >= "2014-05-07 05:38:04" & timestamp < "2014-05-07 20:50:44" ~ "D11",
                                timestamp >= "2014-05-07 20:50:44" & timestamp < "2014-05-08 05:36:22" ~ "N12",
                                timestamp >= "2014-05-08 05:36:22" & timestamp < "2014-05-08 20:52:21" ~ "D12",
                                timestamp >= "2014-05-08 20:52:21" & timestamp < "2014-05-09 05:34:42" ~ "N13",
                                timestamp >= "2014-05-09 05:34:42" & timestamp < "2014-05-09 20:53:57" ~ "D13",
                                timestamp >= "2014-05-09 20:53:57" & timestamp < "2014-05-10 05:33:03" ~ "N14",
                                timestamp >= "2014-05-10 05:33:03" & timestamp < "2014-05-10 20:55:33" ~ "D14",
                                timestamp >= "2014-05-10 20:55:33" & timestamp < "2014-05-11 05:31:26" ~ "N15",
                                timestamp >= "2014-05-11 05:31:26" & timestamp < "2014-05-11 20:57:08" ~ "D15",
                                timestamp >= "2014-05-11 20:57:08" & timestamp < "2014-05-12 05:29:51" ~ "N16",
                                timestamp >= "2014-05-12 05:29:51" & timestamp < "2014-05-12 20:58:43" ~ "D16",
                                timestamp >= "2014-05-12 20:58:43" & timestamp < "2014-05-13 05:28:18" ~ "N17",
                                timestamp >= "2014-05-13 05:28:18" & timestamp < "2014-05-13 21:00:16" ~ "D17",
                                timestamp >= "2014-05-13 21:00:16" & timestamp < "2014-05-14 05:26:46" ~ "N18",
                                timestamp >= "2014-05-14 05:26:46" & timestamp < "2014-05-14 21:01:49" ~ "D18",
                                timestamp >= "2014-05-14 21:01:49" & timestamp < "2014-05-15 05:25:17" ~ "N19",
                                timestamp >= "2014-05-15 05:25:17" & timestamp < "2014-05-15 21:03:21" ~ "D19",
                                timestamp >= "2014-05-15 21:03:21" & timestamp < "2014-05-16 05:23:50" ~ "N20",
                                timestamp >= "2014-05-16 05:23:50" & timestamp < "2014-05-16 21:04:52" ~ "D20",
                                timestamp >= "2014-05-16 21:04:52" & timestamp < "2014-05-17 05:22:25" ~ "N21",
                                timestamp >= "2014-05-17 05:22:25" & timestamp < "2014-05-17 21:06:22" ~ "D21",
                                timestamp >= "2014-05-17 21:06:22" & timestamp < "2014-05-18 05:21:02" ~ "N22",
                                timestamp >= "2014-05-18 05:21:02" & timestamp < "2014-05-18 21:07:51" ~ "D22",
                                timestamp >= "2014-05-18 21:07:51" & timestamp < "2014-05-19 05:19:41" ~ "N23",
                                timestamp >= "2014-05-19 05:19:41" & timestamp < "2014-05-19 21:09:18" ~ "D23")) 

  # For each fish, remove any time slots where the fish was detected only three or fewer times (i.e., < 4 VPS positions exist)
  bltr_list_DS[[i]]$ctmm.data <- bltr_list_DS[[i]]$ctmm.data %>%
    dplyr::group_by(TOD) %>%
    dplyr::mutate(TODn = length(TOD)) %>%
    dplyr::filter(TODn > 3) %>%
    dplyr::select(-TODn) %>%
    dplyr::ungroup()
  
  # Save the fish relocation data for each time slot as individual lists
  ctmm.list <- split(bltr_list_DS[[i]]$ctmm.data, f = bltr_list_DS[[i]]$ctmm.data$TOD)
  
  bltr_list_DS[[i]] <- rlist::list.append(bltr_list_DS[[i]], "ctmm.list" = ctmm.list)
  
  # Remove the TOD column from the time slot data sets in the ctmm.list object
  for(j in seq_along(bltr_list_DS[[i]]$ctmm.list)){
    
    bltr_list_DS[[i]]$ctmm.list[[j]] <- bltr_list_DS[[i]]$ctmm.list[[j]] %>%
      dplyr::select(-TOD)
    
  }
  
}
  
# Convert ctmm telemetry data sets to ctmm telemetry objects
for(i in seq_along(bltr_list_DS)){
  
  for(j in seq_along(bltr_list_DS[[i]]$ctmm.list)){
    
    ctmm.telemetry <- ctmm::as.telemetry(bltr_list_DS[[i]]$ctmm.list[[j]], timezone = "Canada/Pacific", datum = "WGS84", 
                                         projection = crs("epsg:32610"))
    
    bltr_list_DS[[i]]$ctmm.list[[j]] <- rlist::list.append(bltr_list_DS[[i]]$ctmm.list[[j]], "ctmm.telemetry" = ctmm.telemetry)
    
  }
}

# 2. Match ctmm object IDs w/ ctmm ID list ####

# Create a new list of bltr ctmm telemetry objects, and change their names to match those in ctmm_ID_list
ctmm_telemetry_list_DS <- unlist(bltr_list_DS, recursive = FALSE)

ctmm_telemetry_list_DS <- unlist(ctmm_telemetry_list_DS, recursive = FALSE)

ctmm_telemetry_list_DS <- purrr::discard(ctmm_telemetry_list_DS, is_double)

ctmm_telemetry_list_DS <- purrr::discard(ctmm_telemetry_list_DS, is_character)

ctmm_telemetry_list_DS <- unlist(ctmm_telemetry_list_DS, recursive = FALSE)

ctmm_telemetry_list_DS <- purrr::discard(ctmm_telemetry_list_DS, is_double)

ctmm_telemetry_list_DS <- purrr::discard(ctmm_telemetry_list_DS, is_character)

names(ctmm_telemetry_list_DS) <- names(ctmm_ID_list_DS)

# Remove objects no longer needed
remove(ctmm_ID_list_DS, ctmm.data, ctmm.list, ctmm.telemetry, i, j)

# 3. Model fitting ####

# --- #

# NOTE:

# If the projection is not set below to the median for the list, you cannot visualize rasters of the UD estimates in R with ctmm::plot().
# However, the UD estimates will still be accurate, and the rasters can be exported for plotting in ArcGIS or QGIS.

# BUT, if the projection is set to the median, UD centroids cannot be extracted!!!!!!

#projection(ctmm_telemetry_list_DS) <- median(ctmm_telemetry_list_DS)

# --- #

# The files created during calculation of AKDEs are massive.
# So, in the code below, the list of telemetry objects used to estimate AKDEs is partitioned into smaller lists
# This is critical if AKDEs are to be estimated with a basic laptop or desktop computer

# Load outlet shapefile
# The shapefile is used in the AKDE model to restrict the area over which the UD can be estimated  
outlet_DS <- terra::vect("GIS/Chilko_Outlet_UTM_VPS_downstream.shp")
outlet_DS <- as(outlet_DS, "Spatial") # Convert to SPDF
class(outlet_DS)

# Parameters you can tweak
chunk_size <- 10 # define the number of time slots for which AKDEs are estimated in each iteration (too many will crash the session)
outdir <- "DATA/AKDE_data_sets_2014_downstream" # create a directory for saving outputs
dir.create(outdir, showWarnings = FALSE, recursive = TRUE)

# Split the list of telemetry objects into chunks (last chunk can be smaller)
idx <- seq_along(ctmm_telemetry_list_DS)
chunk_ids <- ceiling(idx / chunk_size)
ctmm_chunks <- split(ctmm_telemetry_list_DS, chunk_ids) # list of lists
# Names of elements within each chunk are preserved

# Define a function to extract UD area + centroid from a single AKDE object
extract_ud_metrics <- function(ud_obj, id_label) {
  # Area CI (95% UD) and effective sample size
  s <- summary(ud_obj, units = FALSE)
  UD_est   <- s$CI[2]
  UD_low   <- s$CI[1]
  UD_high  <- s$CI[3]
  effective_n <- s$DOF[1]
  
  # Centroid: coordinates for the mode of the UD (minimum CDF value over the raster grid)
  foo <- data.frame(ud_obj$CDF)
  X   <- data.frame(ud_obj$r$x)
  Y   <- ud_obj$r$y
  colnames(foo) <- Y
  XY <- dplyr::bind_cols(X, foo)
  colnames(XY)[1] <- "X"
  
  centroid.coords <- XY %>%
    tidyr::pivot_longer(!X, names_to = "Y", values_to = "UD") %>%
    dplyr::arrange(UD) %>%
    dplyr::slice(1) %>%                       
    dplyr::mutate(
      X = round(as.numeric(X), 2),
      Y = round(as.numeric(Y), 2)
    )
  
  data.frame(
    ID = id_label,
    UD_est = UD_est,
    UD_low = UD_low,
    UD_high = UD_high,
    effective_n = effective_n,
    centroid_X = centroid.coords$X,
    centroid_Y = centroid.coords$Y,
    stringsAsFactors = FALSE
  )
}

# Loop over chunks to fit the AKDE model and extract metrics for UD size and centroid coordinates
AKDE_data_DS <- dplyr::tibble()   # Empty results table for saving UD sizes and centroid coordinates

for (k in seq_along(ctmm_chunks)) {
  telem_list <- ctmm_chunks[[k]]
  
  # 1) Fit candidate movement models for each telemetry object in this chunk
  FITS <- lapply(telem_list, function(telem) {
    GUESS <- ctmm::ctmm.guess(telem, interactive = FALSE)
    ctmm::ctmm.select(telem, CTMM = GUESS, method = "pHREML", trace = 3)
  })
  names(FITS) <- names(telem_list)
  
  # Save FITS for this chunk
  fits_path <- file.path(outdir, sprintf("bltr_timeslot_fitted_mods_%02d.rda", k))
  save(FITS, file = fits_path)
  
  # 2) AKDE for this chunk
  wAKDEc <- ctmm::akde(
    telem_list,
    FITS,
    weights = TRUE,
    SP = outlet_DS,
    SP.in = TRUE,
    trace = TRUE,
    units = FALSE
  )
  
  # Save AKDEs for this chunk
  akde_path <- file.path(outdir, sprintf("bltr_wAKDEc_timeslot_estimates_%02d.rda", k))
  save(wAKDEc, file = akde_path)
  
  # 3) Extract UD areas + centroids for each fish in this chunk and append
  chunk_df <- dplyr::bind_rows(
    mapply(
      FUN = extract_ud_metrics,
      ud_obj = wAKDEc,
      id_label = names(wAKDEc),
      SIMPLIFY = FALSE
    )
  ) %>%
    dplyr::mutate(timeslot = k)
  
  AKDE_data_DS <- dplyr::bind_rows(AKDE_data_DS, chunk_df)
}

# Set the row names to their default
rownames(AKDE_data_DS) <- NULL

```

```{r Centroid_to_fence_distance_downstream, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# Distances between each UD centroid and the location of the smolt fence are calculated along the shortest in-water path

# Assign SpatRaster to another object
cost_spat <- outlet.raster.DS

# Barriers must be NA; passable cells = 1
cost_spat <- classify(cost_spat, rcl = matrix(c(-Inf, 0, NA), ncol=3, byrow=TRUE), include.lowest=TRUE)
cost_spat[!is.na(cost_spat)] <- 1

# Convert SpatRaster to RasterLayer for compatibility with gdistance
cost_raster <- raster::raster(cost_spat)          
stopifnot(inherits(cost_raster, "RasterLayer"))

# Create a transition layer (tr) and a geo-corrected transition layer (trc)
tr  <- gdistance::transition(cost_raster, function(v) 1/mean(v), directions = 16)
trc <- gdistance::geoCorrection(tr, type = "c", multpl = FALSE)

# Ensure coordinates for fence location & bltr UD centroids are in the same CRS as the raster
bltr_sf  <- sf::st_as_sf(AKDE_data_DS, coords = c("centroid_X","centroid_Y"), crs = terra::crs(cost_spat))
fence_sf <- sf::st_as_sf(tibble::tibble(location="fence", Easting = 420930, Northing = 5720012),
                         coords = c("Easting","Northing"), crs = terra::crs(cost_spat))

# Set the target coordinates for the fence location
B <- matrix(sf::st_coordinates(fence_sf), nrow = 1)

# Calculate shortest-path distances in meters
acc <- gdistance::accCost(trc, B)  # RasterLayer of accumulated (shortest path) distance
dists <- terra::extract(terra::rast(acc), terra::vect(bltr_sf))[,2]
bltr_sf$UD_fence_distance_m <- dists

table(is.na(dists))   # TRUE here = unreachable (completely blocked by barriers)

# Drop geometry and timeslot columns
# Format results table
xy <- sf::st_coordinates(bltr_sf)

AKDE_DS <- sf::st_drop_geometry(bltr_sf)

AKDE_DS$centroid_X <- xy[,"X"]
AKDE_DS$centroid_Y <- xy[,"Y"]

AKDE_DS <- AKDE_DS %>%
  dplyr::select(-timeslot) %>%
  dplyr::mutate(row = row_number()) %>%
  dplyr::relocate(1:5, centroid_X, centroid_Y, row, UD_fence_distance_m)

# Save UD size estimates and distances from UD centroids to fence location
#saveRDS(AKDE_DS, "DATA/Data_for_analysis/AKDE_data_DS.rds")

```

```{r AKDE_upstream, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Create ctmm telemetry data sets ####

# Create a ctmm telemetry data set for each time slot of interest (e.g., Night 1, Day 1,... Day 23), for each fish 
for(i in seq_along(bltr_list_US)){
  
  # 1. Analysis of data with the ctmm package requires that animal relocation data conforms to Movebank naming conventions
  ctmm.data <- bltr_list_US[[i]]$bltr.positions %>%
    dplyr::select(Fish_ID, timestamp, Easting, Northing) %>%
    dplyr::rename(ID = Fish_ID,
                  UTM.Easting = Easting,
                  UTM.Northing = Northing) %>%
    dplyr::mutate(UTM.zone = as.character("10 +north"))
  
  
  # Save the ctmm.data object to bltr_list
  bltr_list_US[[i]] <- rlist::list.append(bltr_list_US[[i]], "ctmm.data" = ctmm.data) 
  
  
  # 2. Separate positions into individual data sets for each time slot
  # Time slots correspond to specific daytime or nighttime periods
  # This code could be updated to be executed with an interval join
  bltr_list_US[[i]]$ctmm.data <- bltr_list_US[[i]]$ctmm.data %>%
    dplyr::mutate(TOD = case_when(timestamp < "2014-04-27 05:56:29" ~ "N1",
                                timestamp >= "2014-04-27 05:56:29" & timestamp < "2014-04-27 20:34:12" ~ "D1",
                                timestamp >= "2014-04-27 20:34:12" & timestamp < "2014-04-28 05:54:32" ~ "N2",
                                timestamp >= "2014-04-28 05:54:32" & timestamp < "2014-04-28 20:35:53" ~ "D2",
                                timestamp >= "2014-04-28 20:35:53" & timestamp < "2014-04-29 05:52:37" ~ "N3",
                                timestamp >= "2014-04-29 05:52:37" & timestamp < "2014-04-29 20:37:33" ~ "D3",
                                timestamp >= "2014-04-29 20:37:33" & timestamp < "2014-04-30 05:50:43" ~ "N4",
                                timestamp >= "2014-04-30 05:50:43" & timestamp < "2014-04-30 20:39:13" ~ "D4",
                                timestamp >= "2014-04-30 20:39:13" & timestamp < "2014-05-01 05:48:50" ~ "N5",
                                timestamp >= "2014-05-01 05:48:50" & timestamp < "2014-05-01 20:40:53" ~ "D5",
                                timestamp >= "2014-05-01 20:40:53" & timestamp < "2014-05-02 05:46:59" ~ "N6",
                                timestamp >= "2014-05-02 05:46:59" & timestamp < "2014-05-02 20:42:32" ~ "D6",
                                timestamp >= "2014-05-02 20:42:32" & timestamp < "2014-05-03 05:45:09" ~ "N7",
                                timestamp >= "2014-05-03 05:45:09" & timestamp < "2014-05-03 20:44:11" ~ "D7",
                                timestamp >= "2014-05-03 20:44:11" & timestamp < "2014-05-04 05:43:20" ~ "N8",
                                timestamp >= "2014-05-04 05:43:20" & timestamp < "2014-05-04 20:45:50" ~ "D8",
                                timestamp >= "2014-05-04 20:45:50" & timestamp < "2014-05-05 05:41:33" ~ "N9",
                                timestamp >= "2014-05-05 05:41:33" & timestamp < "2014-05-05 20:47:28" ~ "D9",
                                timestamp >= "2014-05-05 20:47:28" & timestamp < "2014-05-06 05:39:48" ~ "N10",
                                timestamp >= "2014-05-06 05:39:48" & timestamp < "2014-05-06 20:49:06" ~ "D10",
                                timestamp >= "2014-05-06 20:49:06" & timestamp < "2014-05-07 05:38:04" ~ "N11",
                                timestamp >= "2014-05-07 05:38:04" & timestamp < "2014-05-07 20:50:44" ~ "D11",
                                timestamp >= "2014-05-07 20:50:44" & timestamp < "2014-05-08 05:36:22" ~ "N12",
                                timestamp >= "2014-05-08 05:36:22" & timestamp < "2014-05-08 20:52:21" ~ "D12",
                                timestamp >= "2014-05-08 20:52:21" & timestamp < "2014-05-09 05:34:42" ~ "N13",
                                timestamp >= "2014-05-09 05:34:42" & timestamp < "2014-05-09 20:53:57" ~ "D13",
                                timestamp >= "2014-05-09 20:53:57" & timestamp < "2014-05-10 05:33:03" ~ "N14",
                                timestamp >= "2014-05-10 05:33:03" & timestamp < "2014-05-10 20:55:33" ~ "D14",
                                timestamp >= "2014-05-10 20:55:33" & timestamp < "2014-05-11 05:31:26" ~ "N15",
                                timestamp >= "2014-05-11 05:31:26" & timestamp < "2014-05-11 20:57:08" ~ "D15",
                                timestamp >= "2014-05-11 20:57:08" & timestamp < "2014-05-12 05:29:51" ~ "N16",
                                timestamp >= "2014-05-12 05:29:51" & timestamp < "2014-05-12 20:58:43" ~ "D16",
                                timestamp >= "2014-05-12 20:58:43" & timestamp < "2014-05-13 05:28:18" ~ "N17",
                                timestamp >= "2014-05-13 05:28:18" & timestamp < "2014-05-13 21:00:16" ~ "D17",
                                timestamp >= "2014-05-13 21:00:16" & timestamp < "2014-05-14 05:26:46" ~ "N18",
                                timestamp >= "2014-05-14 05:26:46" & timestamp < "2014-05-14 21:01:49" ~ "D18",
                                timestamp >= "2014-05-14 21:01:49" & timestamp < "2014-05-15 05:25:17" ~ "N19",
                                timestamp >= "2014-05-15 05:25:17" & timestamp < "2014-05-15 21:03:21" ~ "D19",
                                timestamp >= "2014-05-15 21:03:21" & timestamp < "2014-05-16 05:23:50" ~ "N20",
                                timestamp >= "2014-05-16 05:23:50" & timestamp < "2014-05-16 21:04:52" ~ "D20",
                                timestamp >= "2014-05-16 21:04:52" & timestamp < "2014-05-17 05:22:25" ~ "N21",
                                timestamp >= "2014-05-17 05:22:25" & timestamp < "2014-05-17 21:06:22" ~ "D21",
                                timestamp >= "2014-05-17 21:06:22" & timestamp < "2014-05-18 05:21:02" ~ "N22",
                                timestamp >= "2014-05-18 05:21:02" & timestamp < "2014-05-18 21:07:51" ~ "D22",
                                timestamp >= "2014-05-18 21:07:51" & timestamp < "2014-05-19 05:19:41" ~ "N23",
                                timestamp >= "2014-05-19 05:19:41" & timestamp < "2014-05-19 21:09:18" ~ "D23")) 

  # For each fish, remove any time slots where the fish was detected only three or fewer times (i.e., < 4 VPS positions exist)
  bltr_list_US[[i]]$ctmm.data <- bltr_list_US[[i]]$ctmm.data %>%
    dplyr::group_by(TOD) %>%
    dplyr::mutate(TODn = length(TOD)) %>%
    dplyr::filter(TODn > 3) %>%
    dplyr::select(-TODn) %>%
    dplyr::ungroup()
  
  # Save the fish relocation data for each time slot as individual lists
  ctmm.list <- split(bltr_list_US[[i]]$ctmm.data, f = bltr_list_US[[i]]$ctmm.data$TOD)
  
  bltr_list_US[[i]] <- rlist::list.append(bltr_list_US[[i]], "ctmm.list" = ctmm.list)
  
  # Remove the TOD column from the time slot data sets in the ctmm.list object
  for(j in seq_along(bltr_list_US[[i]]$ctmm.list)){
    
    bltr_list_US[[i]]$ctmm.list[[j]] <- bltr_list_US[[i]]$ctmm.list[[j]] %>%
      dplyr::select(-TOD)
    
  }
  
}
  
# Convert ctmm telemetry data sets to ctmm telemetry objects
for(i in seq_along(bltr_list_US)){
  
  for(j in seq_along(bltr_list_US[[i]]$ctmm.list)){
    
    ctmm.telemetry <- ctmm::as.telemetry(bltr_list_US[[i]]$ctmm.list[[j]], timezone = "Canada/Pacific", datum = "WGS84", 
                                         projection = crs("epsg:32610"))
    
    bltr_list_US[[i]]$ctmm.list[[j]] <- rlist::list.append(bltr_list_US[[i]]$ctmm.list[[j]], "ctmm.telemetry" = ctmm.telemetry)
    
  }
}

# 2. Match ctmm object IDs w/ ctmm ID list ####

# Create a new list of bltr ctmm telemetry objects, and change their names to match those in ctmm_ID_list
ctmm_telemetry_list_US <- unlist(bltr_list_US, recursive = FALSE)

ctmm_telemetry_list_US <- unlist(ctmm_telemetry_list_US, recursive = FALSE)

ctmm_telemetry_list_US <- purrr::discard(ctmm_telemetry_list_US, is_double)

ctmm_telemetry_list_US <- purrr::discard(ctmm_telemetry_list_US, is_character)

ctmm_telemetry_list_US <- unlist(ctmm_telemetry_list_US, recursive = FALSE)

ctmm_telemetry_list_US <- purrr::discard(ctmm_telemetry_list_US, is_double)

ctmm_telemetry_list_US <- purrr::discard(ctmm_telemetry_list_US, is_character)

names(ctmm_telemetry_list_US) <- names(ctmm_ID_list_US)

# Remove objects no longer needed
remove(ctmm_ID_list_US, ctmm.data, ctmm.list, ctmm.telemetry, i, j)

# 3. Model fitting ####

# --- #
# NOTE:

# If the projection is not set below to the median for the list, you cannot visualize rasters of the UD estimates in R with ctmm::plot().
# However, the UD estimates will still be accurate, and the rasters can be exported for plotting in ArcGIS or QGIS.

# BUT, if the projection is set to the median, UD centroids cannot be extracted!!!!!!

#projection(ctmm_telemetry_list_US) <- median(ctmm_telemetry_list_US)

# --- #

# The files created during calculation of AKDEs are massive.
# So, in the code below, the list of telemetry objects used to estimate AKDEs is partitioned into smaller lists
# This is critical if AKDEs are to be estimated with a basic laptop or desktop computer

# Load outlet shapefile
# The shapefile is used in the AKDE model to restrict the area over which the UD can be estimated  
outlet_US <- terra::vect("GIS/Chilko_Outlet_UTM_VPS_upstream.shp")
outlet_US <- as(outlet_US, "Spatial") # Convert to SPDF
class(outlet_US)

# Parameters you can tweak
chunk_size <- 10 # define the number of time slots for which AKDEs are estimated in each iteration (too many will crash the session)
outdir <- "DATA/AKDE_data_sets_2014_upstream" # create a directory for saving outputs
dir.create(outdir, showWarnings = FALSE, recursive = TRUE)

# Split the list of telemetry objects into chunks (last chunk can be smaller)
idx <- seq_along(ctmm_telemetry_list_US)
chunk_ids <- ceiling(idx / chunk_size)
ctmm_chunks <- split(ctmm_telemetry_list_US, chunk_ids) # list of lists
# Names of elements within each chunk are preserved

# Define a function to extract UD area + centroid from a single AKDE object
extract_ud_metrics <- function(ud_obj, id_label) {
  # Area CI (95% UD) and effective sample size
  s <- summary(ud_obj, units = FALSE)
  UD_est   <- s$CI[2]
  UD_low   <- s$CI[1]
  UD_high  <- s$CI[3]
  effective_n <- s$DOF[1]
  
  # Centroid: coordinates for the mode of the UD (minimum CDF value over the raster grid)
  foo <- data.frame(ud_obj$CDF)
  X   <- data.frame(ud_obj$r$x)
  Y   <- ud_obj$r$y
  colnames(foo) <- Y
  XY <- dplyr::bind_cols(X, foo)
  colnames(XY)[1] <- "X"
  
  centroid.coords <- XY %>%
    tidyr::pivot_longer(!X, names_to = "Y", values_to = "UD") %>%
    dplyr::arrange(UD) %>%
    dplyr::slice(1) %>%                       
    dplyr::mutate(
      X = round(as.numeric(X), 2),
      Y = round(as.numeric(Y), 2)
    )
  
  data.frame(
    ID = id_label,
    UD_est = UD_est,
    UD_low = UD_low,
    UD_high = UD_high,
    effective_n = effective_n,
    centroid_X = centroid.coords$X,
    centroid_Y = centroid.coords$Y,
    stringsAsFactors = FALSE
  )
}

# Loop over chunks to fit the AKDE model and extract metrics for UD size and centroid coordinates
AKDE_data_US <- dplyr::tibble()   # master results table

for (k in seq_along(ctmm_chunks)) {
  telem_list <- ctmm_chunks[[k]]
  
  # 1) Fit candidate movement models for each telemetry object in this chunk
  FITS <- lapply(telem_list, function(telem) {
    GUESS <- ctmm::ctmm.guess(telem, interactive = FALSE)
    ctmm::ctmm.select(telem, CTMM = GUESS, method = "pHREML", trace = 3)
  })
  names(FITS) <- names(telem_list)
  
  # Save FITS for this chunk
  fits_path <- file.path(outdir, sprintf("bltr_timeslot_fitted_mods_%02d.rda", k))
  save(FITS, file = fits_path)
  
  # 2) AKDE for this chunk
  wAKDEc <- ctmm::akde(
    telem_list,
    FITS,
    weights = TRUE,
    SP = outlet_US,
    SP.in = TRUE,
    trace = TRUE,
    units = FALSE
  )
  
  # Save AKDEs for this chunk
  akde_path <- file.path(outdir, sprintf("bltr_wAKDEc_timeslot_estimates_%02d.rda", k))
  save(wAKDEc, file = akde_path)
  
  # 3) Extract UD areas + centroids for each fish in this chunk and append
  chunk_df <- dplyr::bind_rows(
    mapply(
      FUN = extract_ud_metrics,
      ud_obj = wAKDEc,
      id_label = names(wAKDEc),
      SIMPLIFY = FALSE
    )
  ) %>%
    dplyr::mutate(timeslot = k)
  
  AKDE_data_US <- dplyr::bind_rows(AKDE_data_US, chunk_df)
}

# Set the row names to their default
rownames(AKDE_data_US) <- NULL

# Save the combined summary table
saveRDS(AKDE_data_US, file.path(outdir, "bltr_AKDE_summary_all_timeslots.rds"))

```

```{r Centroid_to_fence_distance_upstream, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# Distances between each UD centroid and the location of the smolt fence are calculated along the shortest in-water path

# Assign SpatRaster to another object
cost_spat <- outlet.raster.US

# Barriers must be NA; passable cells = 1
cost_spat <- classify(cost_spat, rcl = matrix(c(-Inf, 0, NA), ncol=3, byrow=TRUE), include.lowest=TRUE)
cost_spat[!is.na(cost_spat)] <- 1

# Convert SpatRaster to RasterLayer for compatibility with gdistance
cost_raster <- raster::raster(cost_spat)          
stopifnot(inherits(cost_raster, "RasterLayer"))

# Create a transition layer (tr) and a geo-corrected transition layer (trc)
tr  <- gdistance::transition(cost_raster, function(v) 1/mean(v), directions = 16)
trc <- gdistance::geoCorrection(tr, type = "c", multpl = FALSE)

# Ensure coordinates for fence location & bltr UD centroids are in the same CRS as the raster
bltr_sf  <- sf::st_as_sf(AKDE_data_US, coords = c("centroid_X","centroid_Y"), crs = terra::crs(cost_spat))
fence_sf <- sf::st_as_sf(tibble::tibble(location="fence", Easting = 420929, Northing = 5720007),
                         coords = c("Easting","Northing"), crs = terra::crs(cost_spat))

# Set the target coordinates for the fence location
B <- matrix(sf::st_coordinates(fence_sf), nrow = 1)

# Calculate shortest-path distances in meters
acc <- gdistance::accCost(trc, B)  # RasterLayer of accumulated (shortest path) distance
dists <- terra::extract(terra::rast(acc), terra::vect(bltr_sf))[,2]
bltr_sf$UD_fence_distance_m <- dists

table(is.na(dists))   # TRUE here = unreachable (completely blocked by barriers)

# Drop geometry and timeslot columns
# Format results table
xy <- sf::st_coordinates(bltr_sf)

AKDE_US <- sf::st_drop_geometry(bltr_sf)

AKDE_US$centroid_X <- xy[,"X"]
AKDE_US$centroid_Y <- xy[,"Y"]

AKDE_US <- AKDE_US %>%
  dplyr::select(-timeslot) %>%
  dplyr::mutate(row = row_number()) %>%
  dplyr::relocate(1:5, centroid_X, centroid_Y, row, UD_fence_distance_m)

# Save UD size estimates and distances from UD centroids to fence location
#saveRDS(AKDE_US, "DATA/Data_for_analysis/AKDE_data_US.rds")

```

```{r Distance_between_sequential_positions_downstream, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# Assign SpatRaster to another object
cost_spat <- outlet.raster.DS

# Barriers must be NA; passable cells = 1
cost_spat <- classify(cost_spat, rcl = matrix(c(-Inf, 0, NA), 1, 3, byrow=TRUE), include.lowest=TRUE)
cost_spat[!is.na(cost_spat)] <- 1

# Convert SpatRaster to RasterLayer for compatibility with gdistance
cost_raster <- raster::raster(cost_spat)          
stopifnot(inherits(cost_raster, "RasterLayer"))

# Create a transition layer (tr) and a geo-corrected transition layer (trc)
tr  <- gdistance::transition(cost_raster, function(v) 1/mean(v), directions = 16)
trc <- gdistance::geoCorrection(tr, type = "c", multpl = FALSE)

# Convert positions to sf object
locs_sf <- sf::st_as_sf(bltr.positions.DS, coords = c("Easting","Northing"), crs = terra::crs(cost_spat))

# Extract numeric coords so we can build sequential pairs easily
xy <- sf::st_coordinates(locs_sf)

pairs_df <- locs_sf %>%
  mutate(X = xy[,1], Y = xy[,2]) %>%
  group_by(Fish_ID) %>%
  arrange(timestamp, .by_group = TRUE) %>%
  mutate(X_next = lead(X),
         Y_next = lead(Y),
         timestamp_next = lead(timestamp)) %>%
  filter(!is.na(X_next) & !is.na(Y_next)) %>%
  ungroup() %>%
  sf::st_drop_geometry()

# Set-up for parallel computing via future package
options(future.globals.maxSize = 1 * 1024^3)
future::plan(multisession, workers = max(1, parallel::detectCores() - 1))
on.exit(future::plan(sequential), add = TRUE)

# Split by fish ID and compute shortest-path distance per step (i.e., for each pair of sequential positions per bull trout)
# Function that computes one pair’s distance
pair_dist <- function(x1, y1, x2, y2) {
  A <- matrix(c(x1, y1), nrow = 1)
  B <- matrix(c(x2, y2), nrow = 1)
  # Returns length-1 numeric; NA/Inf if unreachable (fully blocked by NA cells)
  as.numeric(gdistance::costDistance(trc, A, B))
}

by_id <- split(pairs_df, pairs_df$Fish_ID)

res_list <- furrr::future_map(
  by_id,
  ~ {
    df <- .x
    df$step_dist_m <- mapply(pair_dist, df$X, df$Y, df$X_next, df$Y_next)
    df$reachable   <- is.finite(df$step_dist_m)
    df
  },
  .options = furrr::furrr_options(seed = NULL)
)

# Save the calculated distances
bltr_distances_DS <- list_rbind(res_list) %>%
  transmute(
    Fish_ID,
    timestamp_start = timestamp,
    timestamp_end = timestamp_next,
    from_Easting = X, 
    from_Northing = Y,
    to_Easting = X_next, 
    to_Northing = Y_next,
    step_dist_m,
    reachable
  )

bltr.temp <- bltr_distances_DS %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::slice_min(timestamp_start) %>%
  dplyr::select(Fish_ID, timestamp_start, from_Easting, from_Northing) %>%
  dplyr::rename(timestamp = timestamp_start,
                Easting = from_Easting,
                Northing = from_Northing) %>%
  dplyr::mutate(step_dist_m = NA)
  

bltr_distances_DS <- bltr_distances_DS %>%
  dplyr::select(Fish_ID, timestamp_end, to_Easting, to_Northing, step_dist_m) %>%
  dplyr::rename(timestamp = timestamp_end,
                Easting = to_Easting,
                Northing = to_Northing)

bltr_distances_DS <- bltr_distances_DS %>%
  dplyr::bind_rows(bltr.temp) %>%
  dplyr::arrange(Fish_ID, timestamp)

# Reset parallel computing to sequential
plan(sequential)

# Save distances to file
#saveRDS(bltr_distances_DS, "DATA/Data_for_analysis/movement_data_DS.rds")

```

```{r Distance_between_sequential_positions_upstream, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# Using the bltr positions data, calculate the shortest-path distance between sequential positions

# Assign SpatRaster to another object
cost_spat <- outlet.raster.US

# Barriers must be NA; passable cells = 1
cost_spat <- classify(cost_spat, rcl = matrix(c(-Inf, 0, NA), 1, 3, byrow=TRUE), include.lowest=TRUE)
cost_spat[!is.na(cost_spat)] <- 1

# Convert SpatRaster to RasterLayer for compatibility with gdistance
cost_raster <- raster::raster(cost_spat)          
stopifnot(inherits(cost_raster, "RasterLayer"))

# Create a transition layer (tr) and a geo-corrected transition layer (trc)
tr  <- gdistance::transition(cost_raster, function(v) 1/mean(v), directions = 16)
trc <- gdistance::geoCorrection(tr, type = "c", multpl = FALSE)

# Convert positions to sf object
locs_sf <- sf::st_as_sf(bltr.positions.US, coords = c("Easting","Northing"), crs = terra::crs(cost_spat))

# Extract numeric coords so we can build sequential pairs easily
xy <- sf::st_coordinates(locs_sf)

pairs_df <- locs_sf %>%
  mutate(X = xy[,1], Y = xy[,2]) %>%
  group_by(Fish_ID) %>%
  arrange(timestamp, .by_group = TRUE) %>%
  mutate(X_next = lead(X),
         Y_next = lead(Y),
         timestamp_next = lead(timestamp)) %>%
  filter(!is.na(X_next) & !is.na(Y_next)) %>%
  ungroup() %>%
  sf::st_drop_geometry()

# Set-up for parallel computing via future package
options(future.globals.maxSize = 1 * 1024^3)
future::plan(multisession, workers = max(1, parallel::detectCores() - 1))
on.exit(future::plan(sequential), add = TRUE)

# Split by fish ID and compute shortest-path distance per step (i.e., for each pair of sequential positions per bull trout)
# Function that computes one pair’s distance
pair_dist <- function(x1, y1, x2, y2) {
  A <- matrix(c(x1, y1), nrow = 1)
  B <- matrix(c(x2, y2), nrow = 1)
  # Returns length-1 numeric; NA/Inf if unreachable (fully blocked by NA cells)
  as.numeric(gdistance::costDistance(trc, A, B))
}

by_id <- split(pairs_df, pairs_df$Fish_ID)

res_list <- furrr::future_map(
  by_id,
  ~ {
    df <- .x
    df$step_dist_m <- mapply(pair_dist, df$X, df$Y, df$X_next, df$Y_next)
    df$reachable   <- is.finite(df$step_dist_m)
    df
  },
  .options = furrr::furrr_options(seed = NULL)
)


# Save the calculated distances
bltr_distances_US <- list_rbind(res_list) %>%
  transmute(
    Fish_ID,
    timestamp_start = timestamp,
    timestamp_end = timestamp_next,
    from_Easting = X, 
    from_Northing = Y,
    to_Easting = X_next, 
    to_Northing = Y_next,
    step_dist_m,
    reachable
  )


bltr.temp <- bltr_distances_US %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::slice_min(timestamp_start) %>%
  dplyr::select(Fish_ID, timestamp_start, from_Easting, from_Northing) %>%
  dplyr::rename(timestamp = timestamp_start,
                Easting = from_Easting,
                Northing = from_Northing) %>%
  dplyr::mutate(step_dist_m = NA)
  

bltr_distances_US <- bltr_distances_US %>%
  dplyr::select(Fish_ID, timestamp_end, to_Easting, to_Northing, step_dist_m) %>%
  dplyr::rename(timestamp = timestamp_end,
                Easting = to_Easting,
                Northing = to_Northing)

bltr_distances_US <- bltr_distances_US %>%
  dplyr::bind_rows(bltr.temp) %>%
  dplyr::arrange(Fish_ID, timestamp)

# Reset parallel computing to sequential
plan(sequential)

# Save distances to file
#saveRDS(bltr_distances_US, "DATA/Data_for_analysis/movement_data_US.rds")


```

## Movement metrics for 2015

Utilization distributions are calculated separately for bull trout in 2015 from those in 2014 because the models used to estimate AKDEs incorporate a shapefile of the study area over which UDs can be estimated. In 2014, we assumed UDs for bull trout on the downstream side of the fence could only be estimated for the area downstream of the fence, and the same assumption was made for bull trout on the upstream side of the fence. In 2015, UDs could be estimated throughout the entire study area because the fence was not deployed.

```{r Set_up_data_for_AKDE_estimation_2015, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Check that all data sets are properly formatted ####

str(bltr.positions.15)

tz(bltr.positions.15$DateTime)

str(bltr.data.15)

tz(bltr.data.15$ReleaseDateTime)

str(solar.data.15)

tz(solar.data.15$Date)


# Change timezone for bltr.positions.15 DateTime
bltr.positions.15 <- bltr.positions.15 %>%
  dplyr::mutate(DateTime = lubridate::with_tz(DateTime, tzone = "Canada/Pacific"))

# Set the time zone for the Date column in `solar.lunar.data.15` to Canada/Pacific
solar.data.15 <- solar.data.15 %>%
  dplyr::mutate(Date = lubridate::force_tz(Date, tzone = "Canada/Pacific")) %>%
  dplyr::select(Date, sunrise, sunset)


# 2. Create lists of fish IDs and time slot IDs ####

# Create a list with names of combined fish IDs and time slot IDs; use as reference for naming the objects in the list of ctmm telemetry objects

bltr.positions.15 <- bltr.positions.15 %>%
  dplyr::rename(timestamp = DateTime) %>%
  dplyr::mutate(TOD = case_when(timestamp < "2015-04-20 06:11:10" ~ "N1",
                                timestamp >= "2015-04-20 06:11:10" & timestamp < "2015-04-20 20:22:00" ~ "D1",
                                timestamp >= "2015-04-20 20:22:00" & timestamp < "2015-04-21 06:09:05" ~ "N2",
                                timestamp >= "2015-04-21 06:09:05" & timestamp < "2015-04-21 20:23:41" ~ "D2",
                                timestamp >= "2015-04-21 20:23:41" & timestamp < "2015-04-22 06:07:01" ~ "N3",
                                timestamp >= "2015-04-22 06:07:01" & timestamp < "2015-04-22 20:25:22" ~ "D3",
                                timestamp >= "2015-04-22 20:25:22" & timestamp < "2015-04-23 06:04:58" ~ "N4",
                                timestamp >= "2015-04-23 06:04:58" & timestamp < "2015-04-23 20:27:03" ~ "D4",
                                timestamp >= "2015-04-23 20:27:03" & timestamp < "2015-04-24 06:02:56" ~ "N5",
                                timestamp >= "2015-04-24 06:02:56" & timestamp < "2015-04-24 20:28:44" ~ "D5",
                                timestamp >= "2015-04-24 20:28:44" & timestamp < "2015-04-25 06:00:56" ~ "N6",
                                timestamp >= "2015-04-25 06:00:56" & timestamp < "2015-04-25 20:30:25" ~ "D6",
                                timestamp >= "2015-04-25 20:30:25" & timestamp < "2015-04-26 05:58:56" ~ "N7",
                                timestamp >= "2015-04-26 05:58:56" & timestamp < "2015-04-26 20:32:05" ~ "D7",
                                timestamp >= "2015-04-26 20:32:05" & timestamp < "2015-04-27 05:56:58" ~ "N8",
                                timestamp >= "2015-04-27 05:56:58" & timestamp < "2015-04-27 20:33:46" ~ "D8",
                                timestamp >= "2015-04-27 20:33:46" & timestamp < "2015-04-28 05:55:01" ~ "N9",
                                timestamp >= "2015-04-28 05:55:01" & timestamp < "2015-04-28 20:35:26" ~ "D9",
                                timestamp >= "2015-04-28 20:35:26" & timestamp < "2015-04-29 05:53:05" ~ "N10",
                                timestamp >= "2015-04-29 05:53:05" & timestamp < "2015-04-29 20:37:07" ~ "D10",
                                timestamp >= "2015-04-29 20:37:07" & timestamp < "2015-04-30 05:51:11" ~ "N11",
                                timestamp >= "2015-04-30 05:51:11" & timestamp < "2015-04-30 20:38:47" ~ "D11",
                                timestamp >= "2015-04-30 20:38:47" & timestamp < "2015-05-01 05:49:18" ~ "N12",
                                timestamp >= "2015-05-01 05:49:18" & timestamp < "2015-05-01 20:40:26" ~ "D12",
                                timestamp >= "2015-05-01 20:40:26" & timestamp < "2015-05-02 05:47:26" ~ "N13",
                                timestamp >= "2015-05-02 05:47:26" & timestamp < "2015-05-02 20:42:06" ~ "D13",
                                timestamp >= "2015-05-02 20:42:06" & timestamp < "2015-05-03 05:45:36" ~ "N14",
                                timestamp >= "2015-05-03 05:45:36" & timestamp < "2015-05-03 20:43:45" ~ "D14",
                                timestamp >= "2015-05-03 20:43:45" & timestamp < "2015-05-04 05:43:47" ~ "N15",
                                timestamp >= "2015-05-04 05:43:47" & timestamp < "2015-05-04 20:45:24" ~ "D15",
                                timestamp >= "2015-05-04 20:45:24" & timestamp < "2015-05-05 05:42:00" ~ "N16",
                                timestamp >= "2015-05-05 05:42:00" & timestamp < "2015-05-05 20:47:03" ~ "D16",
                                timestamp >= "2015-05-05 20:47:03" & timestamp < "2015-05-06 05:40:14" ~ "N17",
                                timestamp >= "2015-05-06 05:40:14" & timestamp < "2015-05-06 20:48:41" ~ "D17",
                                timestamp >= "2015-05-06 20:48:41" & timestamp < "2015-05-07 05:38:30" ~ "N18",
                                timestamp >= "2015-05-07 05:38:30" & timestamp < "2015-05-07 20:50:18" ~ "D18",
                                timestamp >= "2015-05-07 20:50:18" & timestamp < "2015-05-08 05:36:47" ~ "N19",
                                timestamp >= "2015-05-08 05:36:47" & timestamp < "2015-05-08 20:51:56" ~ "D19",
                                timestamp >= "2015-05-08 20:51:56" & timestamp < "2015-05-09 05:35:06" ~ "N20",
                                timestamp >= "2015-05-09 05:35:06" & timestamp < "2015-05-09 20:53:32" ~ "D20",
                                timestamp >= "2015-05-09 20:53:32" & timestamp < "2015-05-10 05:33:27" ~ "N21"))


# 3. Create ctmm_ID_list objects for 2015 AKDE estimation ####

ctmm_ID_list_15 <- bltr.positions.15

# 4. Filter time slots with an insufficient number of positions for UD estimation ####

# For each fish, remove any time slots where the fish was detected only three or fewer times (i.e., < 4 VPS positions exist)
ctmm_ID_list_15 <- ctmm_ID_list_15 %>% 
  dplyr::group_by(Fish_ID, TOD) %>% 
  dplyr::mutate(TODn = length(TOD)) %>% 
  dplyr::filter(TODn > 3) %>% 
  dplyr::select(-TODn) %>% 
  dplyr::ungroup()

# 5. Create lists of Fish IDs and time slots ####
  
# Arrange by Fish ID and time slot ID to match the order that will be produced in the bltr_list  
ctmm_ID_list_15 <- ctmm_ID_list_15 %>% 
  dplyr::select(Fish_ID, TOD) %>% 
  dplyr::arrange(Fish_ID, TOD) %>% 
  dplyr::distinct() %>% 
  tidyr::unite(ctmm_ID, Fish_ID, TOD, sep = "_", remove = TRUE)

ctmm_ID_list_15 <- as.list(ctmm_ID_list_15$ctmm_ID)

names(ctmm_ID_list_15) <- ctmm_ID_list_15


# 6. Separate the bltr_list_15 ####

# Create a list of bull trout positions

bltr_list_15 <- split(bltr.positions.15, f = bltr.positions.15$Fish_ID)
bltr_list_15 <- purrr::map(bltr_list_15, tibble::as_tibble)
list2env(bltr_list_15, envir = .GlobalEnv)
remove(bltr_list_15)

# Convert individual data frames into lists
BT06 <- list("bltr.positions.15" = BT06)
BT08 <- list("bltr.positions.15" = BT08)
BT10 <- list("bltr.positions.15" = BT10)
BT12 <- list("bltr.positions.15" = BT12)
BT13 <- list("bltr.positions.15" = BT13)
BT17 <- list("bltr.positions.15" = BT17)
BT26 <- list("bltr.positions.15" = BT26)
BT27 <- list("bltr.positions.15" = BT27)
BT28 <- list("bltr.positions.15" = BT28)
BT29 <- list("bltr.positions.15" = BT29)
BT30 <- list("bltr.positions.15" = BT30)


# Combine lists into a single nested list
bltr_list_15 <- list("BT06" = BT06, "BT08" = BT08, "BT10" = BT10, "BT12" = BT12, "BT13" = BT13, "BT17" = BT17, 
                     "BT26" = BT26, "BT27" = BT27, "BT28" = BT28, "BT29" = BT29, "BT30" = BT30)

# Remove individual bull trout lists
remove(BT06, BT08, BT10, BT12, BT13, BT17, BT26, BT27, BT28, BT29, BT30)


```

```{r AKDE_2015, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Create ctmm telemetry data sets ####

# Create a ctmm telemetry data set for each time slot of interest (e.g., Night 1, Day 1,... Day 23), for each fish 
for(i in seq_along(bltr_list_15)){
  
  # 1. Analysis of data with the ctmm package requires that animal relocation data conform to Movebank naming conventions
  ctmm.data <- bltr_list_15[[i]]$bltr.positions.15 %>%
    dplyr::select(Fish_ID, timestamp, Easting, Northing) %>%
    dplyr::rename(ID = Fish_ID,
                  UTM.Easting = Easting,
                  UTM.Northing = Northing) %>%
    dplyr::mutate(UTM.zone = as.character("10 +north"))
  
  
  # Save the ctmm.data object to bltr_list
  bltr_list_15[[i]] <- rlist::list.append(bltr_list_15[[i]], "ctmm.data" = ctmm.data) 
  
  
  # 2. Separate positions into individual data sets for each time slot of interest
  bltr_list_15[[i]]$ctmm.data <- bltr_list_15[[i]]$ctmm.data %>%
    dplyr::mutate(TOD = case_when(timestamp < "2015-04-20 06:11:10" ~ "N1",
                                timestamp >= "2015-04-20 06:11:10" & timestamp < "2015-04-20 20:22:00" ~ "D1",
                                timestamp >= "2015-04-20 20:22:00" & timestamp < "2015-04-21 06:09:05" ~ "N2",
                                timestamp >= "2015-04-21 06:09:05" & timestamp < "2015-04-21 20:23:41" ~ "D2",
                                timestamp >= "2015-04-21 20:23:41" & timestamp < "2015-04-22 06:07:01" ~ "N3",
                                timestamp >= "2015-04-22 06:07:01" & timestamp < "2015-04-22 20:25:22" ~ "D3",
                                timestamp >= "2015-04-22 20:25:22" & timestamp < "2015-04-23 06:04:58" ~ "N4",
                                timestamp >= "2015-04-23 06:04:58" & timestamp < "2015-04-23 20:27:03" ~ "D4",
                                timestamp >= "2015-04-23 20:27:03" & timestamp < "2015-04-24 06:02:56" ~ "N5",
                                timestamp >= "2015-04-24 06:02:56" & timestamp < "2015-04-24 20:28:44" ~ "D5",
                                timestamp >= "2015-04-24 20:28:44" & timestamp < "2015-04-25 06:00:56" ~ "N6",
                                timestamp >= "2015-04-25 06:00:56" & timestamp < "2015-04-25 20:30:25" ~ "D6",
                                timestamp >= "2015-04-25 20:30:25" & timestamp < "2015-04-26 05:58:56" ~ "N7",
                                timestamp >= "2015-04-26 05:58:56" & timestamp < "2015-04-26 20:32:05" ~ "D7",
                                timestamp >= "2015-04-26 20:32:05" & timestamp < "2015-04-27 05:56:58" ~ "N8",
                                timestamp >= "2015-04-27 05:56:58" & timestamp < "2015-04-27 20:33:46" ~ "D8",
                                timestamp >= "2015-04-27 20:33:46" & timestamp < "2015-04-28 05:55:01" ~ "N9",
                                timestamp >= "2015-04-28 05:55:01" & timestamp < "2015-04-28 20:35:26" ~ "D9",
                                timestamp >= "2015-04-28 20:35:26" & timestamp < "2015-04-29 05:53:05" ~ "N10",
                                timestamp >= "2015-04-29 05:53:05" & timestamp < "2015-04-29 20:37:07" ~ "D10",
                                timestamp >= "2015-04-29 20:37:07" & timestamp < "2015-04-30 05:51:11" ~ "N11",
                                timestamp >= "2015-04-30 05:51:11" & timestamp < "2015-04-30 20:38:47" ~ "D11",
                                timestamp >= "2015-04-30 20:38:47" & timestamp < "2015-05-01 05:49:18" ~ "N12",
                                timestamp >= "2015-05-01 05:49:18" & timestamp < "2015-05-01 20:40:26" ~ "D12",
                                timestamp >= "2015-05-01 20:40:26" & timestamp < "2015-05-02 05:47:26" ~ "N13",
                                timestamp >= "2015-05-02 05:47:26" & timestamp < "2015-05-02 20:42:06" ~ "D13",
                                timestamp >= "2015-05-02 20:42:06" & timestamp < "2015-05-03 05:45:36" ~ "N14",
                                timestamp >= "2015-05-03 05:45:36" & timestamp < "2015-05-03 20:43:45" ~ "D14",
                                timestamp >= "2015-05-03 20:43:45" & timestamp < "2015-05-04 05:43:47" ~ "N15",
                                timestamp >= "2015-05-04 05:43:47" & timestamp < "2015-05-04 20:45:24" ~ "D15",
                                timestamp >= "2015-05-04 20:45:24" & timestamp < "2015-05-05 05:42:00" ~ "N16",
                                timestamp >= "2015-05-05 05:42:00" & timestamp < "2015-05-05 20:47:03" ~ "D16",
                                timestamp >= "2015-05-05 20:47:03" & timestamp < "2015-05-06 05:40:14" ~ "N17",
                                timestamp >= "2015-05-06 05:40:14" & timestamp < "2015-05-06 20:48:41" ~ "D17",
                                timestamp >= "2015-05-06 20:48:41" & timestamp < "2015-05-07 05:38:30" ~ "N18",
                                timestamp >= "2015-05-07 05:38:30" & timestamp < "2015-05-07 20:50:18" ~ "D18",
                                timestamp >= "2015-05-07 20:50:18" & timestamp < "2015-05-08 05:36:47" ~ "N19",
                                timestamp >= "2015-05-08 05:36:47" & timestamp < "2015-05-08 20:51:56" ~ "D19",
                                timestamp >= "2015-05-08 20:51:56" & timestamp < "2015-05-09 05:35:06" ~ "N20",
                                timestamp >= "2015-05-09 05:35:06" & timestamp < "2015-05-09 20:53:32" ~ "D20",
                                timestamp >= "2015-05-09 20:53:32" & timestamp < "2015-05-10 05:33:27" ~ "N21")) 

  # For each fish, remove any time slots where the fish was detected only three or fewer times (i.e., < 4 VPS positions exist)
  bltr_list_15[[i]]$ctmm.data <- bltr_list_15[[i]]$ctmm.data %>%
    dplyr::group_by(TOD) %>%
    dplyr::mutate(TODn = length(TOD)) %>%
    dplyr::filter(TODn > 3) %>%
    dplyr::select(-TODn) %>%
    dplyr::ungroup()
  
  # Save the fish relocation data for each time slot as individual lists
  ctmm.list <- split(bltr_list_15[[i]]$ctmm.data, f = bltr_list_15[[i]]$ctmm.data$TOD)
  
  bltr_list_15[[i]] <- rlist::list.append(bltr_list_15[[i]], "ctmm.list" = ctmm.list)
  
  # Remove the TOD column from the time slot data sets in the ctmm.list object
  for(j in seq_along(bltr_list_15[[i]]$ctmm.list)){
    
    bltr_list_15[[i]]$ctmm.list[[j]] <- bltr_list_15[[i]]$ctmm.list[[j]] %>%
      dplyr::select(-TOD)
    
  }
  
}
  
# Convert ctmm telemetry data sets to ctmm telemetry objects
for(i in seq_along(bltr_list_15)){
  
  for(j in seq_along(bltr_list_15[[i]]$ctmm.list)){
    
    ctmm.telemetry <- ctmm::as.telemetry(bltr_list_15[[i]]$ctmm.list[[j]], timezone = "Canada/Pacific", datum = "WGS84", 
                                         projection = crs("epsg:32610"))
    
    bltr_list_15[[i]]$ctmm.list[[j]] <- rlist::list.append(bltr_list_15[[i]]$ctmm.list[[j]], "ctmm.telemetry" = ctmm.telemetry)
    
  }
}


# 2. Match ctmm object IDs w/ ctmm ID list ####

# Create a new list of bltr ctmm telemetry objects, and change their names to match those in ctmm_ID_list
ctmm_telemetry_list_15 <- unlist(bltr_list_15, recursive = FALSE)

ctmm_telemetry_list_15 <- unlist(ctmm_telemetry_list_15, recursive = FALSE)

ctmm_telemetry_list_15 <- purrr::discard(ctmm_telemetry_list_15, is_double)

ctmm_telemetry_list_15 <- purrr::discard(ctmm_telemetry_list_15, is_character)

ctmm_telemetry_list_15 <- unlist(ctmm_telemetry_list_15, recursive = FALSE)

ctmm_telemetry_list_15 <- purrr::discard(ctmm_telemetry_list_15, is_double)

ctmm_telemetry_list_15 <- purrr::discard(ctmm_telemetry_list_15, is_character)

names(ctmm_telemetry_list_15) <- names(ctmm_ID_list_15)

# Remove objects no longer needed
remove(ctmm_ID_list_15, ctmm.data, ctmm.list, ctmm.telemetry, i, j)

# 3. Model fitting ####

# --- #

# NOTE:

# If the projection is not set below to the median for the list, you cannot visualize rasters of the UD estimates in R with ctmm::plot().
# However, the UD estimates will still be accurate, and the rasters can be exported for plotting in ArcGIS or QGIS.

# BUT, if the projection is set to the median, UD centroids cannot be extracted!!!!!!

#projection(ctmm_telemetry_list_DS) <- median(ctmm_telemetry_list_DS)

# --- #

# The files created during calculation of AKDEs are massive.
# So, in the code below, the list of telemetry objects used to estimate AKDEs is partitioned into smaller lists
# This is critical if AKDEs are to be estimated with a basic laptop or desktop computer

# Load outlet shapefile
# The shapefile is used in the AKDE model to restrict the area over which the UD can be estimated  
outlet <- terra::vect("GIS/Chilko_Outlet_UTM_VPS_Study.shp")
outlet <- as(outlet, "Spatial") # Convert to SPDF
class(outlet)

# Parameters you can tweak
chunk_size <- 10 # define the number of time slots for which AKDEs are estimated in each iteration (too many will crash the session)
outdir <- "DATA/AKDE_data_sets_2015" # create a directory for saving outputs
dir.create(outdir, showWarnings = FALSE, recursive = TRUE)

# Split the list of telemetry objects into chunks (last chunk can be smaller)
idx <- seq_along(ctmm_telemetry_list_15)
chunk_ids <- ceiling(idx / chunk_size)
ctmm_chunks <- split(ctmm_telemetry_list_15, chunk_ids) # list of lists
# Names of elements within each chunk are preserved

# Define a function to extract UD area + centroid from a single AKDE object
extract_ud_metrics <- function(ud_obj, id_label) {
  # Area CI (95% UD) and effective sample size
  s <- summary(ud_obj, units = FALSE)
  UD_est   <- s$CI[2]
  UD_low   <- s$CI[1]
  UD_high  <- s$CI[3]
  effective_n <- s$DOF[1]
  
  # Centroid: coordinates for the mode of the UD (minimum CDF value over the raster grid)
  foo <- data.frame(ud_obj$CDF)
  X   <- data.frame(ud_obj$r$x)
  Y   <- ud_obj$r$y
  colnames(foo) <- Y
  XY <- dplyr::bind_cols(X, foo)
  colnames(XY)[1] <- "X"
  
  centroid.coords <- XY %>%
    tidyr::pivot_longer(!X, names_to = "Y", values_to = "UD") %>%
    dplyr::arrange(UD) %>%
    dplyr::slice(1) %>%                       
    dplyr::mutate(
      X = round(as.numeric(X), 2),
      Y = round(as.numeric(Y), 2)
    )
  
  data.frame(
    ID = id_label,
    UD_est = UD_est,
    UD_low = UD_low,
    UD_high = UD_high,
    effective_n = effective_n,
    centroid_X = centroid.coords$X,
    centroid_Y = centroid.coords$Y,
    stringsAsFactors = FALSE
  )
}

# Loop over chunks to fit the AKDE model and extract metrics for UD size and centroid coordinates
AKDE_data_15 <- dplyr::tibble()   # Empty results table for saving UD sizes and centroid coordinates

for (k in seq_along(ctmm_chunks)) {
  telem_list <- ctmm_chunks[[k]]
  
  # 1) Fit candidate movement models for each telemetry object in this chunk
  FITS <- lapply(telem_list, function(telem) {
    GUESS <- ctmm::ctmm.guess(telem, interactive = FALSE)
    ctmm::ctmm.select(telem, CTMM = GUESS, method = "pHREML", trace = 3)
  })
  names(FITS) <- names(telem_list)
  
  # Save FITS for this chunk
  fits_path <- file.path(outdir, sprintf("bltr_timeslot_fitted_mods_%02d.rda", k))
  save(FITS, file = fits_path)
  
  # 2) AKDE for this chunk
  wAKDEc <- ctmm::akde(
    telem_list,
    FITS,
    weights = TRUE,
    SP = outlet,
    SP.in = TRUE,
    trace = TRUE,
    units = FALSE
  )
  
  # Save AKDEs for this chunk
  akde_path <- file.path(outdir, sprintf("bltr_wAKDEc_timeslot_estimates_%02d.rda", k))
  save(wAKDEc, file = akde_path)
  
  # 3) Extract UD areas + centroids for each fish in this chunk and append
  chunk_df <- dplyr::bind_rows(
    mapply(
      FUN = extract_ud_metrics,
      ud_obj = wAKDEc,
      id_label = names(wAKDEc),
      SIMPLIFY = FALSE
    )
  ) %>%
    dplyr::mutate(timeslot = k)
  
  AKDE_data_15 <- dplyr::bind_rows(AKDE_data_15, chunk_df)
}

# Set the row names to their default
rownames(AKDE_data_15) <- NULL

```

```{r Centroid_to_fence_distance_2015, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# Distances between each UD centroid and the location of the smolt fence are calculated along the shortest in-water path

# Assign SpatRaster to another object
cost_spat <- outlet.raster

# Barriers must be NA; passable cells = 1
cost_spat <- classify(cost_spat, rcl = matrix(c(-Inf, 0, NA), ncol=3, byrow=TRUE), include.lowest=TRUE)
cost_spat[!is.na(cost_spat)] <- 1

# Convert SpatRaster to RasterLayer for compatibility with gdistance
cost_raster <- raster::raster(cost_spat)          
stopifnot(inherits(cost_raster, "RasterLayer"))

# Create a transition layer (tr) and a geo-corrected transition layer (trc)
tr  <- gdistance::transition(cost_raster, function(v) 1/mean(v), directions = 16)
trc <- gdistance::geoCorrection(tr, type = "c", multpl = FALSE)

# Ensure coordinates for fence location & bltr UD centroids are in the same CRS as the raster
bltr_sf  <- sf::st_as_sf(AKDE_data_15, coords = c("centroid_X","centroid_Y"), crs = terra::crs(cost_spat))
fence_sf <- sf::st_as_sf(tibble::tibble(location="fence", Easting = 420929, Northing = 5720007),
                         coords = c("Easting","Northing"), crs = terra::crs(cost_spat))

# Set the target coordinates for the fence location
B <- matrix(sf::st_coordinates(fence_sf), nrow = 1)

# Calculate shortest-path distances in meters
acc <- gdistance::accCost(trc, B)  # RasterLayer of accumulated (shortest path) distance
dists <- terra::extract(terra::rast(acc), terra::vect(bltr_sf))[,2]
bltr_sf$UD_fence_distance_m <- dists

table(is.na(dists))   # TRUE here = unreachable (completely blocked by barriers)

# Drop geometry and timeslot columns
# Format results table
xy <- sf::st_coordinates(bltr_sf)

AKDE_15 <- sf::st_drop_geometry(bltr_sf)

AKDE_15$centroid_X <- xy[,"X"]
AKDE_15$centroid_Y <- xy[,"Y"]

AKDE_15 <- AKDE_15 %>%
  dplyr::select(-timeslot) %>%
  dplyr::mutate(row = row_number()) %>%
  dplyr::relocate(1:5, centroid_X, centroid_Y, row, UD_fence_distance_m)

# Save UD size estimates and distances from UD centroids to fence location
#saveRDS(AKDE_15, "DATA/Data_for_analysis/AKDE_data_15.rds")

```

```{r Distance_between_sequential_positions_2015, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# Using the bltr positions data, calculate the shortest-path distance between sequential positions

# Assign SpatRaster to another object
cost_spat <- outlet.raster

# Barriers must be NA; passable cells = 1
cost_spat <- classify(cost_spat, rcl = matrix(c(-Inf, 0, NA), 1, 3, byrow=TRUE), include.lowest=TRUE)
cost_spat[!is.na(cost_spat)] <- 1

# Convert SpatRaster to RasterLayer for compatibility with gdistance
cost_raster <- raster::raster(cost_spat)          
stopifnot(inherits(cost_raster, "RasterLayer"))

# Create a transition layer (tr) and a geo-corrected transition layer (trc)
tr  <- gdistance::transition(cost_raster, function(v) 1/mean(v), directions = 16)
trc <- gdistance::geoCorrection(tr, type = "c", multpl = FALSE)

# Convert positions to sf object
locs_sf <- sf::st_as_sf(bltr.positions.15, coords = c("Easting","Northing"), crs = terra::crs(cost_spat))

# Extract numeric coords so we can build sequential pairs easily
xy <- sf::st_coordinates(locs_sf)

pairs_df <- locs_sf %>%
  mutate(X = xy[,1], Y = xy[,2]) %>%
  group_by(Fish_ID) %>%
  arrange(timestamp, .by_group = TRUE) %>%
  mutate(X_next = lead(X),
         Y_next = lead(Y),
         timestamp_next = lead(timestamp)) %>%
  filter(!is.na(X_next) & !is.na(Y_next)) %>%
  ungroup() %>%
  sf::st_drop_geometry()

# Set-up for parallel computing via future package
options(future.globals.maxSize = 1 * 1024^3)
future::plan(multisession, workers = max(1, parallel::detectCores() - 1))
on.exit(future::plan(sequential), add = TRUE)

# Split by fish ID and compute shortest-path distance per step (i.e., for each pair of sequential positions per bull trout)
# Function that computes one pair’s distance
pair_dist <- function(x1, y1, x2, y2) {
  A <- matrix(c(x1, y1), nrow = 1)
  B <- matrix(c(x2, y2), nrow = 1)
  # Returns length-1 numeric; NA/Inf if unreachable (fully blocked by NA cells)
  as.numeric(gdistance::costDistance(trc, A, B))
}

by_id <- split(pairs_df, pairs_df$Fish_ID)

res_list <- furrr::future_map(
  by_id,
  ~ {
    df <- .x
    df$step_dist_m <- mapply(pair_dist, df$X, df$Y, df$X_next, df$Y_next)
    df$reachable   <- is.finite(df$step_dist_m)
    df
  },
  .options = furrr::furrr_options(seed = NULL)
)

# Save the calculated distances
bltr_distances_15 <- list_rbind(res_list) %>%
  transmute(
    Fish_ID,
    timestamp_start = timestamp,
    timestamp_end = timestamp_next,
    from_Easting = X, 
    from_Northing = Y,
    to_Easting = X_next, 
    to_Northing = Y_next,
    step_dist_m,
    reachable
  )


bltr.temp <- bltr_distances_15 %>%
  dplyr::group_by(Fish_ID) %>%
  dplyr::slice_min(timestamp_start) %>%
  dplyr::select(Fish_ID, timestamp_start, from_Easting, from_Northing) %>%
  dplyr::rename(timestamp = timestamp_start,
                Easting = from_Easting,
                Northing = from_Northing) %>%
  dplyr::mutate(step_dist_m = NA)
  

bltr_distances_15 <- bltr_distances_15 %>%
  dplyr::select(Fish_ID, timestamp_end, to_Easting, to_Northing, step_dist_m) %>%
  dplyr::rename(timestamp = timestamp_end,
                Easting = to_Easting,
                Northing = to_Northing)

bltr_distances_15 <- bltr_distances_15 %>%
  dplyr::bind_rows(bltr.temp) %>%
  dplyr::arrange(Fish_ID, timestamp)

# Reset parallel computing to sequential
plan(sequential)

# Save distances to file
#saveRDS(bltr_distances_15, "DATA/Data_for_analysis/movement_data_15.rds")

```

# Step 3. Pre-analysis data formatting

Format and add covariates to each of the movement metrics data sets.

The processed data loaded here includes earlier versions of bull trout positions / VPS data. These data sets are removed and data sets created in 'Step 2. Calculate movement metrics' are loaded. Only the data sets for covariates are retained from the processed data.

```{r Load_processed_data_and_movement_metrics_2014, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Load processed data, but only retain data sets for covariates

processed_datasets <- readRDS("DATA/processed_datasets.rds")

list2env(processed_datasets, envir = .GlobalEnv)

remove(processed_datasets, bltr.positions.14, bltr.positions.15, receiver.locations.14, receiver.locations.15,
       receivers.14, receivers.15, ref.tag.locations.14, ref.tag.locations.15, reference.tag.positions.14,
       reference.tag.positions.15)


# Load data sets for movement metrics calculated in Step 2

AKDE_data_US <- readRDS("DATA/data_for_analysis/AKDE_data_US.rds")

AKDE_data_DS <- readRDS("DATA/data_for_analysis/AKDE_data_DS.rds")

AKDE_data_15 <- readRDS("DATA/data_for_analysis/AKDE_data_15.rds")

MVMT_data_US <- readRDS("DATA/data_for_analysis/movement_data_US.rds")

MVMT_data_DS <- readRDS("DATA/data_for_analysis/movement_data_DS.rds")

MVMT_data_15 <- readRDS("DATA/data_for_analysis/movement_data_15.rds")

```

The 'split-at-gap' function in the code chunk below is used for formatting the movement data sets.

```{r Split_at_gap_function, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Define a function that can be used to:
# (1) split movement paths into multiple tracks based on time difference between sequential locations 
# (2) after splitting complete paths into tracks, delete tracks that are less than a defined time period

split_at_gap <- function(data, max_gap = 60, shortest_track = 0) {
    # Number of tracks
    n_tracks <- length(unique(data$ID))
    
    # Save old ID and reinitialise ID column
    data$ID_old <- data$ID
    data$ID <- character(nrow(data))
    
    # Loop over tracks (i.e., over IDs)
    for(i_track in 1:n_tracks) {
        # Indices for this track
        ind_this_track <- which(data$ID_old == unique(data$ID_old)[i_track])
        track_length <- length(ind_this_track)
        
        # Time intervals in min
        dtimes <- difftime(data$time[ind_this_track[-1]], 
                           data$time[ind_this_track[-track_length]],
                           units = "mins")
        
        # Indices of gaps longer than max_gap
        ind_gap <- c(0, which(dtimes > max_gap), track_length)
        
        # Create new ID based on split track
        subtrack_ID <- rep(1:(length(ind_gap) - 1), diff(ind_gap))
        data$ID[ind_this_track] <- paste0(data$ID_old[ind_this_track], "-", subtrack_ID)
    }
    
    # Only keep sub-tracks longer than some duration
    track_lengths <- sapply(unique(data$ID), function(id) {
        ind <- which(data$ID == id)
        difftime(data$time[ind[length(ind)]], data$time[ind[1]], units = "min")
    })
    ID_keep <- names(track_lengths)[which(track_lengths >= shortest_track)]
    data <- subset(data, ID %in% ID_keep)
    
    return(data)
}

```

Below, data sets are compiled for analyses of movement metrics. 

```{r Compile_AKDE_data_for_analysis, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Format smolt density data ####

# Format the smolt density data for combination w/ AKDE 2014 data below

# Time slot information based on the rounded start and stop dates/times needs to be added to the smolt density data set.
# Also, filter the smolt density data to remove density estimates for dates prior to the beginning of bull trout tracking.

smolt.density.summary <- smolt.density %>%
  dplyr::filter(DateTime > "2014-04-26 20:00:00") %>%
  dplyr::mutate(Timeslot = case_when(DateTime < "2014-04-27 06:00:00" ~ "N1",
                                DateTime >= "2014-04-27 06:00:00" & DateTime < "2014-04-27 21:00:00" ~ "D1",
                                DateTime >= "2014-04-27 21:00:00" & DateTime < "2014-04-28 06:00:00" ~ "N2",
                                DateTime >= "2014-04-28 06:00:00" & DateTime < "2014-04-28 21:00:00" ~ "D2",
                                DateTime >= "2014-04-28 21:00:00" & DateTime < "2014-04-29 06:00:00" ~ "N3",
                                DateTime >= "2014-04-29 06:00:00" & DateTime < "2014-04-29 21:00:00" ~ "D3",
                                DateTime >= "2014-04-29 21:00:00" & DateTime < "2014-04-30 06:00:00" ~ "N4",
                                DateTime >= "2014-04-30 06:00:00" & DateTime < "2014-04-30 21:00:00" ~ "D4",
                                DateTime >= "2014-04-30 21:00:00" & DateTime < "2014-05-01 06:00:00" ~ "N5",
                                DateTime >= "2014-05-01 06:00:00" & DateTime < "2014-05-01 21:00:00" ~ "D5",
                                DateTime >= "2014-05-01 21:00:00" & DateTime < "2014-05-02 06:00:00" ~ "N6",
                                DateTime >= "2014-05-02 06:00:00" & DateTime < "2014-05-02 21:00:00" ~ "D6",
                                DateTime >= "2014-05-02 21:00:00" & DateTime < "2014-05-03 06:00:00" ~ "N7",
                                DateTime >= "2014-05-03 06:00:00" & DateTime < "2014-05-03 21:00:00" ~ "D7",
                                DateTime >= "2014-05-03 21:00:00" & DateTime < "2014-05-04 06:00:00" ~ "N8",
                                DateTime >= "2014-05-04 06:00:00" & DateTime < "2014-05-04 21:00:00" ~ "D8",
                                DateTime >= "2014-05-04 21:00:00" & DateTime < "2014-05-05 06:00:00" ~ "N9",
                                DateTime >= "2014-05-05 06:00:00" & DateTime < "2014-05-05 21:00:00" ~ "D9",
                                DateTime >= "2014-05-05 21:00:00" & DateTime < "2014-05-06 06:00:00" ~ "N10",
                                DateTime >= "2014-05-06 06:00:00" & DateTime < "2014-05-06 21:00:00" ~ "D10",
                                DateTime >= "2014-05-06 21:00:00" & DateTime < "2014-05-07 06:00:00" ~ "N11",
                                DateTime >= "2014-05-07 06:00:00" & DateTime < "2014-05-07 21:00:00" ~ "D11",
                                DateTime >= "2014-05-07 21:00:00" & DateTime < "2014-05-08 06:00:00" ~ "N12",
                                DateTime >= "2014-05-08 06:00:00" & DateTime < "2014-05-08 21:00:00" ~ "D12",
                                DateTime >= "2014-05-08 21:00:00" & DateTime < "2014-05-09 06:00:00" ~ "N13",
                                DateTime >= "2014-05-09 06:00:00" & DateTime < "2014-05-09 21:00:00" ~ "D13",
                                DateTime >= "2014-05-09 21:00:00" & DateTime < "2014-05-10 06:00:00" ~ "N14",
                                DateTime >= "2014-05-10 06:00:00" & DateTime < "2014-05-10 21:00:00" ~ "D14",
                                DateTime >= "2014-05-10 21:00:00" & DateTime < "2014-05-11 06:00:00" ~ "N15",
                                DateTime >= "2014-05-11 06:00:00" & DateTime < "2014-05-11 21:00:00" ~ "D15",
                                DateTime >= "2014-05-11 21:00:00" & DateTime < "2014-05-12 06:00:00" ~ "N16",
                                DateTime >= "2014-05-12 06:00:00" & DateTime < "2014-05-12 21:00:00" ~ "D16",
                                DateTime >= "2014-05-12 21:00:00" & DateTime < "2014-05-13 06:00:00" ~ "N17",
                                DateTime >= "2014-05-13 06:00:00" & DateTime < "2014-05-13 21:00:00" ~ "D17",
                                DateTime >= "2014-05-13 21:00:00" & DateTime < "2014-05-14 06:00:00" ~ "N18",
                                DateTime >= "2014-05-14 06:00:00" & DateTime < "2014-05-14 21:00:00" ~ "D18",
                                DateTime >= "2014-05-14 21:00:00" & DateTime < "2014-05-15 06:00:00" ~ "N19",
                                DateTime >= "2014-05-15 06:00:00" & DateTime < "2014-05-15 21:00:00" ~ "D19",
                                DateTime >= "2014-05-15 21:00:00" & DateTime < "2014-05-16 06:00:00" ~ "N20",
                                DateTime >= "2014-05-16 06:00:00" & DateTime < "2014-05-16 21:00:00" ~ "D20",
                                DateTime >= "2014-05-16 21:00:00" & DateTime < "2014-05-17 06:00:00" ~ "N21",
                                DateTime >= "2014-05-17 06:00:00" & DateTime < "2014-05-17 21:00:00" ~ "D21",
                                DateTime >= "2014-05-17 21:00:00" & DateTime < "2014-05-18 06:00:00" ~ "N22",
                                DateTime >= "2014-05-18 06:00:00" & DateTime < "2014-05-18 21:00:00" ~ "D22",
                                DateTime >= "2014-05-18 21:00:00" & DateTime < "2014-05-19 06:00:00" ~ "N23"))


# 2. Combine AKDE data sets ####

# For analysis of UD centroid distance to the fence, centroids downstream of the fence are transformed to negative values
# For 2015 data, DS centroids are those with a Northing coordinate > the Northing coordinate for the fence deployment location (5720007) 
AKDE_data_DS <- AKDE_data_DS %>%
  dplyr::mutate(UD_fence_distance_m = -abs(UD_fence_distance_m))

AKDE_data_15 <- AKDE_data_15 %>%
  dplyr::mutate(UD_fence_distance_m = if_else(centroid_Y < 5720007, UD_fence_distance_m, -abs(UD_fence_distance_m)))

# Add a year column to 2014 and 2015 data sets
AKDE_data_14 <- AKDE_data_DS %>%
  dplyr::bind_rows(AKDE_data_US) %>%
  dplyr::mutate(study_year = "2014")

AKDE_data_15 <- AKDE_data_15 %>%
  dplyr::mutate(study_year = "2015")

# Before combining data sets for 2014 and 2015, bltr size measurements and smolt density estimates must be added to the 2014 data.
bltr.temp <- bltr.data.14 %>%
  dplyr::select(Fish_ID, FL_cm, Mass_kg)

AKDE_data_14 <- AKDE_data_14 %>%
  tidyr::separate(ID, into = c("Fish_ID", "Timeslot"), sep = "_", remove = TRUE) %>%
  dplyr::left_join(bltr.temp, by = "Fish_ID")

# The timing of sunrise and sunset were used to define daytime and nighttime periods for each day.
# But, smolt density data are provided at hourly intervals.
# So, the smolt data that will be associated with each time slot must be rounded to the nearest hour.

# First, add 'start' and 'stop' times based on the timing of sunrise and sunset to the AKDE data for each time slot.

AKDE_data_14 <- AKDE_data_14 %>%
  dplyr::mutate(start = case_when(Timeslot == "N1" ~ "2014-04-26 20:32:32", Timeslot == "D1" ~ "2014-04-27 05:56:29",
                                  Timeslot == "N2" ~ "2014-04-27 20:34:12", Timeslot == "D2" ~ "2014-04-28 05:54:32", 
                                  Timeslot == "N3" ~ "2014-04-28 20:35:53", Timeslot == "D3" ~ "2014-04-29 05:52:37",
                                  Timeslot == "N4" ~ "2014-04-29 20:37:33", Timeslot == "D4" ~ "2014-04-30 05:50:43", 
                                  Timeslot == "N5" ~ "2014-04-30 20:39:13", Timeslot == "D5" ~ "2014-05-01 05:48:50", 
                                  Timeslot == "N6" ~ "2014-05-01 20:40:53", Timeslot == "D6" ~ "2014-05-02 05:46:59",
                                  Timeslot == "N7" ~ "2014-05-02 20:42:32", Timeslot == "D7" ~ "2014-05-03 05:45:09", 
                                  Timeslot == "N8" ~ "2014-05-03 20:44:11", Timeslot == "D8" ~ "2014-05-04 05:43:20", 
                                  Timeslot == "N9" ~ "2014-05-04 20:45:50", Timeslot == "D9" ~ "2014-05-05 05:41:33",
                                  Timeslot == "N10" ~ "2014-05-05 20:47:28", Timeslot == "D10" ~ "2014-05-06 05:39:48", 
                                  Timeslot == "N11" ~ "2014-05-06 20:49:06", Timeslot == "D11" ~ "2014-05-07 05:38:04", 
                                  Timeslot == "N12" ~ "2014-05-07 20:50:44", Timeslot == "D12" ~ "2014-05-08 05:36:22",
                                  Timeslot == "N13" ~ "2014-05-08 20:52:21", Timeslot == "D13" ~ "2014-05-09 05:34:42", 
                                  Timeslot == "N14" ~ "2014-05-09 20:53:57", Timeslot == "D14" ~ "2014-05-10 05:33:03",
                                  Timeslot == "N15" ~ "2014-05-10 20:55:33", Timeslot == "D15" ~ "2014-05-11 05:31:26",
                                  Timeslot == "N16" ~ "2014-05-11 20:57:08", Timeslot == "D16" ~ "2014-05-12 05:29:51", 
                                  Timeslot == "N17" ~ "2014-05-12 20:58:43", Timeslot == "D17" ~ "2014-05-13 05:28:18", 
                                  Timeslot == "N18" ~ "2014-05-13 21:00:16", Timeslot == "D18" ~ "2014-05-14 05:26:46",
                                  Timeslot == "N19" ~ "2014-05-14 21:01:49", Timeslot == "D19" ~ "2014-05-15 05:25:17", 
                                  Timeslot == "N20" ~ "2014-05-15 21:03:21", Timeslot == "D20" ~ "2014-05-16 05:23:50", 
                                  Timeslot == "N21" ~ "2014-05-16 21:04:52", Timeslot == "D21" ~ "2014-05-17 05:22:25",
                                  Timeslot == "N22" ~ "2014-05-17 21:06:22", Timeslot == "D22" ~ "2014-05-18 05:21:02", 
                                  Timeslot == "N23" ~ "2014-05-18 21:07:51")) %>%
  
  dplyr::mutate(stop = case_when(Timeslot == "N1" ~ "2014-04-27 05:56:29", Timeslot == "D1" ~ "2014-04-27 20:34:12",
                                 Timeslot == "N2" ~ "2014-04-28 05:54:32", Timeslot == "D2" ~ "2014-04-28 20:35:53", 
                                 Timeslot == "N3" ~ "2014-04-29 05:52:37", Timeslot == "D3" ~ "2014-04-29 20:37:33",
                                 Timeslot == "N4" ~ "2014-04-30 05:50:43", Timeslot == "D4" ~ "2014-04-30 20:39:13", 
                                 Timeslot == "N5" ~ "2014-05-01 05:48:50", Timeslot == "D5" ~ "2014-05-01 20:40:53",
                                 Timeslot == "N6" ~ "2014-05-02 05:46:59", Timeslot == "D6" ~ "2014-05-02 20:42:32",
                                 Timeslot == "N7" ~ "2014-05-03 05:45:09", Timeslot == "D7" ~ "2014-05-03 20:44:11", 
                                 Timeslot == "N8" ~ "2014-05-04 05:43:20", Timeslot == "D8" ~ "2014-05-04 20:45:50", 
                                 Timeslot == "N9" ~ "2014-05-05 05:41:33", Timeslot == "D9" ~ "2014-05-05 20:47:28",
                                 Timeslot == "N10" ~ "2014-05-06 05:39:48", Timeslot == "D10" ~ "2014-05-06 20:49:06", 
                                 Timeslot == "N11" ~ "2014-05-07 05:38:04", Timeslot == "D11" ~ "2014-05-07 20:50:44", 
                                 Timeslot == "N12" ~ "2014-05-08 05:36:22", Timeslot == "D12" ~ "2014-05-08 20:52:21",
                                 Timeslot == "N13" ~ "2014-05-09 05:34:42", Timeslot == "D13" ~ "2014-05-09 20:53:57", 
                                 Timeslot == "N14" ~ "2014-05-10 05:33:03", Timeslot == "D14" ~ "2014-05-10 20:55:33", 
                                 Timeslot == "N15" ~ "2014-05-11 05:31:26", Timeslot == "D15" ~ "2014-05-11 20:57:08",
                                 Timeslot == "N16" ~ "2014-05-12 05:29:51", Timeslot == "D16" ~ "2014-05-12 20:58:43", 
                                 Timeslot == "N17" ~ "2014-05-13 05:28:18", Timeslot == "D17" ~ "2014-05-13 21:00:16", 
                                 Timeslot == "N18" ~ "2014-05-14 05:26:46", Timeslot == "D18" ~ "2014-05-14 21:01:49",
                                 Timeslot == "N19" ~ "2014-05-15 05:25:17", Timeslot == "D19" ~ "2014-05-15 21:03:21", 
                                 Timeslot == "N20" ~ "2014-05-16 05:23:50", Timeslot == "D20" ~ "2014-05-16 21:04:52", 
                                 Timeslot == "N21" ~ "2014-05-17 05:22:25", Timeslot == "D21" ~ "2014-05-17 21:06:22",
                                 Timeslot == "N22" ~ "2014-05-18 05:21:02", Timeslot == "D22" ~ "2014-05-18 21:07:51", 
                                 Timeslot == "N23" ~ "2014-05-19 05:19:41"))

# Create a data set that summarizes the start and stop times for the photo period time slots
timeslots <- AKDE_data_14 %>%
  dplyr::select(Timeslot, start, stop) %>%
  dplyr::arrange(start) %>%
  dplyr::distinct()

# Smolt density data are provided at hourly intervals.
# Thus, to match these data with the AKDE time slot estimates, the start and stop times for each time slot must be rounded.
# All time slots that are during daytime will begin at 06:00 and end at 21:00.
# All time slots that are during nighttime will begin at 21:00 and end at 06:00 on the following day.

AKDE_data_14 <- AKDE_data_14 %>%
  dplyr::mutate(start.time = "06:00:00",
                stop.time = "21:00:00",
                start.date = as.POSIXct(start, format = "%Y-%m-%d"),
                stop.date = as.POSIXct(stop, format = "%Y-%m-%d")) %>%
  tidyr::unite(start.rounded, start.date, start.time, sep = " ", remove = FALSE) %>%
  tidyr::unite(stop.rounded, stop.date, stop.time, sep = " ", remove = FALSE) %>%
  dplyr::mutate(start.rounded = as.POSIXct(start.rounded, format = "%Y-%m-%d %H:%M:%S", tz = "Canada/Pacific")) %>%
  dplyr::mutate(stop.rounded = as.POSIXct(stop.rounded, format = "%Y-%m-%d %H:%M:%S", tz = "Canada/Pacific")) %>%
  dplyr::select(row, Fish_ID, Timeslot, UD_est, UD_fence_distance_m, study_year, FL_cm, Mass_kg, start, stop, start.rounded, stop.rounded)


# Add a row number to the smolt.density.summary data
timeslots <- timeslots %>%
  dplyr::mutate(row_no = row_number())

foo <- timeslots %>%
  dplyr::select(Timeslot, row_no)

smolt.density.summary <- smolt.density.summary %>%
  dplyr::left_join(foo, by = "Timeslot")

# Summarize smolt density by time slot
smolt.density.summary <- smolt.density.summary %>%
  dplyr::group_by(Timeslot, row_no) %>%
  dplyr::summarise(total_smolts = sum(smolt_count)) %>%
  dplyr::arrange(row_no)

# Combine the summarized smolt density data w/ the AKDE data
AKDE_data_14 <- AKDE_data_14 %>%
  dplyr::left_join(smolt.density.summary, by = "Timeslot") %>%
  dplyr::select(-c(row_no, start, stop, start.rounded, stop.rounded))

# Remove columns from AKDE_data_15 that are not in AKDE_data_14, and add those that are missing
AKDE_data_15 <- AKDE_data_15 %>%
  tidyr::separate(ID, into = c("Fish_ID", "Timeslot"), sep = "_", remove = TRUE) %>%
  dplyr::select(-c(UD_low, UD_high, effective_n, centroid_X, centroid_Y)) %>%
  dplyr::relocate(row, Fish_ID, Timeslot, UD_est, UD_fence_distance_m, study_year)

# Combine 2014 and 2015 AKDE data
AKDE_data <- AKDE_data_14 %>%
  dplyr::bind_rows(AKDE_data_15)

# Convert distance from meters to km and UD size from m2 to km2
AKDE_data <- AKDE_data %>%
  dplyr::mutate(relative_dist_km = UD_fence_distance_m/1000,
                UD_km = UD_est * 0.000001)

AKDE_data_14 <- AKDE_data_14 %>%
  dplyr::mutate(relative_dist_km = UD_fence_distance_m/1000,
                UD_km = UD_est * 0.000001)

# Create a column that identifies the diel period as either Day (D) or Night (N)
AKDE_data <- AKDE_data %>%
  tidyr::separate(Timeslot, into = c("Photoperiod", "X"), sep = 1, remove = FALSE) %>%
  dplyr::select(-X)

AKDE_data_14 <- AKDE_data_14 %>%
  tidyr::separate(Timeslot, into = c("Photoperiod", "X"), sep = 1, remove = FALSE) %>%
  dplyr::select(-X)

# Format columns
AKDE_data$Fish_ID <- as.factor(AKDE_data$Fish_ID)
AKDE_data$Timeslot <- as.factor(AKDE_data$Timeslot)
AKDE_data$Photoperiod <- as.factor(AKDE_data$Photoperiod)
AKDE_data$UD_est <- as.numeric(AKDE_data$UD_est)
AKDE_data$UD_km <- as.numeric(AKDE_data$UD_km) 
AKDE_data$UD_fence_distance_m <- as.numeric(AKDE_data$UD_fence_distance_m)
AKDE_data$relative_dist_km <- as.numeric(AKDE_data$relative_dist_km)
AKDE_data$FL_cm <- as.numeric(AKDE_data$FL_cm)
AKDE_data$Mass_kg <- as.numeric(AKDE_data$Mass_kg)
AKDE_data$total_smolts <- as.numeric(AKDE_data$total_smolts)
AKDE_data <- AKDE_data %>% 
  dplyr::mutate(diel_period = if_else(Photoperiod == "D", as.factor(0), as.factor(1)))

AKDE_data_14$Fish_ID <- as.factor(AKDE_data_14$Fish_ID)
AKDE_data_14$Timeslot <- as.factor(AKDE_data_14$Timeslot)
AKDE_data_14$Photoperiod <- as.factor(AKDE_data_14$Photoperiod)
AKDE_data_14$UD_est <- as.numeric(AKDE_data_14$UD_est)
AKDE_data_14$UD_km <- as.numeric(AKDE_data_14$UD_km) 
AKDE_data_14$UD_fence_distance_m <- as.numeric(AKDE_data_14$UD_fence_distance_m)
AKDE_data_14$relative_dist_km <- as.numeric(AKDE_data_14$relative_dist_km)
AKDE_data_14$FL_cm <- as.numeric(AKDE_data_14$FL_cm)
AKDE_data_14$Mass_kg <- as.numeric(AKDE_data_14$Mass_kg)
AKDE_data_14$total_smolts <- as.numeric(AKDE_data_14$total_smolts)
AKDE_data_14 <- AKDE_data_14 %>% 
  dplyr::mutate(diel_period = if_else(Photoperiod == "D", as.factor(0), as.factor(1)))

# 3. Remove temporary objects ####

remove(AKDE_data_15, AKDE_data_DS, AKDE_data_US, bltr.temp, foo, timeslots)

gc()

```

```{r Compile_MVMT_data_for_analysis, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Prepare data to be split into unique tracks ####

# Combine DS and UD data from 2014
MVMT_data_14 <- rbind(MVMT_data_DS, MVMT_data_US)

# Ensure data are arranged before splitting at gaps
MVMT_data_14 <- MVMT_data_14 %>%
  dplyr::arrange(Fish_ID, timestamp) %>%
  dplyr::mutate(study_year = "2014")

MVMT_data_15 <- MVMT_data_15 %>%
  dplyr::arrange(Fish_ID, timestamp) %>%
  dplyr::mutate(study_year = "2015")

# 2. Create lists of bltr positions from the MVMT data sets ####

# Create a nested list of bltr positions with a sub-list for each fish
temp.list.14 <- split(MVMT_data_14, f = MVMT_data_14$Fish_ID)
temp.list.15 <- split(MVMT_data_15, f = MVMT_data_15$Fish_ID)

bltr.list.14 <- lapply(names(temp.list.14), function(group_name) {
  # Wrap each data frame in a sub-list, name the data frame inside each sub-list as 'positions'
  list(positions = temp.list.14[[group_name]])  # Inside the sub-list, name the data frame as 'positions'
  })

bltr.list.15 <- lapply(names(temp.list.15), function(group_name) {
  # Wrap each data frame in a sub-list, name the data frame inside each sub-list as 'positions'
  list(positions = temp.list.15[[group_name]])  # Inside the sub-list, name the data frame as 'positions'
  })

# Assign the grouping variable name to the nested list
names(bltr.list.14) <- names(temp.list.14) 
names(bltr.list.15) <- names(temp.list.15) 

# 3. Split movement paths ####

# Using the split-at-gap function, split movement paths whenever positions are separated by a gap of > 15 minutes; set shortest track argument to = 0

for (i in seq_along(bltr.list.14)) {
  
  # Extract the data frame from the sub-list
  df <- bltr.list.14[[i]]$positions
  
  # Create a column named "ID" for Fish_ID, and one named "time" for DateTime
  df <- df %>%
    dplyr::mutate(ID = Fish_ID,
                  time = timestamp)
  
  # Split the track using the `split-at-gap` function
  # Create new data sets for each combination of max_gap and min_positions 
  df_split <- split_at_gap(data = df, max_gap = 0.25*60, shortest_track = 0)

  # Save the new DFs
  bltr.list.14[[i]]$positions_split <- df_split

}

for (i in seq_along(bltr.list.15)) {
  
  # Extract the data frame from the sub-list
  df <- bltr.list.15[[i]]$positions
  
  # Create a column named "ID" for Fish_ID, and one named "time" for DateTime
  df <- df %>%
    dplyr::mutate(ID = Fish_ID,
                  time = timestamp)
  
  # Split the track using the `split-at-gap` function
  # Create new data sets for each combination of max_gap and min_positions 
  df_split <- split_at_gap(data = df, max_gap = 0.25*60, shortest_track = 0)

  # Save the new DFs
  bltr.list.15[[i]]$positions_split <- df_split

}

# Combine data sets that contain track splitting information
bltr.tracks.14 <- purrr::map_df(bltr.list.14, ~ .x[[2]])
bltr.tracks.15 <- purrr::map_df(bltr.list.15, ~ .x[[2]])

# Use tracks to create a new MVMT data sets
bltr.tracks.14 <- bltr.tracks.14 %>%
  dplyr::group_by(ID) %>%
  dplyr::mutate(time_diff = time - lag(time),
                distance_m = if_else(is.na(time_diff), NA, step_dist_m)) %>%
  dplyr::select(-c(step_dist_m, time, ID_old))

bltr.tracks.15 <- bltr.tracks.15 %>%
  dplyr::group_by(ID) %>%
  dplyr::mutate(time_diff = time - lag(time),
                distance_m = if_else(is.na(time_diff), NA, step_dist_m)) %>%
  dplyr::select(-c(step_dist_m, time, ID_old))


# How many rows were assigned NA values for step distance after movement paths were split into separate tracks?
(x <- sum(is.na(bltr.tracks.14$distance_m))) # 7,145 positions from 2014 data

(x/length(bltr.tracks.14$distance_m))*100 # 15.5% of all positions from 2014

(x <- sum(is.na(bltr.tracks.15$distance_m))) # 1,397 positions from 2015 data

(x/length(bltr.tracks.15$distance_m))*100 # 2.6% of all positions from 2015

# When 2014 and 2015 data are combined, the number of rows assigned NA values after splitting total movement paths into unique tracks is 8,542 (8.6%)


# Remove all rows with NA values
bltr.tracks.14 <- bltr.tracks.14 %>%
  dplyr::filter(!is.na(distance_m))

bltr.tracks.15 <- bltr.tracks.15 %>%
  dplyr::filter(!is.na(distance_m))


# 4. Append smolt density and bltr size data to 2014 tracks ####

# Add FL and Mass data
bltr.temp <- bltr.data.14 %>%
  dplyr::select(Fish_ID, FL_cm, Mass_kg)

bltr.tracks.14 <- bltr.tracks.14 %>%
  dplyr::left_join(bltr.temp, by = "Fish_ID")

# Add smolt density data
# Smolt density data are estimated hourly; so, the bltr.tracks data should be reduced to this same interval
# Smolt date and smolt hour will be used to join the smolt density data
bltr.tracks.14 <- bltr.tracks.14 %>%
  dplyr::arrange(Fish_ID, timestamp) %>%
  dplyr::mutate(hour = as.factor(lubridate::hour(timestamp)),
                smolt_hour = as.numeric(hour)) %>%
  tidyr::separate(timestamp, into = c("Date", "Time"), sep = " ", remove = FALSE) %>%
  dplyr::mutate(Date = as.POSIXct(Date, tz = "Canada/Pacific"),
                smolt_date = as.Date(Date))

# Hour as a factor must be re-leveled 
bltr.tracks.14$hour <- factor(bltr.tracks.14$hour, levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9",
                                                              "10", "11", "12", "13", "14", "15", "16", "17", 
                                                              "18", "19", "20", "21", "22", "23"))

bltr.tracks.14 <- bltr.tracks.14 %>%
  dplyr::mutate(hour_continuous = as.numeric(hour))


# Summarize tracks by hour
bltr.tracks.14.summarized <- bltr.tracks.14 %>% 
  dplyr::group_by(Fish_ID, Date, hour_continuous) %>%
  dplyr::summarise(distance_m = sum(distance_m),
                   .groups = "drop") %>%
  dplyr::arrange(Fish_ID, Date, hour_continuous)


# Add FL to summarized data
foo <- bltr.tracks.14 %>%
  dplyr::ungroup() %>%
  dplyr::select(Fish_ID, FL_cm) %>%
  dplyr::distinct()

bltr.tracks.14.summarized <- bltr.tracks.14.summarized %>%
  dplyr::left_join(foo, by = "Fish_ID")


# Add smolt density to summarized data 
foo <- smolt.density %>%
  dplyr::select(Date, Hour, smolt_count) %>%
  dplyr::rename(hour_continuous = Hour)

bltr.tracks.14.summarized <- bltr.tracks.14.summarized %>%
  dplyr::left_join(foo, by = c("Date", "hour_continuous"))


# Summarize hourly distances moved for 2015 
bltr.tracks.15 <- bltr.tracks.15 %>%
  dplyr::arrange(Fish_ID, timestamp) %>%
  dplyr::mutate(hour = as.factor(lubridate::hour(timestamp))) %>%
  tidyr::separate(timestamp, into = c("Date", "Time"), sep = " ", remove = FALSE) %>%
  dplyr::mutate(Date = as.POSIXct(Date, tz = "Canada/Pacific"))


# Hour as a factor must be re-leveled 
bltr.tracks.15$hour <- factor(bltr.tracks.15$hour, levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9",
                                                              "10", "11", "12", "13", "14", "15", "16", "17", 
                                                              "18", "19", "20", "21", "22", "23"))

bltr.tracks.15 <- bltr.tracks.15 %>%
  dplyr::mutate(hour_continuous = as.numeric(hour))


# Summarize tracks by hour
bltr.tracks.15.summarized <- bltr.tracks.15 %>% 
  dplyr::group_by(Fish_ID, Date, hour_continuous) %>%
  dplyr::summarise(distance_m = sum(distance_m),
                   .groups = "drop") %>%
  dplyr::arrange(Fish_ID, Date, hour_continuous) %>%
  dplyr::mutate(FL_cm = as.numeric(NA),
                smolt_count = as.numeric(NA))

str(bltr.tracks.14.summarized)
str(bltr.tracks.15.summarized)

# Add a year column and combine data sets from both years
bltr.tracks.14.summarized <- bltr.tracks.14.summarized %>%
  dplyr::mutate(study_year = "2014")

bltr.tracks.15.summarized <- bltr.tracks.15.summarized %>%
  dplyr::mutate(study_year = "2015")

bltr.tracks.summarized <- rbind(bltr.tracks.14.summarized, bltr.tracks.15.summarized)

```

```{r Standardize_&_format_covariates, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# Standardize FL and smolt density

# Mean and SD for FL in 2014 (omit the bltr that was never detected)
mean_SD_FL <- bltr.data.14 %>%
  dplyr::filter(Fish_ID != "BT01") %>%
  dplyr::summarize(mean_FL = mean(FL_cm),
                   sd_FL = sd(FL_cm))

# Mean and SD for smolt density for 2014 diel periods (AKDE data are at the resolution of diel period, so extract from AKDE data)
mean_SD_smolts <- AKDE_data_14 %>%
  dplyr::select(Timeslot, total_smolts) %>%
  dplyr::distinct() %>%
  dplyr::summarize(mean_smolts = mean(total_smolts),
                   sd_smolts = sd(total_smolts))

# Mean and SD for hourly smolt density for 2014
mean_SD_smolts_hourly <- smolt.density %>%
  dplyr::filter(Date > "2014-04-25" & Date < "2014-05-20") %>%
  dplyr::summarise(mean_smolts = mean(smolt_count),
                   sd_smolts = sd(smolt_count))

# Standardize FL and smolt count in AKDE data
AKDE_data_14 <- AKDE_data_14 %>%
  dplyr::mutate(z.smolts = ((total_smolts - mean_SD_smolts$mean_smolts) / (mean_SD_smolts$sd_smolts)),
                z.FL = ((FL_cm - mean_SD_FL$mean_FL) / (mean_SD_FL$sd_FL))
                )

# Standardize FL and smolt count in bltr tracks data
bltr.tracks.14.summarized <- bltr.tracks.14.summarized %>%
  dplyr::mutate(z.smolts = (smolt_count - mean_SD_smolts_hourly$mean_smolts) / mean_SD_smolts_hourly$sd_smolts) %>%
  dplyr::mutate(z.FL = (FL_cm - mean_SD_FL$mean_FL) / mean_SD_FL$sd_FL)

# Check that all columns are in the proper format
str(AKDE_data_14)

str(bltr.tracks.14.summarized)
bltr.tracks.14.summarized$Fish_ID <- as.factor(bltr.tracks.14.summarized$Fish_ID)
bltr.tracks.14.summarized <- bltr.tracks.14.summarized %>%
  dplyr::mutate(DoY = lubridate::yday(Date)) %>%
  dplyr::mutate(DoY = as.factor(DoY))

str(AKDE_data)
AKDE_data$study_year <- as.factor(AKDE_data$study_year)
AKDE_data  <- AKDE_data  %>%
  tidyr::unite(ID_year, Fish_ID, study_year, sep = "_", remove = FALSE) %>%
  tidyr::unite(Timeslot, Timeslot, study_year, sep = "_", remove = FALSE) %>%
  dplyr::mutate(ID_year = as.factor(ID_year),
                Timeslot = as.factor(Timeslot))


str(bltr.tracks.summarized)
bltr.tracks.summarized$Fish_ID <- as.factor(bltr.tracks.summarized$Fish_ID)
bltr.tracks.summarized <- bltr.tracks.summarized %>%
  dplyr::mutate(DoY = lubridate::yday(Date)) %>%
  dplyr::mutate(DoY = as.factor(DoY))
bltr.tracks.summarized <- bltr.tracks.summarized %>%
  tidyr::unite(ID_year, Fish_ID, study_year, sep = "_", remove = FALSE) %>%
  tidyr::unite(Day_year, DoY, study_year, sep = "_", remove = FALSE) %>%
  dplyr::mutate(ID_year = as.factor(ID_year),
                Day_year = as.factor(Day_year))
bltr.tracks.summarized <- bltr.tracks.summarized %>%
  dplyr::mutate(study_year = as.factor(study_year))

# Rename bltr.tracks datasets
MVMT_data_14 <- bltr.tracks.14.summarized
MVMT_data <- bltr.tracks.summarized

# Remove objects no longer needed
remove(bltr.data.14, bltr.data.15, bltr.list.14, bltr.list.15, bltr.temp, bltr.tracks.14, bltr.tracks.14.summarized, bltr.tracks.summarized,
       bltr.tracks.15, bltr.tracks.15.summarized, df, df_split, foo, MVMT_data_15, MVMT_data_DS, MVMT_data_US,
       temp.list.14, temp.list.15, i, x, split_at_gap)


```

# Step 4. Statistical analysis & Figures
## 2014 only

```{r Data_exploration, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# Data exploration functions ####

# Write functions from Zuur et al. 2009

panel.cor <- function(x, y, digits=1, prefix="", cex.cor = 6)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r1=cor(x,y,use="pairwise.complete.obs")
  r <- abs(cor(x, y,use="pairwise.complete.obs"))
  txt <- format(c(r1, 0.123456789), digits=digits)[1]
  txt <- paste(prefix, txt, sep="")
  if(missing(cex.cor)) { cex <- 0.9/strwidth(txt) } else {
     cex = cex.cor}
  text(0.5, 0.5, txt, cex = cex * r)
}

panel.smooth2=function (x, y, col = par("col"), bg = NA, pch = par("pch"),
                        cex = 1, col.smooth = "black", span = 2/3, iter = 3, ...)
{
  points(x, y, pch = pch, col = col, bg = bg, cex = cex)
  ok <- is.finite(x) & is.finite(y)
  if (any(ok))
    lines(stats::lowess(x[ok], y[ok], f = span, iter = iter),
          col = 1, ...)
}

panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="white", ...)
}

corvif <- function(dataz) {
  dataz <- as.data.frame(dataz)
  
  #vif part
  form    <- formula(paste("fooy ~ ",paste(strsplit(names(dataz)," "),collapse=" + ")))
  dataz   <- data.frame(fooy=1 + rnorm(nrow(dataz)) ,dataz)
  lm_mod  <- lm(form,dataz)
  
  cat("\n\nVariance inflation factors\n\n")
  print(myvif(lm_mod))
}

myvif <- function(mod) {
  v <- vcov(mod)
  assign <- attributes(model.matrix(mod))$assign
  if (names(coefficients(mod)[1]) == "(Intercept)") {
    v <- v[-1, -1]
    assign <- assign[-1]
  } else warning("No intercept: vifs may not be sensible.")
  terms <- labels(terms(mod))
  n.terms <- length(terms)
  if (n.terms < 2) stop("The model contains fewer than 2 terms")
  if (length(assign) > dim(v)[1] ) {
    diag(tmp_cor)<-0
    if (any(tmp_cor==1.0)){
      return("Sample size is too small, 100% collinearity is present")
    } else {
      return("Sample size is too small")
    }
  }
  R <- cov2cor(v)
  detR <- det(R)
  result <- matrix(0, n.terms, 3)
  rownames(result) <- terms
  colnames(result) <- c("GVIF", "Df", "GVIF^(1/2Df)")
  for (term in 1:n.terms) {
    subs <- which(assign == term)
    result[term, 1] <- det(as.matrix(R[subs, subs])) * det(as.matrix(R[-subs, -subs])) / detR
    result[term, 2] <- length(subs)
  }
  if (all(result[, 2] == 1)) {
    result <- data.frame(GVIF=result[, 1])
  } else {
    result[, 3] <- result[, 1]^(1/(2 * result[, 2]))
  }
  invisible(result)
}


# Check for collinearity - AKDE data ####

# 1. Correlation matrix

UD_data_matrix <- AKDE_data_14 %>%
  dplyr::select(UD_km, diel_period, FL_cm, Mass_kg, total_smolts) %>%
  dplyr::mutate_if(is.factor, as.numeric) %>%
  as.matrix()

# Compute the correlation matrix from 'cor' function
cor_UD_matrix <- stats::cor(UD_data_matrix)

# Rendering
cor_UD_matrix %>% 
  round(2) %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(font_size = 9)

# Plot the correlation matrix
library(corrplot)
corrplot(cor_UD_matrix, type = "upper", order = "hclust", method = "number",
         tl.col = "black", tl.srt = 45, mar = c(0,0,0,0))


# 2. Variance inflation factors

z <- cbind(AKDE_data_14$UD_km, AKDE_data_14$FL_cm, AKDE_data_14$Mass_kg, AKDE_data_14$total_smolts, AKDE_data_14$Photoperiod)

colnames(z) <- c("UD_km", "FL", "Mass", "total_smolts", "photo")

# Calculate VIFs with the custom function "corvif" from Zuur et al. 2009
corvif(z)

# The accepted VIF threshold for determining whether correlation between variables exists is 3-5 (Zuur et al. 2009). 
# Remove one variable at a time until all variables are < 3.
corvif(z[, c(-3)]) # Remove Mass

# Removal of variables w/ VIFs > 3 resulted in the retention of the same variables as filtering based on correlation coefficients.


# Check for collinearity - MVMT data ####

# 1. Correlation matrix

mvmt_data_matrix <- MVMT_data_14 %>%
  dplyr::select(distance_m, hour_continuous, FL_cm, smolt_count) %>%
  dplyr::mutate_if(is.factor, as.numeric) %>%
  as.matrix()

# Compute the correlation matrix from 'cor' function
cor_matrix <- stats::cor(mvmt_data_matrix)

# Rendering
cor_matrix %>% 
  round(2) %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling(font_size = 9)

# Plot the correlation matrix
library(corrplot)
corrplot(cor_matrix, type = "upper", order = "hclust", method = "number",
         tl.col = "black", tl.srt = 45, mar = c(0,0,0,0))


# 2. Variance inflation factors

z <- cbind(MVMT_data_14$distance_m, MVMT_data_14$FL_cm, MVMT_data_14$smolt_count, MVMT_data_14$hour_continuous)

# Calculate VIFs with the custom function "corvif" from Zuur et al. 2009
corvif(z)

```

```{r UD_size_2014, eval=TRUE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Write out the full model GLMM with all original fixed effects and random intercepts #### 

# Gamma w/ log link 
UD.1 <- glmmTMB::glmmTMB(UD_est ~  diel_period * z.smolts * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                         family = Gamma(link = "log"),
                         data = AKDE_data_14)

# Gamma w/ log link and dispersion formula to deal w/ heterogeneity
UD.2 <- glmmTMB::glmmTMB(UD_est ~  diel_period * z.smolts * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                         family = Gamma(link = "log"),
                         dispformula = ~ diel_period,
                         data = AKDE_data_14)

# Tweedie w/ log link
UD.3 <- glmmTMB::glmmTMB(UD_est ~  diel_period * z.smolts * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                         family = glmmTMB::tweedie(link = "log"),
                         data = AKDE_data_14)

# Tweedie w/ log link and dispersion formula to deal w/ heterogeneity
UD.4 <- glmmTMB::glmmTMB(UD_est ~  diel_period * z.smolts * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                         family = glmmTMB::tweedie(link = "log"),
                         dispformula = ~ diel_period,
                         data = AKDE_data_14)

# Compare models w/ different family / link functions
MuMIn::AICc(UD.1, UD.2, UD.3, UD.4)

# Use model 4

# Re-write the global model to an object named "UD.global".
UD.global <- glmmTMB::glmmTMB(UD_est ~  diel_period * z.smolts * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                         family = glmmTMB::tweedie(link = "log"),
                         dispformula = ~ diel_period,
                         data = AKDE_data_14,
                         na.action = "na.fail",
                         REML = FALSE)

# 2. Model selection and averaging ####

# Perform all-subsets regression on the global model
UD.model.list <- MuMIn::dredge(global.model = UD.global, rank = "AICc")

UD.model.subset <- MuMIn::get.models(UD.model.list, subset = delta < 4)

UD.model.avg <- MuMIn::model.avg(UD.model.subset, fit = TRUE)

# 3. Interpretation of results ####

summary(UD.model.avg)

# --- Intercept (Diel Period: Day) ---

# The coefficient for Intercept is 9.20903
exp(9.20903)
# Exponentiated, this value is 9986.905
# So, the expected UD size during the day for a bull trout of average FL at the average smolt migration density is 9,987 m^2.

# --- Diel Period (Night) ---

# The coefficient for diel period (night) is 1.64843
exp(1.64843)
# Exponentiated, this value is 5.198811
# So, the UD size for the avg. FL during nighttime w/ avg. smolt density is expected to be ~5.2-times larger than during the day

exp(9.20903) * exp(1.64843)
# The estimated UD size under these conditions is 51,920 m^2

# 4. Diel period X FL plot (NOT NEEDED; FL effect is not significant) ####

# # Create a sequence of FL values across the full range ()
# FL_seq <- seq(46.2, 71.2, by = 0.1)
# 
# # Standardize the sequence
# z.FL_seq <- (FL_seq - mean_SD_FL$mean_FL) / mean_SD_FL$sd_FL
#   
# # Create new_data
# new_data <- expand.grid(
#   diel_period = levels(AKDE_data$diel_period),
#   z.FL = z.FL_seq,
#   z.smolts = mean.z.smolts$mean
# )
# 
# # Predict response on log scale
# preds <- predict(UD.model.avg, newdata = new_data, full = TRUE, type = "link", re.form = NA, se.fit = TRUE)
# 
# # Compute 95% CIs
# new_data$fit_link <- preds$fit
# new_data$se_link <- preds$se.fit
# 
# # 95% CIs on the link (log) scale
# new_data$lwr_link <- new_data$fit_link - 1.96 * new_data$se_link
# new_data$upr_link <- new_data$fit_link + 1.96 * new_data$se_link
# 
# # Back-transform to response scale
# new_data$fit_resp <- exp(new_data$fit_link)
# new_data$lwr_resp <- exp(new_data$lwr_link)
# new_data$upr_resp <- exp(new_data$upr_link)
# 
# # Add FL on the response scale (back transform standardized values)
# new_data$FL_cm <- new_data$z.FL * mean_SD_FL$sd_FL + mean_SD_FL$mean_FL
# 
# UD_means_14 <- new_data %>%
#   dplyr::select(1, 8:11)
# 
# # Rename levels for diel period
# AKDE_data_14 <- AKDE_data_14 %>%
#   dplyr::mutate(diel_period_plot = if_else(diel_period == "0", "Day", "Night"))
# 
# UD_means_14 <- UD_means_14 %>%
#   dplyr::mutate(diel_period_plot = if_else(diel_period == "0", "Day", "Night"))
# 
# 
# # Compare fitted values across diel periods and FL
# (plot_UD_FL_2014 <- ggplot() +
#   geom_ribbon(data = UD_means_14, aes(x = FL_cm, ymin = lwr_resp/1000, ymax = upr_resp/1000, fill = diel_period_plot), 
#               alpha = 0.5, show.legend = TRUE) +
#   
#   geom_jitter(data = AKDE_data_14, aes(x = FL_cm, y = UD_est/1000, color = diel_period_plot), position = position_jitter(width = 0.25, seed = 0), 
#               shape = 21, size = 1.2, stroke = 0.3, alpha = 1, show.legend = TRUE) +
#   
#   geom_line(data = UD_means_14, aes(x = FL_cm, y = fit_resp/1000, colour = diel_period_plot),
#              linewidth = 0.75, show.legend = TRUE) +
#   
#   scale_fill_manual(values = c("#fdb863", "#a6cee3")) +
#   
#   scale_colour_manual(values = c("#d95f02", "#084594")) +
#   
#   scale_y_continuous(limits = c(0, 420)) +
#   
#   labs(
#     x = "<br>Fork length (cm)",
#     y = "95% utilization distribution<br>(thousands of m<sup>2</sup>)<br>",
#     fill = "Diel period 2014:",
#     colour = "Diel period 2014:") +
#   
#   my_theme +
#   
#   theme(legend.position = "inside",
#         legend.position.inside = c(0.50, 0.95),
#         legend.box = "vertical",
#         legend.direction = "horizontal",
#         legend.text = element_text(size = 10, family = "Arial"),
#         legend.title = element_text(size = 10, family = "Arial"),
#         legend.key.size = unit(0.5, "cm")))
```

```{r UD_centroid_distance_2014, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Linear model ####

# Start with a linear model that includes as many explanatory variables and biologically-sound interactions as possible.  
dist_lm <- lm(relative_dist_km ~ z.smolts * diel_period * z.FL, data = AKDE_data_14)

par(mfrow = c(2,2))
par(mar = c(5, 5, 2, 2))
plot(dist_lm)
par(mfrow = c(1,1))

# 2. Fit the linear model with GLS ####

dist_gls <- nlme::gls(relative_dist_km ~ z.smolts * diel_period * z.FL, data = AKDE_data_14, method = "REML")

# 3. Determine the random component structure of the model ####

# First, we need to code an integer variable that contains only values of 1 for every row.
AKDE_data_14$int <- 1

# Model built w/ nlme package
dist_lme <- nlme::lme(relative_dist_km ~ z.smolts * diel_period * z.FL, 
          random = list(int = nlme::pdIdent(form = ~ 0 + as.factor(Fish_ID)), Timeslot = ~1),
          data = AKDE_data_14, method = "REML", na.action = "na.fail")

# Model built w/ lme4 package
dist_lmer <- lme4::lmer(relative_dist_km ~ z.smolts * diel_period * z.FL +
                        (1 | Timeslot) + (1 | Fish_ID),
                        data = AKDE_data_14, REML = TRUE, na.action = "na.fail")

# Compare the two models to see that the method for including crossed random effects in the nlme::lme() function worked.
summary(dist_lme)
summary(dist_lmer)

MuMIn::AICc(dist_lme, dist_lmer)

# 4. Compare new model with old model ####
MuMIn::AICc(dist_lmer, dist_gls)

# Next, determine which random effects should be retained in the model.

lme.fish <- lme4::lmer(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID), 
                      data = AKDE_data_14, REML = TRUE, na.action = "na.fail")


lme.timeslot <- lme4::lmer(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Timeslot), 
                      data = AKDE_data_14, REML = TRUE, na.action = "na.fail")

MuMIn::AICc(dist_lmer, lme.fish, lme.timeslot)

# The model with both random effects has the lowest AICc.

# 5. Check the assumptions of the random intercept model ####

# Now that the optimal random component of the model has been chosen, we can re-check the variance structure.
dist_lmer <- lme4::lmer(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot), 
                      data = AKDE_data_14, REML = TRUE, na.action = "na.fail") 

par(mfrow = c(1,1))
par(mar = c(5, 5, 2, 2))
plot(residuals(dist_lmer, type = "pearson") ~ fitted(dist_lmer), xlab = "Fitted values", ylab = "Normalized residuals")
par(mfrow = c(1,1))

hist(residuals(dist_lmer))

# The assumption of normality is met, but not the assumption of homogeneity.
# There are a few large residuals that may be outliers, and if removed may help make normality more convincing.

# Check the relationship b/w fence distance and each covariate to determine which covariates might be influencing heterogeneity.

# Store the residuals in an object
NRs <- residuals(dist_lmer, type = "pearson")

hist(NRs) # Residuals are slightly skewed; normality not met.

plot(NRs ~ z.FL, data = AKDE_data_14, xlab = "Length", ylab = "Residuals") 
abline(h = 0, lty = 2)

plot(NRs ~ z.smolts, data = AKDE_data_14, xlab = "smolt count", ylab = "Residuals") 
abline(h = 0, lty = 2)

boxplot(NRs ~ diel_period, data = AKDE_data_14, xlab = "Diel", ylab = "Residuals") 
abline(h = 0, lty = 2)

coplot(NRs ~ z.FL|diel_period, data = AKDE_data_14)
coplot(NRs ~ z.smolts|diel_period, data = AKDE_data_14)

# All covariates look ok; none seem to be driving the observed heterogeneity in the residuals.
# Try to address residual heterogeneity by setting the variance structure for the covariates.

# 6. Define the variance structure for the global model ####

# To include >1 random intercept and account for heterogeneity, use glmmTMB package

dist_lme <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail")

# diel period
dist_2 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ diel_period)

# FL
dist_3 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ z.FL)

# smolts
dist_4 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ z.smolts)

# diel period + FL
dist_5 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ diel_period + z.FL)

# diel period + smolts
dist_6 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ diel_period + z.smolts)

# FL + smolts
dist_7 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ z.FL + z.smolts)

# diel period + smolts + FL
dist_8 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ diel_period + z.smolts + z.FL)

# diel period X smolts
dist_9 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ diel_period * z.smolts)

# diel period X FL
dist_10 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ diel_period * z.FL)

# FL X smolts
dist_11 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ z.FL * z.smolts)

# diel period X smolts X FL
dist_12 <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = TRUE, na.action = "na.fail",
                           dispformula = ~ diel_period * z.smolts * z.FL)


AIC(dist_lme, dist_2, dist_3, dist_4, dist_5, dist_6, dist_7, dist_8, dist_9, dist_10, dist_11, dist_12)

# Model # 2 is the best fit (variance structure for diel period) followed by model # 6 (variance structure for diel period + smolts)

E2 <- residuals(dist_2, type = "pearson") 
coplot(E2 ~ z.smolts | diel_period, data = AKDE_data_14, ylab = "Normalised residuals")
coplot(E2 ~ z.FL | diel_period, data = AKDE_data_14, ylab = "Normalised residuals")
boxplot(E2 ~ diel_period, data = AKDE_data_14, xlab = "Diel", ylab = "Residuals") 
abline(h = 0, lty = 2)

hist(residuals(dist_2, type = "pearson"), breaks = 20)

# Setting the variance structure lowered the model AIC substantially, and helped with the plot of residuals vs fitted values.

remove(dist_lme, dist_2, dist_3, dist_4, dist_5, dist_6, dist_7, dist_8, dist_9, dist_10, dist_11, dist_12)


# Specify global model
dist_global <- glmmTMB::glmmTMB(relative_dist_km ~ z.smolts * diel_period * z.FL + (1 | Fish_ID) + (1 | Timeslot),
                           data = AKDE_data_14, REML = FALSE, na.action = "na.fail",
                           dispformula = ~ diel_period)

# 7. Model selection and averaging ####

# Perform all-subsets regression
dist.model.list <- MuMIn::dredge(global.model = dist_global, rank = "AICc")

dist.model.subset <- MuMIn::get.models(dist.model.list, subset = delta < 4)

dist.model.avg <- MuMIn::model.avg(dist.model.subset, fit = TRUE)

# 8. Interpretation of results ####

summary(dist.model.avg)

# ***Note that coefficient estimates in the MS have been transformed from km to m.***

# --- Intercept (Day) ---

# The intercept is 0.6935524 
# Thus, the expected centroid distance to the fence during the day for a bull trout with average FL at average smolt density is 0.69 km

# --- Diel period (Night) ---
# The coefficient for diel period (night) is -0.4442296
# So, the expected centroid distance to the fence at night for a bull trout w/ average FL and at average smolt density is 0.25 km (0.6935524 - 0.4442296)
# The effect is significant

# --- Fork length (Day) ---
# The coefficient for FL is -0.0719381
# So, the distance to the fence location during daytime is expected to decrease by 0.07 km for each 1 SD increase in bltr FL (~6.4 cm)
# This could be interpreted as daytime UDs are larger for smaller bull trout

# --- Fork length X Night ---
# The coefficient for the interaction b/w diel period (night) and FL is 0.0936539
# We can get the slope for FL X Night by adding the slopes for FL and diel_period: FL
# -0.0719381 + 0.0936539 = 0.0217158
# So, at night, for each 1 SD increase in FL (~6.4 cm) there was an increase in distance to the fence of ~0.02 km

# So the difference between the day and night slopes (~0.09) is statistically significant, even though the slope for day (−0.07) is not.

# 9. Diel period X FL plot ####

# Create a sequence of FL values across the full range (excluding the one fish that was never detected)
FL_seq <- seq(46.2, 71.2, by = 0.1)

# Standardize the sequence
z.FL_seq <- (FL_seq - mean_SD_FL$mean_FL) / mean_SD_FL$sd_FL


mean.z.FL <- AKDE_data_14 %>%
  dplyr::select(Fish_ID, z.FL) %>%
  dplyr::distinct() %>%
  dplyr::summarise(mean = mean(z.FL))

mean.z.smolts <- AKDE_data_14 %>%
  dplyr::select(Timeslot, z.smolts) %>%
  dplyr::distinct() %>%
  dplyr::summarise(mean = mean(z.smolts))


# Create new_data
new_data <- expand.grid(
  diel_period = levels(AKDE_data_14$diel_period),
  z.FL = z.FL_seq,
  z.smolts = mean.z.smolts$mean)

# Predict response
preds <- predict(dist.model.avg, newdata = new_data, full = TRUE, type = "response", re.form = NA, se.fit = TRUE)

new_data$fit_resp <- preds$fit
new_data$se_resp <- preds$se.fit

# 95% CIs on the link (log) scale
new_data$lwr_resp <- new_data$fit_resp - 1.96 * new_data$se_resp
new_data$upr_resp <- new_data$fit_resp + 1.96 * new_data$se_resp

# Add FL on the response scale (back transform standardized values)
new_data$FL_cm <- new_data$z.FL * mean_SD_FL$sd_FL + mean_SD_FL$mean_FL

dist_means_14 <- new_data

# Rename levels for diel period
AKDE_data_14 <- AKDE_data_14 %>%
  dplyr::mutate(diel_period_plot = if_else(diel_period == "0", "Day", "Night"))

dist_means_14 <- dist_means_14 %>%
  dplyr::mutate(diel_period_plot = if_else(diel_period == "0", "Day", "Night"))


# Compare fitted values across diel periods and study years 
(plot_centroid_FL_2014 <- ggplot() +
  geom_ribbon(data = dist_means_14, aes(x = FL_cm, ymin = lwr_resp*1000, ymax = upr_resp*1000, fill = diel_period_plot), 
              alpha = 0.5, show.legend = TRUE) +
  
  geom_jitter(data = AKDE_data_14, aes(x = FL_cm, y = relative_dist_km*1000, color = diel_period_plot), position = position_jitter(width = 0.25, seed = 0), 
              shape = 21, size = 1.2, stroke = 0.3, alpha = 1, show.legend = TRUE) +
  
  geom_line(data = dist_means_14, aes(x = FL_cm, y = fit_resp*1000, colour = diel_period_plot),
             linewidth = 0.75, show.legend = TRUE) +
  
  scale_fill_manual(values = c("#fdb863", "#a6cee3")) +
  
  scale_colour_manual(values = c("#d95f02", "#084594")) +
  
  scale_y_continuous(limits = c(-100, 1700), breaks = c(0, 500, 1000, 1500)) +
  
  labs(
    x = "<br>Fork length (cm)",
    y = "Distance between centroids<br>and fence location (m)<br>",
    fill = "Diel period 2014:",
    colour = "Diel period 2014:") +
  
  my_theme +
  
  theme(legend.position = "inside",
        legend.position.inside = c(0.50, 0.95),
        legend.box = "vertical",
        legend.direction = "horizontal",
        legend.text = element_text(size = 10, family = "Arial"),
        legend.title = element_text(size = 10, family = "Arial"),
        legend.key.size = unit(0.5, "cm")))

```

```{r Hourly_distance_moved_2014, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# Set the knots for the cyclic GAMM smoother
knots <- list(hour_continuous = c(0.5, 24.5))

# Fit a model w/ Gaussian distribution 
m0 <- mgcv::gam(distance_m ~ 
                  s(hour_continuous, bs = "cc", k = 12) + 
                  s(z.smolts, bs = "tp", k = 4) +
                  z.FL +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE)

set.seed(12345)
gam.check(m0, rep = 500)
gratia::appraise(m0)
plot(m0, pages = 1, seWithMean = TRUE)


# Gaussian distribution is not appropriate; try Tweedie (Gamma not appropriate b/c of zeros in the response)
m1 <- mgcv::gam(distance_m ~ 
                  s(hour_continuous, bs = "cc", k = 12) + 
                  s(z.smolts, bs = "tp", k = 4) +
                  z.FL +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

set.seed(12345)
gam.check(m1, rep = 500)
gratia::appraise(m1)
plot(m1, pages = 1, seWithMean = TRUE)
summary(m1)


# Model diagnostic plots look better, but max basis functions (k = ) is set too low for hours variable 
m2 <- mgcv::gam(distance_m ~ 
                  s(hour_continuous, bs = "cc", k = 16) + 
                  s(z.smolts, bs = "tp", k = 10) +
                  z.FL +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

set.seed(12345)
gam.check(m2, rep = 500)
gratia::appraise(m2)
plot(m2, pages = 1, seWithMean = TRUE)
summary(m2)

# Max basis functions set appropriately for all covariates. 
# Construct alternative models

m1 <- mgcv::gam(distance_m ~ 1 +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m2 <- mgcv::gam(distance_m ~
                  FL_cm +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m3 <- mgcv::gam(distance_m ~
                  s(hour_continuous, bs = "cc", k = 16) + 
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m4 <- mgcv::gam(distance_m ~
                  s(hour_continuous, bs = "cc", k = 16) + 
                  FL_cm +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m5 <- mgcv::gam(distance_m ~
                  s(smolt_count, bs = "tp", k = 8) +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m6 <- mgcv::gam(distance_m ~
                  s(z.smolts, bs = "tp", k = 8) +
                  FL_cm +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m7 <- mgcv::gam(distance_m ~
                  s(hour_continuous, bs = "cc", k = 16) + 
                  s(smolt_count, bs = "tp", k = 8) +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m8 <- mgcv::gam(distance_m ~
                  s(hour_continuous, bs = "cc", k = 16) + 
                  s(smolt_count, bs = "tp", k = 8) +
                  FL_cm +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m9 <- mgcv::gam(distance_m ~
                  s(hour_continuous, bs = "cc", k = 16) + 
                  s(smolt_count, bs = "tp", k = 8) +
                  ti(hour_continuous, smolt_count, k = c(16, 8), bs = c("cc", "tp")) +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m10 <- mgcv::gam(distance_m ~
                  s(hour_continuous, bs = "cc", k = 16) + 
                  s(smolt_count, bs = "tp", k = 8) +
                  ti(hour_continuous, smolt_count, k = c(16, 8), bs = c("cc", "tp")) +
                  FL_cm +
                  s(Fish_ID, bs = "re") + 
                  s(DoY, bs = "re"), 
                knots = knots, 
                data = MVMT_data_14, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

# Compare AICc values for candidate models
foo <- as.data.frame(MuMIn::AICc(m1,m2,m3,m4,m5,m6,m7,m8,m9,m10))
foo %>%
  dplyr::arrange(AICc)

remove(m1, m2, m4, m5, m6, m7, m8, m9, m10)

summary(m3)


```

## 2014 & 2015

```{r UD_size_14_15, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Linear model ####

UD.lm <- lm(UD_est ~ diel_period * study_year, data = AKDE_data)

par(mfrow = c(2,2))
par(mar = c(5, 5, 2, 2))
plot(UD.lm)
par(mfrow = c(1,1))

hist(residuals(UD.lm))

# 2. Fit the linear model with GLS ####

UD.gls <- nlme::gls(UD_est ~ diel_period * study_year, data = AKDE_data, method = "REML")

# 3. Determine the random component structure of the model ####

# Model built w/ lme4 package; both random intercepts
UD.lmer <- lme4::lmer(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year), 
                      data = AKDE_data, REML = TRUE, na.action = "na.fail")

lme.fish <- lme4::lmer(UD_est ~ diel_period * study_year + (1 | ID_year),
                       data = AKDE_data, REML = TRUE, na.action = "na.fail")


lme.timeslot <- lme4::lmer(UD_est ~ diel_period * study_year + (1 | Timeslot),
                           data = AKDE_data, REML = TRUE, na.action = "na.fail")

MuMIn::AICc(UD.lmer, lme.fish, lme.timeslot, UD.gls)

# Retain both random intercepts in the model

# 4. Check the assumptions of the random intercept model ####

par(mfrow = c(1,1))
par(mar = c(5, 5, 2, 2))
plot(resid(UD.lmer) ~ fitted(UD.lmer), xlab = "Fitted values", ylab = "Normalized residuals")
par(mfrow = c(1,1))

# # Check assumptions w/ DHARMa package (add/remove hash tag from multiple rows w/ cmnd+shift+c)
# simulationOutput <- DHARMa::simulateResiduals(UD.lmer, plot = FALSE)
# DHARMa::plotQQunif(simulationOutput)
# DHARMa::testDispersion(simulationOutput)
# DHARMa::plotResiduals(simulationOutput)
# DHARMa::plotResiduals(simulationOutput, form = AKDE_14_15$study_year)
# DHARMa::plotResiduals(simulationOutput, form = AKDE_14_15$diel_period)
# 
# hist(residuals(UD.lmer))

# Assumptions of heterogeneity and normality are not met.

# 5. Candidate GLMMs ####

UD_1 <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         family = Gamma(link = "log"),
                         REML = FALSE)

UD_2 <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         family = Gamma(link = "log"),
                         REML = FALSE,
                         dispformula = ~ diel_period)

UD_3 <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         family = Gamma(link = "log"),
                         REML = FALSE,
                         dispformula = ~ study_year)

UD_4 <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         family = Gamma(link = "log"),
                         REML = FALSE,
                         dispformula = ~ diel_period + study_year)

UD_5 <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         family = glmmTMB::tweedie(link = "log"),
                         REML = FALSE)

UD_6 <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         family = glmmTMB::tweedie(link = "log"),
                         REML = FALSE,
                         dispformula = ~ diel_period)

UD_7 <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         family = glmmTMB::tweedie(link = "log"),
                         REML = FALSE,
                         dispformula = ~ study_year)

UD_8 <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         family = glmmTMB::tweedie(link = "log"),
                         REML = FALSE,
                         dispformula = ~ diel_period + study_year)


AIC(UD_1, UD_2, UD_3, UD_4, UD_5, UD_6, UD_7, UD_8)


# Re-write the best-fit model to an object named "UD.global".
UD.global <- glmmTMB::glmmTMB(UD_est ~ diel_period * study_year + (1 | Timeslot) + (1 | ID_year),
                         data = AKDE_data, na.action = "na.fail",
                         dispformula = ~ diel_period,
                         family = glmmTMB::tweedie(link = "log"),
                         REML = FALSE)

# 6. Model selection and model averaging ####

# Perform all-subsets regression on the global model
UD.model.list <- MuMIn::dredge(global.model = UD.global, rank = "AICc")

UD.model.subset <- MuMIn::get.models(UD.model.list, subset = delta < 4)

UD.model.avg <- MuMIn::model.avg(UD.model.subset, fit = TRUE)

# 7. Interpretation of results ####

summary(UD.model.avg)

# The coefficient for Intercept is 9.31288
exp(9.31288)
# Exponentiated, this value is 11079.81
# So, the predicted mean UD size for daytime in 2014 is 11,079.8 m^2

# The coefficient for diel period is 1.56400
exp(1.56400)
# Exponentiated, this value is 4.777895
exp(9.31288) * exp(1.56400)
# So, the predicted mean UD size for night in 2014 is ~4.8-times larger (52,938.17 m^2) than daytime in 2014

# The coefficient for study year is 1.21117
exp(1.21117)
# Exponentiated, this value is 3.357411
exp(9.31288) * exp(1.21117)
# So, the predicted mean UD size for day in 2015 is ~3.4-times larger (37,199.48 m^2) than daytime in 2014 

# The model-averaged coefficient for the interaction between diel period and study year is 0.01756
exp(0.01756)
# Exponentiated, this value is 1.017715; but for the interaction term, we must consider all coefficients
exp(1.56400 + 1.21117 + 0.01756)
exp(9.31288) * exp(1.56400 + 1.21117 + 0.01756)
# So, the predicted mean UD size for night in 2015 (180,883.8 m^2) is ~16.3-times the UD size for day in 2014



# 8. Diel period X Study Year plot ####

# Create grid of new data
newdata <- expand.grid(
  diel_period = levels(AKDE_data$diel_period),
  study_year = levels(AKDE_data$study_year)
)

# Predict response on log scale
preds <- predict(UD.model.avg, newdata = newdata, full = TRUE, type = "link", re.form = NA, se.fit = TRUE)

# Compute 95% CIs
newdata$fit_link <- preds$fit
newdata$se_link <- preds$se.fit

# 95% CIs on the link (log) scale
newdata$lwr_link <- newdata$fit_link - 1.96 * newdata$se_link
newdata$upr_link <- newdata$fit_link + 1.96 * newdata$se_link

# Back-transform to response scale
newdata$fit_resp <- exp(newdata$fit_link)
newdata$lwr_resp <- exp(newdata$lwr_link)
newdata$upr_resp <- exp(newdata$upr_link)

UD_means <- newdata %>%
  dplyr::select(1, 2, 7:9)

# Rename levels for diel period
AKDE_data <- AKDE_data %>%
  dplyr::mutate(diel_period_plot = if_else(diel_period == "0", "Day", "Night"))

UD_means <- UD_means %>%
  dplyr::mutate(diel_period_plot = if_else(diel_period == "0", "Day", "Night"))


# Compare fitted values across diel periods and study years (m^2)
(plot_UD_14_15 <- ggplot() +
  
  geom_jitter(data = AKDE_data, aes(x = study_year, y = UD_est/1000, color = diel_period_plot, group = diel_period_plot),
              position = position_jitterdodge(dodge.width = 0.6, jitter.width = 0.15, seed = 0), shape = 21, size = 1.2, stroke = 0.3, alpha = 1) +
  
  geom_errorbar(data = UD_means, aes(x = study_year, ymin = lwr_resp/1000, ymax = upr_resp/1000, group = diel_period_plot), width = 0.3,
                position = position_dodge(width = 0.6), colour = "black") +
  
  geom_point(data = UD_means, aes(x = study_year, y = fit_resp/1000, fill = diel_period_plot, group = diel_period_plot),
             position = position_dodge(width = 0.6), size = 2.5, shape = 21, colour = "black", stroke = 0.3) +
  
  scale_colour_manual(values = c("#fdb863", "#a6cee3")) +
  
  scale_fill_manual(values = c("#d95f02", "#084594")) +
    
  scale_y_continuous(limits = c(0, 420)) +
  
  labs(
    x = "<br>Year",
    y = "95% utilization distribution<br>(thousands of m<sup>2</sup>)<br>",
    color = "Diel period:",
    fill = "Diel period:"
  ) +
  my_theme +
    theme(
      legend.position = "inside",
      
        legend.position.inside = c(0.50, 0.95),
        legend.box = "vertical",
        legend.direction = "horizontal",
        legend.title = element_blank(),
        legend.text = element_text(size = 10, family = "Arial"),
        legend.key.size = unit(0.25, "cm")))

```

```{r UD_centroid_distance_14_15, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Linear model ####

dist_lm <- lm(relative_dist_km ~ diel_period * study_year, data = AKDE_data)

par(mfrow = c(2,2))
par(mar = c(5, 5, 2, 2))
plot(dist_lm)
par(mfrow = c(1,1))

# 2. Fit the linear model with GLS ####

dist_gls <- nlme::gls(relative_dist_km ~ diel_period * study_year, data = AKDE_data, method = "REML")

# 3. Determine the random component structure of the model ####

# Random effects for fish ID and time slot
dist_lmer <- lme4::lmer(relative_dist_km ~ diel_period * study_year +
                        (1 | Timeslot) + (1 | ID_year),
                        data = AKDE_data, REML = TRUE, na.action = "na.fail")

lme.fish <- lme4::lmer(relative_dist_km ~ diel_period * study_year + (1 | ID_year), 
                      data = AKDE_data, REML = TRUE, na.action = "na.fail")

lme.fish.not.nested <- lme4::lmer(relative_dist_km ~ diel_period * study_year + (1 | Fish_ID), 
                      data = AKDE_data, REML = TRUE, na.action = "na.fail")

lme.timeslot <- lme4::lmer(relative_dist_km ~ diel_period * study_year + (1 | Timeslot), 
                      data = AKDE_data, REML = TRUE, na.action = "na.fail")

dist_lmer_not_nested <- lme4::lmer(relative_dist_km ~ diel_period * study_year +
                        (1 | Timeslot) + (1 | Fish_ID),
                        data = AKDE_data, REML = TRUE, na.action = "na.fail")

mods <- MuMIn::AICc(dist_lmer, lme.fish, lme.timeslot, dist_gls, dist_lmer_not_nested, lme.fish.not.nested)

mods %>%
  dplyr::arrange(AICc)

remove(mods)

# The model with both random effects has the lowest AICc
# Further, the models w/ ID nested in study year have lower AIC than those that do not

# 4. Check the assumptions of the random intercept model ####

# Now that the optimal random component of the model has been chosen, we can re-check the assumptions.
dist_lmer <- lme4::lmer(relative_dist_km ~ diel_period * study_year + (1 | ID_year) + (1 | Timeslot), 
                        data = AKDE_data, REML = TRUE, na.action = "na.fail") 

par(mfrow = c(1,1))
par(mar = c(5, 5, 2, 2))
qqnorm(residuals(dist_lmer))
plot(residuals(dist_lmer, type = "pearson") ~ fitted(dist_lmer), xlab = "Fitted values", ylab = "Normalized residuals")
par(mfrow = c(1,1))

hist(residuals(dist_lmer), breaks = 50)

# Store the residuals in an object
NRs <- residuals(dist_lmer, type = "pearson")

boxplot(NRs ~ diel_period, data = AKDE_data, xlab = "Diel", ylab = "Residuals") 
abline(h = 0, lty = 2)

boxplot(NRs ~ study_year, data = AKDE_data, xlab = "Year", ylab = "Residuals") 
abline(h = 0, lty = 2)

# 5. Model averaging ####

dist_global <- glmmTMB::glmmTMB(relative_dist_km ~ diel_period * study_year + (1 | ID_year) + (1 | Timeslot),
                                data = AKDE_data, REML = FALSE, na.action = "na.fail")

dist_global_1 <- glmmTMB::glmmTMB(relative_dist_km ~ diel_period * study_year + (1 | ID_year) + (1 | Timeslot),
                                data = AKDE_data, REML = FALSE, na.action = "na.fail",
                                dispformula = ~ diel_period)

dist_global_2 <- glmmTMB::glmmTMB(relative_dist_km ~ diel_period * study_year + (1 | ID_year) + (1 | Timeslot),
                                data = AKDE_data, REML = FALSE, na.action = "na.fail",
                                dispformula = ~ study_year)

dist_global_3 <- glmmTMB::glmmTMB(relative_dist_km ~ diel_period * study_year + (1 | ID_year) + (1 | Timeslot),
                                data = AKDE_data, REML = FALSE, na.action = "na.fail",
                                dispformula = ~ diel_period + study_year)

dist_global_4 <- glmmTMB::glmmTMB(relative_dist_km ~ diel_period * study_year + (1 | ID_year) + (1 | Timeslot),
                                data = AKDE_data, REML = FALSE, na.action = "na.fail",
                                dispformula = ~ diel_period * study_year)


AIC(dist_global, dist_global_1, dist_global_2, dist_global_3, dist_global_4)

# Model w/ dispformula for diel period has lowest AIC

remove(dist_global, dist_global_1, dist_global_2, dist_global_3, dist_global_4)

dist_global <- glmmTMB::glmmTMB(relative_dist_km ~ diel_period * study_year + (1 | ID_year) + (1 | Timeslot),
                                data = AKDE_data, REML = FALSE, na.action = "na.fail",
                                dispformula = ~ diel_period)

# Perform all-subsets regression
dist.model.list <- MuMIn::dredge(global.model = dist_global, rank = "AICc")

dist.model.list

# The model that includes the interaction b/w diel period and study year has 90% of AIC weight; we don't need to average over candidate models

# 6. Interpretation of results ####

summary(dist_global)

# The intercept is 0.67. This is the estimated centroid distance from the fence for an individual bltr during the day in 2014.

# The coefficient for diel period (night) is -0.42. So, the centroid distance at night for 2014 is 0.67237 - 0.41486 = 0.25751 km from the fence

# The coefficient for study year is 0.15674. So, the centroid distance at day (reference) for 2015 is 0.67237 + 0.15674 = 0.82911 km from the fence

# The coefficient for the diel period X study year interaction is -0.11059. This means the centroid distance to the fence location for night in 2015 is 0.67237 - 0.41486 + 0.15674 - 0.11059 = 0.30366 km


# 7. Diel period X Study Year plot ####

# Create grid of new data
newdata <- expand.grid(
  diel_period = levels(AKDE_data$diel_period),
  study_year = levels(AKDE_data$study_year)
)

# Predict response
preds <- predict(dist_global, newdata = newdata, type = "response", re.form = NA, se.fit = TRUE)

# Compute 95% CIs
newdata$fit_resp <- preds$fit
newdata$se_resp <- preds$se.fit

# 95% CIs on the response scale (no transformation needed for distance to fence location)
newdata$lwr_resp <- newdata$fit_resp - 1.96 * newdata$se_resp
newdata$upr_resp <- newdata$fit_resp + 1.96 * newdata$se_resp

dist_means <- newdata 

# Rename levels for diel period
AKDE_data <- AKDE_data %>%
  dplyr::mutate(diel_period_plot = if_else(diel_period == "0", "Day", "Night"))

dist_means <- dist_means %>%
  dplyr::mutate(diel_period_plot = if_else(diel_period == "0", "Day", "Night"))


(plot_centroid_distance_14_15 <- ggplot() +
  
  geom_jitter(data = AKDE_data, aes(x = study_year, y = relative_dist_km*1000, color = diel_period_plot, group = diel_period_plot),
              position = position_jitterdodge(dodge.width = 0.6, jitter.width = 0.15, seed = 0), shape = 21, size = 1.2, stroke = 0.3, alpha = 1) +
  
  geom_errorbar(data = dist_means, aes(x = study_year, ymin = lwr_resp*1000, ymax = upr_resp*1000, group = diel_period_plot), width = 0.3,
                position = position_dodge(width = 0.6), colour = "black") +
  
  geom_point(data = dist_means, aes(x = study_year, y = fit_resp*1000, fill = diel_period_plot, group = diel_period_plot),
             position = position_dodge(width = 0.6), size = 2.5, shape = 21, colour = "black", stroke = 0.3) +
  
  scale_colour_manual(values = c("#fdb863", "#a6cee3")) +
  
  scale_fill_manual(values = c("#d95f02", "#084594")) +
    
  scale_y_continuous(limits = c(-100, 1700), breaks = c(0, 500, 1000, 1500)) +
  
  labs(
    x = "<br>Year",
    y = "Distance between centroids<br>and fence location (m)<br>",
    color = "Diel period:",
    fill = "Diel period:"
  ) +
  my_theme +
    theme(legend.position = "inside",
        legend.position.inside = c(0.50, 0.95),
        legend.box = "vertical",
        legend.direction = "horizontal",
        legend.title = element_blank(),
        legend.text = element_text(size = 10, family = "Arial"),
        legend.key.size = unit(0.25, "cm")))

# 6. Combine centroid plots ####

library(patchwork)

(plot_centroids <- plot_centroid_distance_14_15 + plot_centroid_FL_2014)

(plot_centroids <- plot_centroids +
  plot_layout(widths = c(0.4, 1), axis_titles = "collect_y") + 
  plot_annotation(tag_levels = "a", tag_prefix = "(", tag_suffix = ")")) &
  theme(plot.tag = element_text(size = 12),
        plot.margin = margin(t = 0.1, r = 0.1, b = 0.1, l = 0.1, unit = "cm"),
        plot.tag.position = "topleft",
        plot.tag.location = "margin")

```

```{r MVMT_analysis_14_15, eval=FALSE, include=FALSE, message=FALSE, echo=FALSE}

# 1. Determine timing of sunrise and sunset ####

# We will want to delineate the timing for sunrise and sunset in the figure, but that needs to be averaged across all days in the study for both years

solar.data.14 <- solar.data.14 %>%
  dplyr::select(Date, sunrise, sunset) %>%
  dplyr::filter(Date > "2014-04-25" & Date < "2014-05-20")

solar.data.15 <- solar.data.15 %>%
  dplyr::filter(Date > "2015-04-18" & Date < "2015-05-11")

sun_times <- rbind(solar.data.14, solar.data.15)

sun_times <- sun_times %>%
  dplyr::mutate(study_year = lubridate::year(Date),
                Day = Sys.Date()) %>%
  tidyr::separate(sunrise, into = c("X", "rise"), sep = " ", remove = FALSE) %>%
  tidyr::separate(sunset, into = c("Y", "set"), sep = " ", remove = FALSE) %>%
  tidyr::unite(sunrise_time, Day, rise, sep = " ", remove = FALSE) %>%
  tidyr::unite(sunset_time, Day, set, sep = " ", remove = TRUE) %>%
  dplyr::mutate(sunrise_time = as.POSIXct(sunrise_time),
                sunset_time = as.POSIXct(sunset_time)) %>%
  dplyr::select(study_year, sunrise_time, sunset_time) %>%
  dplyr::group_by(study_year) %>%
  dplyr::summarise(min_sunrise = min(sunrise_time),
                   median_sunrise = median(sunrise_time),
                   max_sunrise = max(sunrise_time),
                   min_sunset = min(sunset_time),
                   median_sunset = median(sunset_time),
                   max_sunset = max(sunset_time),
                   .groups = "drop") 


# Create objects for min and max sunrise and sunset times
# We'll plot lines in the figure for each to show the range of times for sunrise and sunset
# We also need to add +1 to each of these values
# This is because in our analysis, 1 = 00:00:00
# So, for instance, a value of 6 on the x-axis = 05:00:00
min_rise <- 5 + 19/60 + 39/3600 + 1 # 05:19:39 + 1
max_rise <- 6 + 13/60 + 15/3600 + 1 # 06:13:15 + 1
min_set <- 20 + 20/60 + 19/3600 + 1 # 20:20:19 + 1
max_set <- 21 + 9/60 + 18/3600  + 1 # 21:09:18 + 1

# 2. GAMM analysis of step lengths ####

# Set the knots for the cyclic GAMM smoother
knots <- list(hour_continuous = c(0.5, 24.5))

# First, fit a model w/ Gaussian distribution 
m0 <- mgcv::gam(distance_m ~ 
                  s(hour_continuous, by = study_year, bs = "cc", k = 10) + 
                  study_year +
                  s(ID_year, bs = "re") + 
                  s(Day_year, bs = "re"), 
                knots = knots, 
                data = MVMT_data, 
                method = "REML",
                select = TRUE)

set.seed(12345)
gam.check(m0, rep = 500)
gratia::appraise(m0)
plot(m0, pages = 1, seWithMean = TRUE)


# Gaussian distribution is not appropriate; try Tweedie
m1 <- mgcv::gam(distance_m ~ 
                  s(hour_continuous, by = study_year, bs = "cc", k = 10) + 
                  study_year +
                  s(ID_year, bs = "re") + 
                  s(Day_year, bs = "re"), 
                knots = knots, 
                data = MVMT_data, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

set.seed(12345)
gam.check(m1, rep = 500)
gratia::appraise(m1)
plot(m1, pages = 1, seWithMean = TRUE)
summary(m1)


# Model diagnostic plots look better, but max DF is set too low for hours variable 
m2 <- mgcv::gam(distance_m ~ 
                  s(hour_continuous, by = study_year, bs = "cc", k = 16) + 
                  study_year +
                  s(ID_year, bs = "re") + 
                  s(Day_year, bs = "re"), 
                knots = knots, 
                data = MVMT_data, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

set.seed(12345)
k.check(m2)
gratia::appraise(m2)
gratia::draw(m2)
plot(m2, pages = 1, seWithMean = TRUE)
summary(m2)
MuMIn::AICc(m2)
AIC(m2)

m3 <- mgcv::gam(distance_m ~ 
                  s(hour_continuous, bs = "cc", k = 16) + 
                  study_year +
                  s(ID_year, bs = "re") + 
                  s(Day_year, bs = "re"), 
                knots = knots, 
                data = MVMT_data, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m4 <- mgcv::gam(distance_m ~ 
                  s(hour_continuous, bs = "cc", k = 16) + 
                  s(ID_year, bs = "re") + 
                  s(Day_year, bs = "re"), 
                knots = knots, 
                data = MVMT_data, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

m5 <- mgcv::gam(distance_m ~ 
                  study_year +
                  s(ID_year, bs = "re") + 
                  s(Day_year, bs = "re"), 
                knots = knots, 
                data = MVMT_data, 
                method = "REML",
                select = TRUE,
                family = tw(link = "log"))

MuMIn::AICc(m2, m3, m4, m5)

# Plot effect of hour between study years

# Evaluate smooths for the hour variable by study year
smooths_14 <- gratia::smooth_estimates(m2, select = "s(hour_continuous):study_year2014")
smooths_15 <- gratia::smooth_estimates(m2, select = "s(hour_continuous):study_year2015")

# Combine the smooths for 2014 and 2015
smooths <- rbind(smooths_14, smooths_15)

# Create custom labels for the plots
label_1 <- grid::textGrob(label = "Night", x = 0.13, y = -4, 
      just = c("center", "top"),
      gp = grid::gpar(col = "black", fontsize = 16, fontfamily = "Arial", fontface = 1))

label_2 <- grid::textGrob(label = "Day", x = 0.55, y = -4, 
      just = c("center", "top"),
      gp = grid::gpar(col = "black", fontsize = 16, fontfamily = "Arial", fontface = 1))

label_3 <- grid::textGrob(label = "Night", x = 0.948, y = -4, 
      just = c("center", "top"),
      gp = grid::gpar(col = "black", fontsize = 16, fontfamily = "Arial", fontface = 1))

plot_labels_day_night <- data.frame(
  x = c(4, 14, 23),
  y = c(-10, -10, -10),
  label = c("Night", "Day", "Night"),
  stringsAsFactors = FALSE  # optional: keeps labels as characters, not factors
)


# Create an initial plot for the smooth of the hour variable for each year
# In this plot, the partial effect of hour on step length is shown
(plot_distance_partial_effects <- smooths %>%
  add_confint() %>%
  ggplot(aes(y = .estimate, x = hour_continuous, group = study_year)) +
  geom_vline(aes(xintercept = 6.6922222), linetype = "dashed") +
  geom_vline(aes(xintercept = 21.7911111), linetype = "dashed") +
  geom_hline(aes(yintercept = 0), linetype = "dashed") +
  geom_ribbon(aes(ymin = .lower_ci, ymax = .upper_ci), alpha = 0.3, fill = "black") +
  geom_line(colour = "black", linewidth = 1) +
  scale_x_continuous(limits = c(1,24), 
                     breaks = (seq(1,24,1)), expand = c(0.01,0),
                     labels = c(1, rep("",2), 4, rep("",2), 7, rep("",2), 10, rep("",2), 
                                13, rep("",2), 16, rep("",2), 19, rep("",2), 22, rep("",2))) +
  labs(
    y = "Partial effect",
    x = "Hour of the day") +
  my_theme + 
  theme(legend.position = "none",
        axis.text = element_text(colour = "black", family = "Arial", size = 12),
        axis.title.x = ggtext::element_markdown(size = 12, family = "Arial", margin = unit(c(20,0,0,0), "pt")),
        axis.title.y = ggtext::element_markdown(size = 12, family = "Arial", margin = unit(c(0,20,0,0), "pt")))+
    annotation_custom(label_1, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +
    annotation_custom(label_2, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +
    annotation_custom(label_3, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) )


# Generate predictions of distance moved by hour of the day.

new_data <- with(MVMT_data,
                 expand.grid(
                   hour_continuous = seq(min(hour_continuous), max(hour_continuous), by = 0.1),
                   study_year = levels(study_year)))


# Create dummy variables as placeholders for the random effects
new_data$ID_year <- factor(levels(MVMT_data$ID_year)[1])
new_data$Day_year <- factor(levels(MVMT_data$Day_year)[1])

pred <- predict.gam(m2, newdata = new_data, se.fit = TRUE, type = "response", exclude = c("s(ID_year)", "s(Day_year)"))

# Add predictions to data frame
new_data$fit <- pred$fit
new_data$se <- pred$se.fit
new_data$lower <- new_data$fit - 1.96 * new_data$se
new_data$upper <- new_data$fit + 1.96 * new_data$se

# Plot the predicted distances moved.
(plot_predicted_distance_moved <- new_data %>%
  ggplot() +
  
  #geom_hline(aes(yintercept = 0)) +
  geom_ribbon(aes(x = hour_continuous, ymin = lower, ymax = upper, group = study_year, fill = study_year), alpha = 0.5) +
  geom_line(aes(y = fit, x = hour_continuous, group = study_year, colour = study_year), linewidth = 0.75) +
    geom_vline(aes(xintercept = 6.6922222), linetype = "dashed") +
  geom_vline(aes(xintercept = 21.7911111), linetype = "dashed") +
  geom_text(data = plot_labels_day_night, aes(x = x, y = y, label = label), size = 4) +
  scale_x_continuous(limits = c(1,24), 
                     breaks = (seq(1,24,1)), expand = c(0.01,0),
                     labels = c(1, rep("",2), 4, rep("",2), 7, rep("",2), 10, rep("",2), 
                                13, rep("",2), 16, rep("",2), 19, rep("",2), 22, rep("",2))) +
  scale_y_continuous(limits = c(-50, max(new_data$upper)+20), expand = c(0,0)) +
    scale_colour_manual(values = c("#2b2b2b", "#a1361e")) +
    scale_fill_manual(values = c("#bfbfbf", "#eecfc5")) +
    labs(
    y = "Total distance moved per hour (m)",
    x = "Hour of the day",
    colour = "Study year:",
    fill = "Study year:") +

  my_theme + 
  theme(legend.position = "inside",
        legend.position.inside = c(0.5, 0.95),
        legend.direction = "horizontal",
        legend.text = element_text(family = "Arial", size = 10),
        legend.title = element_text(family = "Arial", size = 10)))
        

# Summarize range of predicted hourly distances moved between day and night for both years
day_hours <- new_data %>%
  dplyr::filter(hour_continuous >= 7.0 & hour_continuous < 22) %>%
  dplyr::group_by(study_year) %>%
  dplyr::summarise(min_fit = min(fit),
                   min_lwr = min(lower),
                   min_upr = min(upper),
                   max_fit = max(fit),
                   max_lwr = max(lower),
                   max_upr = max(upper))

night_hours <- new_data %>%
  dplyr::filter(hour_continuous < 7.0 | hour_continuous >= 22) %>%
  dplyr::group_by(study_year) %>%
  dplyr::summarise(min_fit = min(fit),
                   min_lwr = min(lower),
                   min_upr = min(upper),
                   max_fit = max(fit),
                   max_lwr = max(lower),
                   max_upr = max(upper))

# 3. Interpret model results ####

summary(m2)

```

